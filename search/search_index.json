{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Intro","text":"<p><code>any-guardrail</code> is a Python library providing a single interface to different guardrails.</p>"},{"location":"#getting-started","title":"Getting Started","text":"<p>Refer to the Quickstart for instructions on installation and usage.</p>"},{"location":"#guardrails","title":"Guardrails","text":"<p>Refer to Guardrails for the parameters for each guardrail.</p> <p>Refer to AnyGuardrail for how to use the <code>AnyGuardrail</code> object.</p>"},{"location":"#for-ai-systems","title":"For AI Systems","text":"<p>This documentation is available in two AI-friendly formats:</p> <ul> <li>llms.txt - A structured overview with curated links to key documentation sections</li> <li>llms-full.txt - Complete documentation content concatenated into a single file</li> </ul>"},{"location":"quickstart/","title":"Quick Start","text":""},{"location":"quickstart/#requirements","title":"Requirements","text":"<ul> <li>Python 3.11 or newer</li> </ul>"},{"location":"quickstart/#installation","title":"Installation","text":"<p>You can install the bare bones library as follows (only [<code>any_guardrails.guardrails.any_llm.AnyLlm</code>] will be available):</p> <pre><code>pip install any-guardrail\n</code></pre> <p>Or you can install it with the required dependencies for different guardrails:</p> <pre><code>pip install any-guardrail[huggingface]\n</code></pre> <p>Refer to pyproject.toml for a list of the options available.</p>"},{"location":"quickstart/#basic-usage","title":"Basic Usage","text":"<p><code>AnyGuardrail</code> provides a seamless interface for interacting with the guardrail models. It allows you to see a list of all the supported guardrails, and to instantiate each supported guardrails. Here is a full example:</p> <pre><code>from any_guardrail import AnyGuardrail, GuardrailName, GuardrailOutput\n\nguardrail = AnyGuardrail.create(GuardrailName.DEEPSET)\n\nresult: GuardrailOutput = guardrail.validate(\"All smiles from me!\")\n\nassert result.valid\n</code></pre>"},{"location":"quickstart/#troubleshooting","title":"Troubleshooting","text":"<p>Some of the models on HuggingFace require extra permissions to use. To do this, you'll need to create a HuggingFace profile and manually go through the permissions. Then, you'll need to download the HuggingFace Hub and login. One way to do this is:</p> <pre><code>pip install --upgrade huggingface_hub\n\nhf auth login\n</code></pre> <p>More information can be found here: HuggingFace Hub</p>"},{"location":"api/any_guardrail/","title":"AnyGuardrail","text":""},{"location":"api/any_guardrail/#anyguardrail","title":"AnyGuardrail","text":""},{"location":"api/any_guardrail/#any_guardrail.api.AnyGuardrail","title":"<code>any_guardrail.api.AnyGuardrail</code>","text":"<p>Factory class for creating guardrail instances.</p> Source code in <code>src/any_guardrail/api.py</code> <pre><code>class AnyGuardrail:\n    \"\"\"Factory class for creating guardrail instances.\"\"\"\n\n    @classmethod\n    def get_supported_guardrails(cls) -&gt; list[GuardrailName]:\n        \"\"\"List all supported guardrails.\"\"\"\n        return list(GuardrailName)\n\n    @classmethod\n    def get_supported_model(cls, guardrail_name: GuardrailName) -&gt; list[str]:\n        \"\"\"Get the model IDs supported by a specific guardrail.\"\"\"\n        guardrail_class = cls._get_guardrail_class(guardrail_name)\n        return guardrail_class.SUPPORTED_MODELS\n\n    @classmethod\n    def get_all_supported_models(cls) -&gt; dict[str, list[str]]:\n        \"\"\"Get all model IDs supported by all guardrails.\"\"\"\n        model_ids = {}\n        for guardrail_name in cls.get_supported_guardrails():\n            model_ids[guardrail_name.value] = cls.get_supported_model(guardrail_name)\n        return model_ids\n\n    @classmethod\n    def create(cls, guardrail_name: GuardrailName, **kwargs: Any) -&gt; Guardrail[Any, Any, Any]:\n        \"\"\"Create a guardrail instance.\n\n        Args:\n            guardrail_name: The name of the guardrail to use.\n            **kwargs: Additional keyword arguments to pass to the guardrail constructor.\n\n        Returns:\n            A guardrail instance.\n\n        \"\"\"\n        guardrail_class = cls._get_guardrail_class(guardrail_name)\n        return guardrail_class(**kwargs)\n\n    @classmethod\n    def _get_guardrail_class(cls, guardrail_name: GuardrailName) -&gt; type[Guardrail[Any, Any, Any]]:\n        guardrail_module_name = f\"{guardrail_name.value}\"\n        module_path = f\"any_guardrail.guardrails.{guardrail_module_name}.{guardrail_module_name}\"\n\n        module = importlib.import_module(module_path)\n        parts = re.split(r\"[^A-Za-z0-9]+\", guardrail_module_name)\n        candidate_name = \"\".join(p.capitalize() for p in parts if p)\n        guardrail_class = getattr(module, candidate_name, None)\n        if inspect.isclass(guardrail_class) and issubclass(guardrail_class, Guardrail):\n            return guardrail_class\n        msg = f\"Could not resolve guardrail class for '{guardrail_module_name}' in {module.__name__}\"\n        raise ImportError(msg)\n</code></pre>"},{"location":"api/any_guardrail/#any_guardrail.api.AnyGuardrail.create","title":"<code>create(guardrail_name, **kwargs)</code>  <code>classmethod</code>","text":"<p>Create a guardrail instance.</p> <p>Parameters:</p> Name Type Description Default <code>guardrail_name</code> <code>GuardrailName</code> <p>The name of the guardrail to use.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the guardrail constructor.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Guardrail[Any, Any, Any]</code> <p>A guardrail instance.</p> Source code in <code>src/any_guardrail/api.py</code> <pre><code>@classmethod\ndef create(cls, guardrail_name: GuardrailName, **kwargs: Any) -&gt; Guardrail[Any, Any, Any]:\n    \"\"\"Create a guardrail instance.\n\n    Args:\n        guardrail_name: The name of the guardrail to use.\n        **kwargs: Additional keyword arguments to pass to the guardrail constructor.\n\n    Returns:\n        A guardrail instance.\n\n    \"\"\"\n    guardrail_class = cls._get_guardrail_class(guardrail_name)\n    return guardrail_class(**kwargs)\n</code></pre>"},{"location":"api/any_guardrail/#any_guardrail.api.AnyGuardrail.get_all_supported_models","title":"<code>get_all_supported_models()</code>  <code>classmethod</code>","text":"<p>Get all model IDs supported by all guardrails.</p> Source code in <code>src/any_guardrail/api.py</code> <pre><code>@classmethod\ndef get_all_supported_models(cls) -&gt; dict[str, list[str]]:\n    \"\"\"Get all model IDs supported by all guardrails.\"\"\"\n    model_ids = {}\n    for guardrail_name in cls.get_supported_guardrails():\n        model_ids[guardrail_name.value] = cls.get_supported_model(guardrail_name)\n    return model_ids\n</code></pre>"},{"location":"api/any_guardrail/#any_guardrail.api.AnyGuardrail.get_supported_guardrails","title":"<code>get_supported_guardrails()</code>  <code>classmethod</code>","text":"<p>List all supported guardrails.</p> Source code in <code>src/any_guardrail/api.py</code> <pre><code>@classmethod\ndef get_supported_guardrails(cls) -&gt; list[GuardrailName]:\n    \"\"\"List all supported guardrails.\"\"\"\n    return list(GuardrailName)\n</code></pre>"},{"location":"api/any_guardrail/#any_guardrail.api.AnyGuardrail.get_supported_model","title":"<code>get_supported_model(guardrail_name)</code>  <code>classmethod</code>","text":"<p>Get the model IDs supported by a specific guardrail.</p> Source code in <code>src/any_guardrail/api.py</code> <pre><code>@classmethod\ndef get_supported_model(cls, guardrail_name: GuardrailName) -&gt; list[str]:\n    \"\"\"Get the model IDs supported by a specific guardrail.\"\"\"\n    guardrail_class = cls._get_guardrail_class(guardrail_name)\n    return guardrail_class.SUPPORTED_MODELS\n</code></pre>"},{"location":"api/guardrails/","title":"Index","text":""},{"location":"api/guardrails/#guardrails","title":"Guardrails","text":"<p>This section documents the available guardrails and their parameters. Select a guardrail to view its API details.</p>"},{"location":"api/guardrails/any_llm/","title":"AnyLLM","text":""},{"location":"api/guardrails/any_llm/#any_guardrail.guardrails.any_llm.any_llm","title":"<code>any_guardrail.guardrails.any_llm.any_llm</code>","text":""},{"location":"api/guardrails/any_llm/#any_guardrail.guardrails.any_llm.any_llm.DEFAULT_MODEL_ID","title":"<code>DEFAULT_MODEL_ID = 'openai:gpt-5-nano'</code>  <code>module-attribute</code>","text":"<p>Will be used as default argument for <code>model_id</code></p>"},{"location":"api/guardrails/any_llm/#any_guardrail.guardrails.any_llm.any_llm.DEFAULT_SYSTEM_PROMPT","title":"<code>DEFAULT_SYSTEM_PROMPT = \"\\nYou are a guardrail designed to ensure that the input text adheres to a specific policy.\\nYour only task is to validate the input_text, don't try to answer the user query.\\n\\nHere is the policy: {policy}\\n\\nYou must return the following:\\n\\n- valid: bool\\n    If the input text provided by the user doesn't adhere to the policy, you must reject it (mark it as valid=False).\\n\\n- explanation: str\\n    A clear explanation of why the input text was rejected or not.\\n\\n- score: float (0-1)\\n    How confident you are about the validation.\\n\"</code>  <code>module-attribute</code>","text":"<p>Will be used as default argument for <code>system_prompt</code></p>"},{"location":"api/guardrails/any_llm/#any_guardrail.guardrails.any_llm.any_llm.AnyLlm","title":"<code>AnyLlm</code>","text":"<p>               Bases: <code>Guardrail[bool, str, float]</code></p> <p>A guardrail using <code>any-llm</code>.</p> Source code in <code>src/any_guardrail/guardrails/any_llm/any_llm.py</code> <pre><code>class AnyLlm(Guardrail[bool, str, float]):\n    \"\"\"A guardrail using `any-llm`.\"\"\"\n\n    def validate(\n        self,\n        input_text: str,\n        policy: str,\n        model_id: str = DEFAULT_MODEL_ID,\n        system_prompt: str = DEFAULT_SYSTEM_PROMPT,\n        **kwargs: Any,\n    ) -&gt; GuardrailOutput[bool, str, float]:\n        \"\"\"Validate the `input_text` against the given `policy`.\n\n        Args:\n            input_text (str): The text to validate.\n            policy (str): The policy to validate against.\n            model_id (str, optional): The model ID to use.\n            system_prompt (str, optional): The system prompt to use.\n                Expected to have a `{policy}` placeholder.\n            **kwargs: Additional keyword arguments to pass to `any_llm.completion` function.\n\n        Returns:\n            GuardrailOutput: The output of the validation.\n\n        \"\"\"\n        result: ChatCompletion = completion(  # type: ignore[assignment]\n            model=model_id,\n            messages=[\n                {\"role\": \"system\", \"content\": system_prompt.format(policy=policy)},\n                {\"role\": \"user\", \"content\": input_text},\n            ],\n            response_format=GuardrailOutputAnyLLM,\n            **kwargs,\n        )\n        return GuardrailOutput(**json.loads(result.choices[0].message.content))  # type: ignore[arg-type]\n</code></pre>"},{"location":"api/guardrails/any_llm/#any_guardrail.guardrails.any_llm.any_llm.AnyLlm.validate","title":"<code>validate(input_text, policy, model_id=DEFAULT_MODEL_ID, system_prompt=DEFAULT_SYSTEM_PROMPT, **kwargs)</code>","text":"<p>Validate the <code>input_text</code> against the given <code>policy</code>.</p> <p>Parameters:</p> Name Type Description Default <code>input_text</code> <code>str</code> <p>The text to validate.</p> required <code>policy</code> <code>str</code> <p>The policy to validate against.</p> required <code>model_id</code> <code>str</code> <p>The model ID to use.</p> <code>DEFAULT_MODEL_ID</code> <code>system_prompt</code> <code>str</code> <p>The system prompt to use. Expected to have a <code>{policy}</code> placeholder.</p> <code>DEFAULT_SYSTEM_PROMPT</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to <code>any_llm.completion</code> function.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>GuardrailOutput</code> <code>GuardrailOutput[bool, str, float]</code> <p>The output of the validation.</p> Source code in <code>src/any_guardrail/guardrails/any_llm/any_llm.py</code> <pre><code>def validate(\n    self,\n    input_text: str,\n    policy: str,\n    model_id: str = DEFAULT_MODEL_ID,\n    system_prompt: str = DEFAULT_SYSTEM_PROMPT,\n    **kwargs: Any,\n) -&gt; GuardrailOutput[bool, str, float]:\n    \"\"\"Validate the `input_text` against the given `policy`.\n\n    Args:\n        input_text (str): The text to validate.\n        policy (str): The policy to validate against.\n        model_id (str, optional): The model ID to use.\n        system_prompt (str, optional): The system prompt to use.\n            Expected to have a `{policy}` placeholder.\n        **kwargs: Additional keyword arguments to pass to `any_llm.completion` function.\n\n    Returns:\n        GuardrailOutput: The output of the validation.\n\n    \"\"\"\n    result: ChatCompletion = completion(  # type: ignore[assignment]\n        model=model_id,\n        messages=[\n            {\"role\": \"system\", \"content\": system_prompt.format(policy=policy)},\n            {\"role\": \"user\", \"content\": input_text},\n        ],\n        response_format=GuardrailOutputAnyLLM,\n        **kwargs,\n    )\n    return GuardrailOutput(**json.loads(result.choices[0].message.content))  # type: ignore[arg-type]\n</code></pre>"},{"location":"api/guardrails/any_llm/#any_guardrail.guardrails.any_llm.any_llm.GuardrailOutputAnyLLM","title":"<code>GuardrailOutputAnyLLM</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Output model for AnyLlm guardrail.</p> Source code in <code>src/any_guardrail/guardrails/any_llm/any_llm.py</code> <pre><code>class GuardrailOutputAnyLLM(BaseModel):\n    \"\"\"Output model for AnyLlm guardrail.\"\"\"\n\n    valid: bool\n    explanation: str\n    score: int\n</code></pre>"},{"location":"api/guardrails/azure_content_safety/","title":"Azure Content Safety","text":""},{"location":"api/guardrails/azure_content_safety/#any_guardrail.guardrails.azure_content_safety.azure_content_safety","title":"<code>any_guardrail.guardrails.azure_content_safety.azure_content_safety</code>","text":""},{"location":"api/guardrails/azure_content_safety/#any_guardrail.guardrails.azure_content_safety.azure_content_safety.AzureContentSafety","title":"<code>AzureContentSafety</code>","text":"<p>               Bases: <code>ThreeStageGuardrail[AzureAnalyzeInput, AzureAnalyzeOutput, bool, dict[str, int | list[str] | None], float]</code></p> <p>Guardrail implementation using Azure Content Safety service.</p> <p>Azure Content Safety provides content moderation capabilities for text and images. To learn more about Azure Content Safety, visit the official documentation.</p> Source code in <code>src/any_guardrail/guardrails/azure_content_safety/azure_content_safety.py</code> <pre><code>class AzureContentSafety(\n    ThreeStageGuardrail[AzureAnalyzeInput, AzureAnalyzeOutput, bool, dict[str, int | list[str] | None], float]\n):\n    \"\"\"Guardrail implementation using Azure Content Safety service.\n\n    Azure Content Safety provides content moderation capabilities for text and images. To learn more about Azure\n    Content Safety, visit the [official documentation](https://github.com/Azure/azure-sdk-for-python/tree/main/sdk/contentsafety/azure-ai-contentsafety`).\n\n    \"\"\"\n\n    SUPPORTED_MODELS: ClassVar = [\"azure-content-safety\"]\n\n    def __init__(\n        self,\n        endpoint: str | None = None,\n        api_key: str | None = None,\n        threshold: int = 2,\n        score_type: str = \"max\",\n        blocklist_names: list[str] | None = None,\n    ) -&gt; None:\n        \"\"\"Initialize Azure Content Safety client.\n\n        Args:\n            endpoint (str): The endpoint URL for the Azure Content Safety service.\n            api_key (str): The API key for authenticating with the service.\n            threshold (int): The threshold for determining if content is unsafe.\n            score_type (str): The type of score to use (\"max\" or \"avg\").\n            blocklist_names (List[str] | None): List of blocklist names to use for content evaluation.\n\n        \"\"\"\n        if api_key:\n            credential = AzureKeyCredential(api_key)\n        else:\n            try:\n                credential = AzureKeyCredential(os.environ[\"CONTENT_SAFETY_KEY\"])\n            except KeyError as e:\n                msg = \"CONTENT_SAFETY_KEY environment variable is not set. Either provide an api_key or set the environment variable.\"\n                raise KeyError(msg) from e\n        if not endpoint:\n            try:\n                endpoint = os.environ[\"CONTENT_SAFETY_ENDPOINT\"]\n            except KeyError as e:\n                msg = \"CONTENT_SAFETY_ENDPOINT environment variable is not set. Either provide an endpoint or set the environment variable.\"\n                raise KeyError(msg) from e\n\n        self.client = ContentSafetyClient(endpoint=endpoint, credential=credential)\n        self.blocklist_client = BlocklistClient(endpoint=endpoint, credential=credential)\n        self.threshold = threshold\n\n        if score_type not in [\"max\", \"avg\"]:\n            msg = \"score_type must be either 'max' or 'avg'\"\n            raise ValueError(msg)\n        self.score_type = score_type\n\n        if blocklist_names:\n            if not isinstance(blocklist_names, list):\n                msg = \"blocklist_names must be a list of strings\"\n                raise ValueError(msg)\n            for name in blocklist_names:\n                if not isinstance(name, str):\n                    msg = \"blocklist_names must be a list of strings\"\n                    raise ValueError(msg)\n        self.blocklist_names = blocklist_names\n\n    def validate(self, content: str) -&gt; GuardrailOutput[bool, dict[str, int | list[str] | None], float]:\n        \"\"\"Validate content using Azure Content Safety.\n\n        Args:\n            content (str): The content to be evaluated.\n\n        Returns:\n            GuardrailOutput: The result of the guardrail evaluation.\n\n        \"\"\"\n        model_inputs = self._pre_processing(content)\n        model_outputs = self._inference(model_inputs)\n        return self._post_processing(model_outputs)\n\n    @error_message(\"Was unable to create or update blocklist.\")\n    def create_or_update_blocklist(\n        self, blocklist_name: str, blocklist_description: str, add_to_blocklist_names: bool = True\n    ) -&gt; None:\n        \"\"\"Create or update a blocklist in Azure Content Safety.\n\n        Args:\n            blocklist_name (str): The name of the blocklist.\n            blocklist_description (str): The description of the blocklist.\n            add_to_blocklist_names (bool): Whether to add the blocklist name to the guardrail's blocklist_names.\n\n        \"\"\"\n        self.blocklist_client.create_or_update_text_blocklist(\n            blocklist_name=blocklist_name,\n            options=TextBlocklist(blocklist_name=blocklist_name, description=blocklist_description),\n        )\n        if add_to_blocklist_names:\n            self.blocklist_names.append(blocklist_name) if self.blocklist_names else setattr(\n                self, \"blocklist_names\", [blocklist_name]\n            )\n\n    @error_message(\"Was unable to add blocklist items.\")\n    def add_blocklist_items(self, blocklist_name: str, blocklist_terms: list[str]) -&gt; None:\n        \"\"\"Add items to a blocklist.\n\n        Args:\n            blocklist_name (str): The name of the blocklist.\n            blocklist_terms (List[str]): The terms to add to the blocklist.\n\n        \"\"\"\n        blocklist_items = []\n        for term in blocklist_terms:\n            blocklist_items.append(TextBlocklistItem(text=term))\n\n        self.blocklist_client.add_or_update_blocklist_items(\n            blocklist_name=blocklist_name,\n            options=AddOrUpdateTextBlocklistItemsOptions(blocklist_items=blocklist_items),\n        )\n\n    @error_message(\"Was unable to list blocklists.\")\n    def list_blocklists(self) -&gt; list[dict[str, str | None]]:\n        \"\"\"List all blocklists in Azure Content Safety.\n\n        Returns:\n            List[Dict[str, str]]: A list of blocklist details.\n\n        \"\"\"\n        blocklists = self.blocklist_client.list_text_blocklists()\n        return [{\"name\": blocklist.blocklist_name, \"description\": blocklist.description} for blocklist in blocklists]\n\n    @error_message(\"Was unable to list blocklist items.\")\n    def list_blocklist_items(self, blocklist_name: str) -&gt; list[dict[str, str | None]]:\n        \"\"\"List items in a blocklist.\n\n        Args:\n            blocklist_name (str): The name of the blocklist.\n\n        Returns:\n            List[Dict[str, str]]: The list of blocklist items.\n\n        \"\"\"\n        blocklist_items = self.blocklist_client.list_text_blocklist_items(blocklist_name=blocklist_name)\n        return [\n            {\"id\": item.blocklist_item_id, \"text\": item.text, \"description\": item.description}\n            for item in blocklist_items\n        ]\n\n    @error_message(\"Was unable to get blocklist.\")\n    def get_blocklist(self, blocklist_name: str) -&gt; dict[str, str | None]:\n        \"\"\"Get a blocklist by name.\n\n        Args:\n            blocklist_name (str): The name of the blocklist.\n\n        Returns:\n            dict[str, str]: The blocklist details.\n\n        \"\"\"\n        blocklist = self.blocklist_client.get_text_blocklist(blocklist_name=blocklist_name)\n        return {\"name\": blocklist.blocklist_name, \"description\": blocklist.description}\n\n    @error_message(\"Was unable to get blocklist item.\")\n    def get_blocklist_item(self, blocklist_name: str, item_id: str) -&gt; dict[str, str | None]:\n        \"\"\"Get a blocklist item by ID.\n\n        Args:\n            blocklist_name (str): The name of the blocklist.\n            item_id (str): The ID of the blocklist item.\n\n        Returns:\n            dict[str, str]: The blocklist item details.\n\n        \"\"\"\n        item = self.blocklist_client.get_text_blocklist_item(blocklist_name=blocklist_name, blocklist_item_id=item_id)\n        return {\"id\": item.blocklist_item_id, \"text\": item.text, \"description\": item.description}\n\n    @error_message(\"Was unable to delete blocklist.\")\n    def delete_blocklist(self, blocklist_name: str) -&gt; None:\n        \"\"\"Delete a blocklist by name.\n\n        Args:\n            blocklist_name (str): The name of the blocklist.\n\n        \"\"\"\n        self.blocklist_client.delete_text_blocklist(blocklist_name=blocklist_name)\n        self.blocklist_names.remove(\n            blocklist_name\n        ) if self.blocklist_names and blocklist_name in self.blocklist_names else None\n\n    @error_message(\"Was unable to delete blocklist item.\")\n    def delete_blocklist_items(self, blocklist_name: str, item_ids: list[str]) -&gt; None:\n        \"\"\"Delete a blocklist item by ID.\n\n        Args:\n            blocklist_name (str): The name of the blocklist.\n            item_ids (List[str]): The IDs of the blocklist items.\n\n        \"\"\"\n        self.blocklist_client.remove_blocklist_items(  # type: ignore [call-overload]\n            blocklist_name=blocklist_name,\n            blocklist_item_id=RemoveTextBlocklistItemsOptions(blocklist_item_ids=item_ids),\n        )\n\n    def _pre_processing(self, text: str) -&gt; GuardrailPreprocessOutput[AzureAnalyzeInput]:\n        if self._is_existing_path(text):\n            try:\n                with open(text, \"rb\") as file:\n                    options: AzureAnalyzeInput = AnalyzeImageOptions(image=ImageData(content=file.read()))\n            except ValueError as e:\n                msg = \"Must provide a file path to an image file.\"\n                raise ValueError(msg) from e\n        else:\n            if self.blocklist_names:\n                options = AnalyzeTextOptions(\n                    text=text, blocklist_names=self.blocklist_names, halt_on_blocklist_hit=False\n                )\n            else:\n                options = AnalyzeTextOptions(text=text)\n        return GuardrailPreprocessOutput(data=options)\n\n    @error_message(\"Was unable to analyze text or image.\")\n    def _inference(\n        self, model_inputs: GuardrailPreprocessOutput[AzureAnalyzeInput]\n    ) -&gt; GuardrailInferenceOutput[AzureAnalyzeOutput]:\n        if isinstance(model_inputs.data, AnalyzeTextOptions):\n            response = self.client.analyze_text(model_inputs.data)\n        else:\n            response = self.client.analyze_image(model_inputs.data)  # type: ignore [assignment]\n        return GuardrailInferenceOutput(data=response)\n\n    def _post_processing(\n        self, model_outputs: GuardrailInferenceOutput[AzureAnalyzeOutput]\n    ) -&gt; GuardrailOutput[bool, dict[str, int | list[str] | None], float]:\n        results_dict = {\n            \"hate\": next(item for item in model_outputs.data.categories_analysis if item.category == TextCategory.HATE),\n            \"self_harm\": next(\n                item for item in model_outputs.data.categories_analysis if item.category == TextCategory.SELF_HARM\n            ),\n            \"sexual\": next(\n                item for item in model_outputs.data.categories_analysis if item.category == TextCategory.SEXUAL\n            ),\n            \"violence\": next(\n                item for item in model_outputs.data.categories_analysis if item.category == TextCategory.VIOLENCE\n            ),\n        }\n\n        explanation = {key: result.severity for key, result in results_dict.items() if result is not None}\n\n        if self.score_type == \"max\":\n            score = max(\n                explanation_score for explanation_score in explanation.values() if explanation_score is not None\n            )\n        else:\n            score = sum(\n                explanation_score for explanation_score in explanation.values() if explanation_score is not None\n            ) / sum(1 for explanation_score in explanation.values() if explanation_score is not None)\n\n        explanation[\"blocklist\"] = model_outputs.data.blocklists_match if self.blocklist_names else None\n\n        valid = score &lt; self.threshold\n        if valid and explanation.get(\"blocklist\"):\n            valid = False\n\n        return GuardrailOutput(valid=valid, explanation=explanation, score=score)\n\n    def _is_existing_path(self, text: str) -&gt; bool:\n        return os.path.exists(text)\n</code></pre>"},{"location":"api/guardrails/azure_content_safety/#any_guardrail.guardrails.azure_content_safety.azure_content_safety.AzureContentSafety.__init__","title":"<code>__init__(endpoint=None, api_key=None, threshold=2, score_type='max', blocklist_names=None)</code>","text":"<p>Initialize Azure Content Safety client.</p> <p>Parameters:</p> Name Type Description Default <code>endpoint</code> <code>str</code> <p>The endpoint URL for the Azure Content Safety service.</p> <code>None</code> <code>api_key</code> <code>str</code> <p>The API key for authenticating with the service.</p> <code>None</code> <code>threshold</code> <code>int</code> <p>The threshold for determining if content is unsafe.</p> <code>2</code> <code>score_type</code> <code>str</code> <p>The type of score to use (\"max\" or \"avg\").</p> <code>'max'</code> <code>blocklist_names</code> <code>List[str] | None</code> <p>List of blocklist names to use for content evaluation.</p> <code>None</code> Source code in <code>src/any_guardrail/guardrails/azure_content_safety/azure_content_safety.py</code> <pre><code>def __init__(\n    self,\n    endpoint: str | None = None,\n    api_key: str | None = None,\n    threshold: int = 2,\n    score_type: str = \"max\",\n    blocklist_names: list[str] | None = None,\n) -&gt; None:\n    \"\"\"Initialize Azure Content Safety client.\n\n    Args:\n        endpoint (str): The endpoint URL for the Azure Content Safety service.\n        api_key (str): The API key for authenticating with the service.\n        threshold (int): The threshold for determining if content is unsafe.\n        score_type (str): The type of score to use (\"max\" or \"avg\").\n        blocklist_names (List[str] | None): List of blocklist names to use for content evaluation.\n\n    \"\"\"\n    if api_key:\n        credential = AzureKeyCredential(api_key)\n    else:\n        try:\n            credential = AzureKeyCredential(os.environ[\"CONTENT_SAFETY_KEY\"])\n        except KeyError as e:\n            msg = \"CONTENT_SAFETY_KEY environment variable is not set. Either provide an api_key or set the environment variable.\"\n            raise KeyError(msg) from e\n    if not endpoint:\n        try:\n            endpoint = os.environ[\"CONTENT_SAFETY_ENDPOINT\"]\n        except KeyError as e:\n            msg = \"CONTENT_SAFETY_ENDPOINT environment variable is not set. Either provide an endpoint or set the environment variable.\"\n            raise KeyError(msg) from e\n\n    self.client = ContentSafetyClient(endpoint=endpoint, credential=credential)\n    self.blocklist_client = BlocklistClient(endpoint=endpoint, credential=credential)\n    self.threshold = threshold\n\n    if score_type not in [\"max\", \"avg\"]:\n        msg = \"score_type must be either 'max' or 'avg'\"\n        raise ValueError(msg)\n    self.score_type = score_type\n\n    if blocklist_names:\n        if not isinstance(blocklist_names, list):\n            msg = \"blocklist_names must be a list of strings\"\n            raise ValueError(msg)\n        for name in blocklist_names:\n            if not isinstance(name, str):\n                msg = \"blocklist_names must be a list of strings\"\n                raise ValueError(msg)\n    self.blocklist_names = blocklist_names\n</code></pre>"},{"location":"api/guardrails/azure_content_safety/#any_guardrail.guardrails.azure_content_safety.azure_content_safety.AzureContentSafety.add_blocklist_items","title":"<code>add_blocklist_items(blocklist_name, blocklist_terms)</code>","text":"<p>Add items to a blocklist.</p> <p>Parameters:</p> Name Type Description Default <code>blocklist_name</code> <code>str</code> <p>The name of the blocklist.</p> required <code>blocklist_terms</code> <code>List[str]</code> <p>The terms to add to the blocklist.</p> required Source code in <code>src/any_guardrail/guardrails/azure_content_safety/azure_content_safety.py</code> <pre><code>@error_message(\"Was unable to add blocklist items.\")\ndef add_blocklist_items(self, blocklist_name: str, blocklist_terms: list[str]) -&gt; None:\n    \"\"\"Add items to a blocklist.\n\n    Args:\n        blocklist_name (str): The name of the blocklist.\n        blocklist_terms (List[str]): The terms to add to the blocklist.\n\n    \"\"\"\n    blocklist_items = []\n    for term in blocklist_terms:\n        blocklist_items.append(TextBlocklistItem(text=term))\n\n    self.blocklist_client.add_or_update_blocklist_items(\n        blocklist_name=blocklist_name,\n        options=AddOrUpdateTextBlocklistItemsOptions(blocklist_items=blocklist_items),\n    )\n</code></pre>"},{"location":"api/guardrails/azure_content_safety/#any_guardrail.guardrails.azure_content_safety.azure_content_safety.AzureContentSafety.create_or_update_blocklist","title":"<code>create_or_update_blocklist(blocklist_name, blocklist_description, add_to_blocklist_names=True)</code>","text":"<p>Create or update a blocklist in Azure Content Safety.</p> <p>Parameters:</p> Name Type Description Default <code>blocklist_name</code> <code>str</code> <p>The name of the blocklist.</p> required <code>blocklist_description</code> <code>str</code> <p>The description of the blocklist.</p> required <code>add_to_blocklist_names</code> <code>bool</code> <p>Whether to add the blocklist name to the guardrail's blocklist_names.</p> <code>True</code> Source code in <code>src/any_guardrail/guardrails/azure_content_safety/azure_content_safety.py</code> <pre><code>@error_message(\"Was unable to create or update blocklist.\")\ndef create_or_update_blocklist(\n    self, blocklist_name: str, blocklist_description: str, add_to_blocklist_names: bool = True\n) -&gt; None:\n    \"\"\"Create or update a blocklist in Azure Content Safety.\n\n    Args:\n        blocklist_name (str): The name of the blocklist.\n        blocklist_description (str): The description of the blocklist.\n        add_to_blocklist_names (bool): Whether to add the blocklist name to the guardrail's blocklist_names.\n\n    \"\"\"\n    self.blocklist_client.create_or_update_text_blocklist(\n        blocklist_name=blocklist_name,\n        options=TextBlocklist(blocklist_name=blocklist_name, description=blocklist_description),\n    )\n    if add_to_blocklist_names:\n        self.blocklist_names.append(blocklist_name) if self.blocklist_names else setattr(\n            self, \"blocklist_names\", [blocklist_name]\n        )\n</code></pre>"},{"location":"api/guardrails/azure_content_safety/#any_guardrail.guardrails.azure_content_safety.azure_content_safety.AzureContentSafety.delete_blocklist","title":"<code>delete_blocklist(blocklist_name)</code>","text":"<p>Delete a blocklist by name.</p> <p>Parameters:</p> Name Type Description Default <code>blocklist_name</code> <code>str</code> <p>The name of the blocklist.</p> required Source code in <code>src/any_guardrail/guardrails/azure_content_safety/azure_content_safety.py</code> <pre><code>@error_message(\"Was unable to delete blocklist.\")\ndef delete_blocklist(self, blocklist_name: str) -&gt; None:\n    \"\"\"Delete a blocklist by name.\n\n    Args:\n        blocklist_name (str): The name of the blocklist.\n\n    \"\"\"\n    self.blocklist_client.delete_text_blocklist(blocklist_name=blocklist_name)\n    self.blocklist_names.remove(\n        blocklist_name\n    ) if self.blocklist_names and blocklist_name in self.blocklist_names else None\n</code></pre>"},{"location":"api/guardrails/azure_content_safety/#any_guardrail.guardrails.azure_content_safety.azure_content_safety.AzureContentSafety.delete_blocklist_items","title":"<code>delete_blocklist_items(blocklist_name, item_ids)</code>","text":"<p>Delete a blocklist item by ID.</p> <p>Parameters:</p> Name Type Description Default <code>blocklist_name</code> <code>str</code> <p>The name of the blocklist.</p> required <code>item_ids</code> <code>List[str]</code> <p>The IDs of the blocklist items.</p> required Source code in <code>src/any_guardrail/guardrails/azure_content_safety/azure_content_safety.py</code> <pre><code>@error_message(\"Was unable to delete blocklist item.\")\ndef delete_blocklist_items(self, blocklist_name: str, item_ids: list[str]) -&gt; None:\n    \"\"\"Delete a blocklist item by ID.\n\n    Args:\n        blocklist_name (str): The name of the blocklist.\n        item_ids (List[str]): The IDs of the blocklist items.\n\n    \"\"\"\n    self.blocklist_client.remove_blocklist_items(  # type: ignore [call-overload]\n        blocklist_name=blocklist_name,\n        blocklist_item_id=RemoveTextBlocklistItemsOptions(blocklist_item_ids=item_ids),\n    )\n</code></pre>"},{"location":"api/guardrails/azure_content_safety/#any_guardrail.guardrails.azure_content_safety.azure_content_safety.AzureContentSafety.get_blocklist","title":"<code>get_blocklist(blocklist_name)</code>","text":"<p>Get a blocklist by name.</p> <p>Parameters:</p> Name Type Description Default <code>blocklist_name</code> <code>str</code> <p>The name of the blocklist.</p> required <p>Returns:</p> Type Description <code>dict[str, str | None]</code> <p>dict[str, str]: The blocklist details.</p> Source code in <code>src/any_guardrail/guardrails/azure_content_safety/azure_content_safety.py</code> <pre><code>@error_message(\"Was unable to get blocklist.\")\ndef get_blocklist(self, blocklist_name: str) -&gt; dict[str, str | None]:\n    \"\"\"Get a blocklist by name.\n\n    Args:\n        blocklist_name (str): The name of the blocklist.\n\n    Returns:\n        dict[str, str]: The blocklist details.\n\n    \"\"\"\n    blocklist = self.blocklist_client.get_text_blocklist(blocklist_name=blocklist_name)\n    return {\"name\": blocklist.blocklist_name, \"description\": blocklist.description}\n</code></pre>"},{"location":"api/guardrails/azure_content_safety/#any_guardrail.guardrails.azure_content_safety.azure_content_safety.AzureContentSafety.get_blocklist_item","title":"<code>get_blocklist_item(blocklist_name, item_id)</code>","text":"<p>Get a blocklist item by ID.</p> <p>Parameters:</p> Name Type Description Default <code>blocklist_name</code> <code>str</code> <p>The name of the blocklist.</p> required <code>item_id</code> <code>str</code> <p>The ID of the blocklist item.</p> required <p>Returns:</p> Type Description <code>dict[str, str | None]</code> <p>dict[str, str]: The blocklist item details.</p> Source code in <code>src/any_guardrail/guardrails/azure_content_safety/azure_content_safety.py</code> <pre><code>@error_message(\"Was unable to get blocklist item.\")\ndef get_blocklist_item(self, blocklist_name: str, item_id: str) -&gt; dict[str, str | None]:\n    \"\"\"Get a blocklist item by ID.\n\n    Args:\n        blocklist_name (str): The name of the blocklist.\n        item_id (str): The ID of the blocklist item.\n\n    Returns:\n        dict[str, str]: The blocklist item details.\n\n    \"\"\"\n    item = self.blocklist_client.get_text_blocklist_item(blocklist_name=blocklist_name, blocklist_item_id=item_id)\n    return {\"id\": item.blocklist_item_id, \"text\": item.text, \"description\": item.description}\n</code></pre>"},{"location":"api/guardrails/azure_content_safety/#any_guardrail.guardrails.azure_content_safety.azure_content_safety.AzureContentSafety.list_blocklist_items","title":"<code>list_blocklist_items(blocklist_name)</code>","text":"<p>List items in a blocklist.</p> <p>Parameters:</p> Name Type Description Default <code>blocklist_name</code> <code>str</code> <p>The name of the blocklist.</p> required <p>Returns:</p> Type Description <code>list[dict[str, str | None]]</code> <p>List[Dict[str, str]]: The list of blocklist items.</p> Source code in <code>src/any_guardrail/guardrails/azure_content_safety/azure_content_safety.py</code> <pre><code>@error_message(\"Was unable to list blocklist items.\")\ndef list_blocklist_items(self, blocklist_name: str) -&gt; list[dict[str, str | None]]:\n    \"\"\"List items in a blocklist.\n\n    Args:\n        blocklist_name (str): The name of the blocklist.\n\n    Returns:\n        List[Dict[str, str]]: The list of blocklist items.\n\n    \"\"\"\n    blocklist_items = self.blocklist_client.list_text_blocklist_items(blocklist_name=blocklist_name)\n    return [\n        {\"id\": item.blocklist_item_id, \"text\": item.text, \"description\": item.description}\n        for item in blocklist_items\n    ]\n</code></pre>"},{"location":"api/guardrails/azure_content_safety/#any_guardrail.guardrails.azure_content_safety.azure_content_safety.AzureContentSafety.list_blocklists","title":"<code>list_blocklists()</code>","text":"<p>List all blocklists in Azure Content Safety.</p> <p>Returns:</p> Type Description <code>list[dict[str, str | None]]</code> <p>List[Dict[str, str]]: A list of blocklist details.</p> Source code in <code>src/any_guardrail/guardrails/azure_content_safety/azure_content_safety.py</code> <pre><code>@error_message(\"Was unable to list blocklists.\")\ndef list_blocklists(self) -&gt; list[dict[str, str | None]]:\n    \"\"\"List all blocklists in Azure Content Safety.\n\n    Returns:\n        List[Dict[str, str]]: A list of blocklist details.\n\n    \"\"\"\n    blocklists = self.blocklist_client.list_text_blocklists()\n    return [{\"name\": blocklist.blocklist_name, \"description\": blocklist.description} for blocklist in blocklists]\n</code></pre>"},{"location":"api/guardrails/azure_content_safety/#any_guardrail.guardrails.azure_content_safety.azure_content_safety.AzureContentSafety.validate","title":"<code>validate(content)</code>","text":"<p>Validate content using Azure Content Safety.</p> <p>Parameters:</p> Name Type Description Default <code>content</code> <code>str</code> <p>The content to be evaluated.</p> required <p>Returns:</p> Name Type Description <code>GuardrailOutput</code> <code>GuardrailOutput[bool, dict[str, int | list[str] | None], float]</code> <p>The result of the guardrail evaluation.</p> Source code in <code>src/any_guardrail/guardrails/azure_content_safety/azure_content_safety.py</code> <pre><code>def validate(self, content: str) -&gt; GuardrailOutput[bool, dict[str, int | list[str] | None], float]:\n    \"\"\"Validate content using Azure Content Safety.\n\n    Args:\n        content (str): The content to be evaluated.\n\n    Returns:\n        GuardrailOutput: The result of the guardrail evaluation.\n\n    \"\"\"\n    model_inputs = self._pre_processing(content)\n    model_outputs = self._inference(model_inputs)\n    return self._post_processing(model_outputs)\n</code></pre>"},{"location":"api/guardrails/azure_content_safety/#any_guardrail.guardrails.azure_content_safety.azure_content_safety.error_message","title":"<code>error_message(message)</code>","text":"<p>Handle exceptions for Azure Content Safety operations.</p> Source code in <code>src/any_guardrail/guardrails/azure_content_safety/azure_content_safety.py</code> <pre><code>def error_message(message: str) -&gt; Callable[[F], F]:\n    \"\"\"Handle exceptions for Azure Content Safety operations.\"\"\"\n\n    def error_handler_decorator(func: F) -&gt; F:\n        \"\"\"Handle exceptions for the wrapped function.\"\"\"\n\n        @functools.wraps(func)\n        def wrapper(*args: Any, **kwargs: Any) -&gt; Any:\n            try:\n                return func(*args, **kwargs)\n            except HttpResponseError as e:\n                raise RuntimeError(message + f\" Details: {e!s}\") from e\n\n        return wrapper  # type: ignore[return-value]\n\n    return error_handler_decorator\n</code></pre>"},{"location":"api/guardrails/deepset/","title":"Deepset","text":""},{"location":"api/guardrails/deepset/#any_guardrail.guardrails.deepset.deepset","title":"<code>any_guardrail.guardrails.deepset.deepset</code>","text":""},{"location":"api/guardrails/deepset/#any_guardrail.guardrails.deepset.deepset.Deepset","title":"<code>Deepset</code>","text":"<p>               Bases: <code>HuggingFace[dict[str, Any], dict[str, Any], bool, None, float]</code></p> <p>Wrapper for prompt injection detection model from Deepset.</p> <p>For more information, please see the model card:</p> <ul> <li>Deepset.</li> </ul> Source code in <code>src/any_guardrail/guardrails/deepset/deepset.py</code> <pre><code>class Deepset(HuggingFace[dict[str, Any], dict[str, Any], bool, None, float]):\n    \"\"\"Wrapper for prompt injection detection model from Deepset.\n\n    For more information, please see the model card:\n\n    - [Deepset](https://huggingface.co/deepset/deberta-v3-base-injection).\n    \"\"\"\n\n    SUPPORTED_MODELS: ClassVar = [\"deepset/deberta-v3-base-injection\"]\n\n    def _post_processing(\n        self, model_outputs: GuardrailInferenceOutput[dict[str, Any]]\n    ) -&gt; GuardrailOutput[bool, None, float]:\n        return _match_injection_label(model_outputs, DEEPSET_INJECTION_LABEL, self.model.config.id2label)\n</code></pre>"},{"location":"api/guardrails/duo_guard/","title":"Duo Guard","text":""},{"location":"api/guardrails/duo_guard/#any_guardrail.guardrails.duo_guard.duo_guard","title":"<code>any_guardrail.guardrails.duo_guard.duo_guard</code>","text":""},{"location":"api/guardrails/duo_guard/#any_guardrail.guardrails.duo_guard.duo_guard.DuoGuard","title":"<code>DuoGuard</code>","text":"<p>               Bases: <code>HuggingFace[dict[str, Any], dict[str, Any], bool, dict[str, bool], float]</code></p> <p>Guardrail that classifies text based on the categories in DUOGUARD_CATEGORIES.</p> <p>For more information, please see the model card:</p> <ul> <li>DuoGuard.</li> </ul> Source code in <code>src/any_guardrail/guardrails/duo_guard/duo_guard.py</code> <pre><code>class DuoGuard(HuggingFace[dict[str, Any], dict[str, Any], bool, dict[str, bool], float]):\n    \"\"\"Guardrail that classifies text based on the categories in DUOGUARD_CATEGORIES.\n\n    For more information, please see the model card:\n\n    - [DuoGuard](https://huggingface.co/collections/DuoGuard/duoguard-models-67a29ad8bd579a404e504d21).\n    \"\"\"\n\n    SUPPORTED_MODELS: ClassVar = [\n        \"DuoGuard/DuoGuard-0.5B\",\n        \"DuoGuard/DuoGuard-1B-Llama-3.2-transfer\",\n        \"DuoGuard/DuoGuard-1.5B-transfer\",\n    ]\n\n    MODELS_TO_TOKENIZER: ClassVar = {\n        \"DuoGuard/DuoGuard-0.5B\": \"Qwen/Qwen2.5-0.5B\",\n        \"DuoGuard/DuoGuard-1B-Llama-3.2-transfer\": \"meta-llama/Llama-3.2-1B\",\n        \"DuoGuard/DuoGuard-1.5B-transfer\": \"Qwen/Qwen2.5-1.5B\",\n    }\n\n    def __init__(self, model_id: str | None = None, threshold: float = DUOGUARD_DEFAULT_THRESHOLD) -&gt; None:\n        \"\"\"Initialize the DuoGuard model.\"\"\"\n        super().__init__(model_id)\n        self.threshold = threshold\n\n    def _load_model(self) -&gt; None:\n        from transformers import AutoModelForSequenceClassification, AutoTokenizer\n\n        self.model = AutoModelForSequenceClassification.from_pretrained(self.model_id)\n        self.tokenizer = AutoTokenizer.from_pretrained(self.MODELS_TO_TOKENIZER[self.model_id])\n        self.tokenizer.pad_token = self.tokenizer.eos_token\n\n    def _post_processing(\n        self, model_outputs: GuardrailInferenceOutput[dict[str, Any]]\n    ) -&gt; GuardrailOutput[bool, dict[str, bool], float]:\n        from torch.nn.functional import sigmoid\n\n        probabilities = sigmoid(model_outputs.data[\"logits\"][0]).tolist()\n        predicted_labels = {\n            category: prob &gt; self.threshold for category, prob in zip(DUOGUARD_CATEGORIES, probabilities, strict=True)\n        }\n        return GuardrailOutput(\n            valid=not any(predicted_labels.values()), explanation=predicted_labels, score=max(probabilities)\n        )\n</code></pre>"},{"location":"api/guardrails/duo_guard/#any_guardrail.guardrails.duo_guard.duo_guard.DuoGuard.__init__","title":"<code>__init__(model_id=None, threshold=DUOGUARD_DEFAULT_THRESHOLD)</code>","text":"<p>Initialize the DuoGuard model.</p> Source code in <code>src/any_guardrail/guardrails/duo_guard/duo_guard.py</code> <pre><code>def __init__(self, model_id: str | None = None, threshold: float = DUOGUARD_DEFAULT_THRESHOLD) -&gt; None:\n    \"\"\"Initialize the DuoGuard model.\"\"\"\n    super().__init__(model_id)\n    self.threshold = threshold\n</code></pre>"},{"location":"api/guardrails/flowjudge/","title":"FlowJudge","text":""},{"location":"api/guardrails/flowjudge/#any_guardrail.guardrails.flowjudge.flowjudge","title":"<code>any_guardrail.guardrails.flowjudge.flowjudge</code>","text":""},{"location":"api/guardrails/flowjudge/#any_guardrail.guardrails.flowjudge.flowjudge.Flowjudge","title":"<code>Flowjudge</code>","text":"<p>               Bases: <code>ThreeStageGuardrail['EvalInputType', 'EvalOutputType', None, str, int]</code></p> <p>Wrapper around FlowJudge, allowing for custom guardrailing based on user defined criteria, metrics, and rubric.</p> <p>Please see the model card for more information: FlowJudge.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>User defined metric name.</p> required <code>criteria</code> <code>str</code> <p>User defined question that they want answered by FlowJudge model.</p> required <code>rubric</code> <code>dict[int, str]</code> <p>A scoring rubric in a likert scale fashion, providing an integer score and then a description of what the value means.</p> required <code>required_inputs</code> <code>list[str]</code> <p>A list of what is required for the judge to consider.</p> required <code>required_output</code> <code>str</code> <p>What is the expected output from the judge.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>Only supports FlowJudge keywords to instantiate FlowJudge.</p> Source code in <code>src/any_guardrail/guardrails/flowjudge/flowjudge.py</code> <pre><code>class Flowjudge(ThreeStageGuardrail[\"EvalInputType\", \"EvalOutputType\", None, str, int]):\n    \"\"\"Wrapper around FlowJudge, allowing for custom guardrailing based on user defined criteria, metrics, and rubric.\n\n    Please see the model card for more information: [FlowJudge](https://huggingface.co/flowaicom/Flow-Judge-v0.1).\n\n    Args:\n        name: User defined metric name.\n        criteria: User defined question that they want answered by FlowJudge model.\n        rubric: A scoring rubric in a likert scale fashion, providing an integer score and then a description of what the\n            value means.\n        required_inputs: A list of what is required for the judge to consider.\n        required_output: What is the expected output from the judge.\n\n    Raises:\n        ValueError: Only supports FlowJudge keywords to instantiate FlowJudge.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        name: str,\n        criteria: str,\n        rubric: dict[int, str],\n        required_inputs: list[str],\n        required_output: str,\n    ) -&gt; None:\n        \"\"\"Initialize the FlowJudgeClass.\"\"\"\n        if MISSING_PACKAGES_ERROR is not None:\n            msg = \"Missing packages for FlowJudge guardrail. You can try `pip install 'any-guardrail[flowjudge]'`\"\n            raise ImportError(msg) from MISSING_PACKAGES_ERROR\n\n        self.metric_name = name\n        self.criteria = criteria\n        self.rubric = rubric\n        self.required_inputs = required_inputs\n        self.required_output = required_output\n        self.metric_prompt = self._define_metric_prompt()\n        self.model = self._load_model()\n\n    def validate(self, inputs: list[dict[str, str]], output: dict[str, str]) -&gt; GuardrailOutput[None, str, int]:\n        \"\"\"Classifies the desired input and output according to the associated metric provided to the judge.\n\n        Args:\n            inputs: A dictionary mapping the required input names to the inputs.\n            output: A dictionary mapping the required output name to the output.\n\n        Return:\n            A score from the RubricItems and feedback related to the rubric and criteria.\n\n        \"\"\"\n        eval_input = self._pre_processing(inputs, output)\n        result = self._inference(eval_input)\n        return self._post_processing(result)\n\n    def _load_model(self) -&gt; FlowJudge:\n        \"\"\"Construct the FlowJudge model using the defined metric prompt that contains the rubric, criteria, and metric.\n\n        Returns:\n            judge: The evaluation model.\n\n        \"\"\"\n        model = Hf(flash_attn=False)\n        return FlowJudge(metric=self.metric_prompt, model=model)\n\n    def _define_metric_prompt(self) -&gt; Metric:\n        \"\"\"Construct the Metric object needed to instantiate the FlowJudge model.\n\n        Returns:\n            The Metric object used to construct the FlowJudge model.\n\n        \"\"\"\n        processed_rubric = self._construct_rubric()\n        return Metric(\n            name=self.metric_name,\n            criteria=self.criteria,\n            rubric=processed_rubric,\n            required_inputs=self.required_inputs,\n            required_output=self.required_output,\n        )\n\n    def _construct_rubric(self) -&gt; list[RubricItem]:\n        \"\"\"Construct the rubric from a user defined rubric dicitionary to construct the Metric object.\n\n        Returns:\n            List of RubricItem objects.\n\n        \"\"\"\n        processed_rubric = []\n        for key, value in self.rubric.items():\n            rubric_item = RubricItem(score=key, description=value)\n            processed_rubric.append(rubric_item)\n        return processed_rubric\n\n    def _pre_processing(\n        self, inputs: list[dict[str, str]], output: dict[str, str]\n    ) -&gt; GuardrailPreprocessOutput[\"EvalInputType\"]:\n        eval_input = EvalInput(inputs=inputs, output=output)\n        return GuardrailPreprocessOutput(data=eval_input)\n\n    def _inference(\n        self, eval_input: GuardrailPreprocessOutput[\"EvalInputType\"]\n    ) -&gt; GuardrailInferenceOutput[\"EvalOutputType\"]:\n        result = self.model.evaluate(eval_input.data, save_results=False)\n        return GuardrailInferenceOutput(data=result)\n\n    def _post_processing(\n        self, model_outputs: GuardrailInferenceOutput[\"EvalOutputType\"]\n    ) -&gt; GuardrailOutput[None, str, int]:\n        return GuardrailOutput(explanation=model_outputs.data.feedback, score=model_outputs.data.score)\n</code></pre>"},{"location":"api/guardrails/flowjudge/#any_guardrail.guardrails.flowjudge.flowjudge.Flowjudge.__init__","title":"<code>__init__(name, criteria, rubric, required_inputs, required_output)</code>","text":"<p>Initialize the FlowJudgeClass.</p> Source code in <code>src/any_guardrail/guardrails/flowjudge/flowjudge.py</code> <pre><code>def __init__(\n    self,\n    name: str,\n    criteria: str,\n    rubric: dict[int, str],\n    required_inputs: list[str],\n    required_output: str,\n) -&gt; None:\n    \"\"\"Initialize the FlowJudgeClass.\"\"\"\n    if MISSING_PACKAGES_ERROR is not None:\n        msg = \"Missing packages for FlowJudge guardrail. You can try `pip install 'any-guardrail[flowjudge]'`\"\n        raise ImportError(msg) from MISSING_PACKAGES_ERROR\n\n    self.metric_name = name\n    self.criteria = criteria\n    self.rubric = rubric\n    self.required_inputs = required_inputs\n    self.required_output = required_output\n    self.metric_prompt = self._define_metric_prompt()\n    self.model = self._load_model()\n</code></pre>"},{"location":"api/guardrails/flowjudge/#any_guardrail.guardrails.flowjudge.flowjudge.Flowjudge.validate","title":"<code>validate(inputs, output)</code>","text":"<p>Classifies the desired input and output according to the associated metric provided to the judge.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>list[dict[str, str]]</code> <p>A dictionary mapping the required input names to the inputs.</p> required <code>output</code> <code>dict[str, str]</code> <p>A dictionary mapping the required output name to the output.</p> required Return <p>A score from the RubricItems and feedback related to the rubric and criteria.</p> Source code in <code>src/any_guardrail/guardrails/flowjudge/flowjudge.py</code> <pre><code>def validate(self, inputs: list[dict[str, str]], output: dict[str, str]) -&gt; GuardrailOutput[None, str, int]:\n    \"\"\"Classifies the desired input and output according to the associated metric provided to the judge.\n\n    Args:\n        inputs: A dictionary mapping the required input names to the inputs.\n        output: A dictionary mapping the required output name to the output.\n\n    Return:\n        A score from the RubricItems and feedback related to the rubric and criteria.\n\n    \"\"\"\n    eval_input = self._pre_processing(inputs, output)\n    result = self._inference(eval_input)\n    return self._post_processing(result)\n</code></pre>"},{"location":"api/guardrails/glider/","title":"Glider","text":""},{"location":"api/guardrails/glider/#any_guardrail.guardrails.glider.glider","title":"<code>any_guardrail.guardrails.glider.glider</code>","text":""},{"location":"api/guardrails/glider/#any_guardrail.guardrails.glider.glider.Glider","title":"<code>Glider</code>","text":"<p>               Bases: <code>HuggingFace[ChatMessages, str, None, str, int | None]</code></p> <p>A prompt based guardrail from Patronus AI that utilizes pass criteria and a rubric to judge text.</p> <p>For more information, see the model card:GLIDER. It outputs its reasoning, highlights for what determined the score, and an integer score.</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str | None</code> <p>HuggingFace path to model.</p> <code>None</code> <code>pass_criteria</code> <code>str</code> <p>A question or description of what you are validating.</p> required <code>rubric</code> <code>str</code> <p>A scoring rubric, describing to the model how to score the provided data.</p> required Raise <p>ValueError: Can only use model path to GLIDER from HuggingFace.</p> Source code in <code>src/any_guardrail/guardrails/glider/glider.py</code> <pre><code>class Glider(HuggingFace[ChatMessages, str, None, str, int | None]):\n    \"\"\"A prompt based guardrail from Patronus AI that utilizes pass criteria and a rubric to judge text.\n\n    For more information, see the model card:[GLIDER](https://huggingface.co/PatronusAI/glider). It outputs its reasoning,\n    highlights for what determined the score, and an integer score.\n\n    Args:\n        model_id: HuggingFace path to model.\n        pass_criteria: A question or description of what you are validating.\n        rubric: A scoring rubric, describing to the model how to score the provided data.\n\n    Raise:\n        ValueError: Can only use model path to GLIDER from HuggingFace.\n\n    \"\"\"\n\n    SUPPORTED_MODELS: ClassVar = [\"PatronusAI/glider\"]\n\n    def __init__(self, pass_criteria: str, rubric: str, model_id: str | None = None) -&gt; None:\n        \"\"\"Initialize the GLIDER guardrail.\"\"\"\n        super().__init__(model_id)\n        self.pass_criteria = pass_criteria\n        self.rubric = rubric\n        self.system_prompt = SYSTEM_PROMPT_GLIDER\n\n    def validate(self, input_text: str, output_text: str | None = None) -&gt; GuardrailOutput[None, str, int | None]:\n        \"\"\"Use the provided pass criteria and rubric to judge the input and output text provided.\n\n        Args:\n            input_text: the initial text.\n            output_text: the subsequent text.\n\n        Returns:\n            An explanation in the format provided by the system prompt.\n\n        \"\"\"\n        message = self._pre_processing(input_text, output_text)\n        result = self._inference(message)\n        return self._post_processing(result)\n\n    def _load_model(self) -&gt; None:\n        from transformers import pipeline\n\n        pipe = pipeline(\"text-generation\", self.model_id, max_new_tokens=2048, return_full_text=False)\n        self.model = pipe\n\n    def _pre_processing(\n        self, input_text: str, output_text: str | None = None\n    ) -&gt; GuardrailPreprocessOutput[ChatMessages]:\n        if output_text is None:\n            data = INPUT_DATA_FORMAT.format(input_text=input_text)\n        else:\n            data = INPUT_OUTPUT_DATA_FORMAT.format(input_text=input_text, output_text=output_text)\n        prompt = self.system_prompt.format(data=data, pass_criteria=self.pass_criteria, rubric=self.rubric)\n        return GuardrailPreprocessOutput(data=[{\"role\": \"user\", \"content\": prompt}])\n\n    def _inference(self, message: GuardrailPreprocessOutput[ChatMessages]) -&gt; GuardrailInferenceOutput[str]:\n        generated_text = self.model(message.data)[0][\"generated_text\"]\n        return GuardrailInferenceOutput(data=generated_text)\n\n    def _post_processing(self, model_outputs: GuardrailInferenceOutput[str]) -&gt; GuardrailOutput[None, str, int | None]:\n        score = re.findall(r\"&lt;score&gt;\\n(\\d+)\\n&lt;/score&gt;\", model_outputs.data)\n        if len(score) != 0 and score[0].isdigit():\n            final_score = int(score[0])\n        else:\n            final_score = None\n\n        return GuardrailOutput(explanation=model_outputs.data, score=final_score)\n</code></pre>"},{"location":"api/guardrails/glider/#any_guardrail.guardrails.glider.glider.Glider.__init__","title":"<code>__init__(pass_criteria, rubric, model_id=None)</code>","text":"<p>Initialize the GLIDER guardrail.</p> Source code in <code>src/any_guardrail/guardrails/glider/glider.py</code> <pre><code>def __init__(self, pass_criteria: str, rubric: str, model_id: str | None = None) -&gt; None:\n    \"\"\"Initialize the GLIDER guardrail.\"\"\"\n    super().__init__(model_id)\n    self.pass_criteria = pass_criteria\n    self.rubric = rubric\n    self.system_prompt = SYSTEM_PROMPT_GLIDER\n</code></pre>"},{"location":"api/guardrails/glider/#any_guardrail.guardrails.glider.glider.Glider.validate","title":"<code>validate(input_text, output_text=None)</code>","text":"<p>Use the provided pass criteria and rubric to judge the input and output text provided.</p> <p>Parameters:</p> Name Type Description Default <code>input_text</code> <code>str</code> <p>the initial text.</p> required <code>output_text</code> <code>str | None</code> <p>the subsequent text.</p> <code>None</code> <p>Returns:</p> Type Description <code>GuardrailOutput[None, str, int | None]</code> <p>An explanation in the format provided by the system prompt.</p> Source code in <code>src/any_guardrail/guardrails/glider/glider.py</code> <pre><code>def validate(self, input_text: str, output_text: str | None = None) -&gt; GuardrailOutput[None, str, int | None]:\n    \"\"\"Use the provided pass criteria and rubric to judge the input and output text provided.\n\n    Args:\n        input_text: the initial text.\n        output_text: the subsequent text.\n\n    Returns:\n        An explanation in the format provided by the system prompt.\n\n    \"\"\"\n    message = self._pre_processing(input_text, output_text)\n    result = self._inference(message)\n    return self._post_processing(result)\n</code></pre>"},{"location":"api/guardrails/harm_guard/","title":"Harm Guard","text":""},{"location":"api/guardrails/harm_guard/#any_guardrail.guardrails.harm_guard.harm_guard","title":"<code>any_guardrail.guardrails.harm_guard.harm_guard</code>","text":""},{"location":"api/guardrails/harm_guard/#any_guardrail.guardrails.harm_guard.harm_guard.HarmGuard","title":"<code>HarmGuard</code>","text":"<p>               Bases: <code>HuggingFace[dict[str, Any], dict[str, Any], bool, None, float]</code></p> <p>Prompt injection detection encoder based model.</p> <p>For more information, please see the model card:</p> <ul> <li>HarmGuard.</li> </ul> Source code in <code>src/any_guardrail/guardrails/harm_guard/harm_guard.py</code> <pre><code>class HarmGuard(HuggingFace[dict[str, Any], dict[str, Any], bool, None, float]):\n    \"\"\"Prompt injection detection encoder based model.\n\n    For more information, please see the model card:\n\n    - [HarmGuard](https://huggingface.co/hbseong/HarmAug-Guard).\n    \"\"\"\n\n    SUPPORTED_MODELS: ClassVar = [\"hbseong/HarmAug-Guard\"]\n\n    def __init__(self, model_id: str | None = None, threshold: float = HARMGUARD_DEFAULT_THRESHOLD) -&gt; None:\n        \"\"\"Initialize the HarmGuard guardrail.\"\"\"\n        super().__init__(model_id)\n        self.threshold = threshold\n\n    def _post_processing(\n        self, model_outputs: GuardrailInferenceOutput[dict[str, Any]]\n    ) -&gt; GuardrailOutput[bool, None, float]:\n        logits = model_outputs.data[\"logits\"][0].numpy()\n        scores = _softmax(logits)  # type: ignore[no-untyped-call]\n        final_score = float(scores[1])\n        return GuardrailOutput(valid=final_score &lt; self.threshold, score=final_score)\n</code></pre>"},{"location":"api/guardrails/harm_guard/#any_guardrail.guardrails.harm_guard.harm_guard.HarmGuard.__init__","title":"<code>__init__(model_id=None, threshold=HARMGUARD_DEFAULT_THRESHOLD)</code>","text":"<p>Initialize the HarmGuard guardrail.</p> Source code in <code>src/any_guardrail/guardrails/harm_guard/harm_guard.py</code> <pre><code>def __init__(self, model_id: str | None = None, threshold: float = HARMGUARD_DEFAULT_THRESHOLD) -&gt; None:\n    \"\"\"Initialize the HarmGuard guardrail.\"\"\"\n    super().__init__(model_id)\n    self.threshold = threshold\n</code></pre>"},{"location":"api/guardrails/injec_guard/","title":"InjecGuard","text":""},{"location":"api/guardrails/injec_guard/#any_guardrail.guardrails.injec_guard.injec_guard","title":"<code>any_guardrail.guardrails.injec_guard.injec_guard</code>","text":""},{"location":"api/guardrails/injec_guard/#any_guardrail.guardrails.injec_guard.injec_guard.InjecGuard","title":"<code>InjecGuard</code>","text":"<p>               Bases: <code>HuggingFace[dict[str, Any], dict[str, Any], bool, None, float]</code></p> <p>Prompt injection detection encoder based model.</p> <p>For more information, please see the model card:</p> <ul> <li>InjecGuard.</li> </ul> Source code in <code>src/any_guardrail/guardrails/injec_guard/injec_guard.py</code> <pre><code>class InjecGuard(HuggingFace[dict[str, Any], dict[str, Any], bool, None, float]):\n    \"\"\"Prompt injection detection encoder based model.\n\n    For more information, please see the model card:\n\n    - [InjecGuard](https://huggingface.co/leolee99/InjecGuard).\n    \"\"\"\n\n    SUPPORTED_MODELS: ClassVar = [\"leolee99/InjecGuard\"]\n\n    def _load_model(self) -&gt; None:\n        from transformers import AutoModelForSequenceClassification, AutoTokenizer\n\n        self.model = AutoModelForSequenceClassification.from_pretrained(self.model_id, trust_remote_code=True)\n        self.tokenizer = AutoTokenizer.from_pretrained(self.model_id)\n\n    def _post_processing(\n        self, model_outputs: GuardrailInferenceOutput[dict[str, Any]]\n    ) -&gt; GuardrailOutput[bool, None, float]:\n        return _match_injection_label(model_outputs, INJECGUARD_LABEL, self.model.config.id2label)\n</code></pre>"},{"location":"api/guardrails/jasper/","title":"Jasper","text":""},{"location":"api/guardrails/jasper/#any_guardrail.guardrails.jasper.jasper","title":"<code>any_guardrail.guardrails.jasper.jasper</code>","text":""},{"location":"api/guardrails/jasper/#any_guardrail.guardrails.jasper.jasper.Jasper","title":"<code>Jasper</code>","text":"<p>               Bases: <code>HuggingFace[dict[str, Any], dict[str, Any], bool, None, float]</code></p> <p>Prompt injection detection encoder based models.</p> <p>For more information, please see the model card:</p> <ul> <li>Jasper Deberta</li> <li>Jasper Gelectra.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str | None</code> <p>HuggingFace path to model.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>Can only use model paths for Jasper models from HuggingFace.</p> Source code in <code>src/any_guardrail/guardrails/jasper/jasper.py</code> <pre><code>class Jasper(HuggingFace[dict[str, Any], dict[str, Any], bool, None, float]):\n    \"\"\"Prompt injection detection encoder based models.\n\n    For more information, please see the model card:\n\n    - [Jasper Deberta](https://huggingface.co/JasperLS/deberta-v3-base-injection)\n    - [Jasper Gelectra](https://huggingface.co/JasperLS/gelectra-base-injection).\n\n    Args:\n        model_id: HuggingFace path to model.\n\n    Raises:\n        ValueError: Can only use model paths for Jasper models from HuggingFace.\n\n    \"\"\"\n\n    SUPPORTED_MODELS: ClassVar = [\"JasperLS/gelectra-base-injection\", \"JasperLS/deberta-v3-base-injection\"]\n\n    def _post_processing(\n        self, model_outputs: GuardrailInferenceOutput[dict[str, Any]]\n    ) -&gt; GuardrailOutput[bool, None, float]:\n        return _match_injection_label(model_outputs, JASPER_INJECTION_LABEL, self.model.config.id2label)\n</code></pre>"},{"location":"api/guardrails/llama_guard/","title":"Llama Guard","text":""},{"location":"api/guardrails/llama_guard/#any_guardrail.guardrails.llama_guard.llama_guard","title":"<code>any_guardrail.guardrails.llama_guard.llama_guard</code>","text":""},{"location":"api/guardrails/llama_guard/#any_guardrail.guardrails.llama_guard.llama_guard.LlamaGuard","title":"<code>LlamaGuard</code>","text":"<p>               Bases: <code>HuggingFace[LlamaGuardPreprocessData, LlamaGuardInferenceData, bool, str, None]</code></p> <p>Wrapper class for Llama Guard 3 &amp; 4 implementations.</p> <p>For more information about the implementations about either off topic model, please see the below model cards:</p> <ul> <li>Meta Llama Guard 3 Docs</li> <li>HuggingFace Llama Guard 3 Docs</li> <li>Meta Llama Guard 4 Docs</li> <li>HuggingFace Llama Guard 4 Docs</li> </ul> Source code in <code>src/any_guardrail/guardrails/llama_guard/llama_guard.py</code> <pre><code>class LlamaGuard(HuggingFace[LlamaGuardPreprocessData, LlamaGuardInferenceData, bool, str, None]):\n    \"\"\"Wrapper class for Llama Guard 3 &amp; 4 implementations.\n\n    For more information about the implementations about either off topic model, please see the below model cards:\n\n    - [Meta Llama Guard 3 Docs](https://www.llama.com/docs/model-cards-and-prompt-formats/llama-guard-3/)\n    - [HuggingFace Llama Guard 3 Docs](https://huggingface.co/meta-llama/Llama-Guard-3-1B)\n    - [Meta Llama Guard 4 Docs](https://www.llama.com/docs/model-cards-and-prompt-formats/llama-guard-4/)\n    - [HuggingFace Llama Guard 4 Docs](https://huggingface.co/meta-llama/Llama-Guard-4-12B)\n    \"\"\"\n\n    SUPPORTED_MODELS: ClassVar = [\n        \"meta-llama/Llama-Guard-3-1B\",\n        \"meta-llama/Llama-Guard-3-8B\",\n        \"meta-llama/Llama-Guard-4-12B\",\n    ]\n\n    def __init__(self, model_id: str | None = None) -&gt; None:\n        \"\"\"Llama guard model. Either Llama Guard 3 or 4 depending on the model id. Defaults to Llama Guard 3.\"\"\"\n        self.model_id = model_id or self.SUPPORTED_MODELS[0]\n        if self._is_version_4:\n            self.tokenizer_class = AutoProcessor\n            self.model_class = Llama4ForConditionalGeneration\n            self.tokenizer_params = {\n                \"return_tensors\": \"pt\",\n                \"add_generation_prompt\": True,\n                \"tokenize\": True,\n                \"return_dict\": True,\n            }\n        elif self.model_id in self.SUPPORTED_MODELS:\n            self.tokenizer_class = AutoTokenizer  # type: ignore[assignment]\n            self.model_class = AutoModelForCausalLM  # type: ignore[assignment]\n            self.tokenizer_params = {\n                \"return_tensors\": \"pt\",\n            }\n        else:\n            msg = f\"Unsupported model_id: {self.model_id}\"\n            raise ValueError(msg)\n        super().__init__(model_id)\n\n    def validate(\n        self, input_text: str, output_text: str | None = None, **kwargs: Any\n    ) -&gt; GuardrailOutput[bool, str, None]:\n        \"\"\"Judge whether the input text or the input text, output text pair are unsafe based on the Llama taxonomy.\n\n        Args:\n            input_text: the prior text before hitting a system or model.\n            output_text: the succeeding text after hitting a system or model.\n            **kwargs: additional keyword arguments, specifically supporting 'excluded_category_keys' and 'categories'.\n                Please see Llama Guard documentation for more details.\n\n        Returns:\n            Provides an explanation that can be parsed to see whether the text is safe or not.\n\n        \"\"\"\n        model_inputs = self._pre_processing(input_text, output_text, **kwargs)\n        model_outputs = self._inference(model_inputs)\n        return self._post_processing(model_outputs)\n\n    def _load_model(self) -&gt; None:\n        self.tokenizer = self.tokenizer_class.from_pretrained(self.model_id)  # type: ignore[no-untyped-call]\n        self.model = self.model_class.from_pretrained(self.model_id)\n\n    def _pre_processing(\n        self, input_text: str, output_text: str | None = None, **kwargs: Any\n    ) -&gt; GuardrailPreprocessOutput[LlamaGuardPreprocessData]:\n        if output_text:\n            if self.model_id == self.SUPPORTED_MODELS[0] or self._is_version_4:\n                conversation = [\n                    {\n                        \"role\": \"user\",\n                        \"content\": [\n                            {\"type\": \"text\", \"text\": input_text},\n                        ],\n                    },\n                    {\n                        \"role\": \"assistant\",\n                        \"content\": [\n                            {\"type\": \"text\", \"text\": output_text},\n                        ],\n                    },\n                ]\n            else:\n                conversation = [\n                    {\n                        \"role\": \"user\",\n                        \"content\": input_text,\n                    },\n                    {\n                        \"role\": \"assistant\",\n                        \"content\": output_text,\n                    },\n                ]\n        else:\n            if self.model_id == self.SUPPORTED_MODELS[0] or self._is_version_4:\n                conversation = [\n                    {\n                        \"role\": \"user\",\n                        \"content\": [\n                            {\"type\": \"text\", \"text\": input_text},\n                        ],\n                    },\n                ]\n            else:\n                conversation = [\n                    {\n                        \"role\": \"user\",\n                        \"content\": input_text,\n                    },\n                ]\n        self._cached_model_inputs = self.tokenizer.apply_chat_template(conversation, **self.tokenizer_params, **kwargs)  # type: ignore[arg-type]\n        return GuardrailPreprocessOutput(data=self._cached_model_inputs)\n\n    def _inference(\n        self, model_inputs: GuardrailPreprocessOutput[LlamaGuardPreprocessData]\n    ) -&gt; GuardrailInferenceOutput[LlamaGuardInferenceData]:\n        if self._is_version_4:\n            output = self.model.generate(**model_inputs.data, max_new_tokens=10, do_sample=False)\n        else:\n            output = self.model.generate(\n                model_inputs.data.get(\"input_ids\"),\n                max_new_tokens=20,\n                pad_token_id=0,\n            )\n        return GuardrailInferenceOutput(data=output)\n\n    def _post_processing(\n        self, model_outputs: GuardrailInferenceOutput[LlamaGuardInferenceData]\n    ) -&gt; GuardrailOutput[bool, str, None]:\n        if self._is_version_4:\n            explanation = self.tokenizer.batch_decode(\n                model_outputs.data[:, self._cached_model_inputs[\"input_ids\"].shape[-1] :],  # type: ignore[call-overload, index, union-attr]\n                skip_special_tokens=True,\n            )[0]\n\n            if \"unsafe\" in explanation.lower():\n                return GuardrailOutput(valid=False, explanation=explanation)\n            return GuardrailOutput(valid=True, explanation=explanation)\n\n        prompt_len = self._cached_model_inputs.get(\"input_ids\").shape[1]  # type: ignore[union-attr]\n        output = model_outputs.data[:, prompt_len:]\n        explanation = self.tokenizer.decode(output[0])  # type: ignore[assignment]\n\n        if \"unsafe\" in explanation.lower():\n            return GuardrailOutput(valid=False, explanation=explanation)\n        return GuardrailOutput(valid=True, explanation=explanation)\n\n    @property\n    def _is_version_4(self) -&gt; bool:\n        return self.model_id == self.SUPPORTED_MODELS[-1]\n</code></pre>"},{"location":"api/guardrails/llama_guard/#any_guardrail.guardrails.llama_guard.llama_guard.LlamaGuard.__init__","title":"<code>__init__(model_id=None)</code>","text":"<p>Llama guard model. Either Llama Guard 3 or 4 depending on the model id. Defaults to Llama Guard 3.</p> Source code in <code>src/any_guardrail/guardrails/llama_guard/llama_guard.py</code> <pre><code>def __init__(self, model_id: str | None = None) -&gt; None:\n    \"\"\"Llama guard model. Either Llama Guard 3 or 4 depending on the model id. Defaults to Llama Guard 3.\"\"\"\n    self.model_id = model_id or self.SUPPORTED_MODELS[0]\n    if self._is_version_4:\n        self.tokenizer_class = AutoProcessor\n        self.model_class = Llama4ForConditionalGeneration\n        self.tokenizer_params = {\n            \"return_tensors\": \"pt\",\n            \"add_generation_prompt\": True,\n            \"tokenize\": True,\n            \"return_dict\": True,\n        }\n    elif self.model_id in self.SUPPORTED_MODELS:\n        self.tokenizer_class = AutoTokenizer  # type: ignore[assignment]\n        self.model_class = AutoModelForCausalLM  # type: ignore[assignment]\n        self.tokenizer_params = {\n            \"return_tensors\": \"pt\",\n        }\n    else:\n        msg = f\"Unsupported model_id: {self.model_id}\"\n        raise ValueError(msg)\n    super().__init__(model_id)\n</code></pre>"},{"location":"api/guardrails/llama_guard/#any_guardrail.guardrails.llama_guard.llama_guard.LlamaGuard.validate","title":"<code>validate(input_text, output_text=None, **kwargs)</code>","text":"<p>Judge whether the input text or the input text, output text pair are unsafe based on the Llama taxonomy.</p> <p>Parameters:</p> Name Type Description Default <code>input_text</code> <code>str</code> <p>the prior text before hitting a system or model.</p> required <code>output_text</code> <code>str | None</code> <p>the succeeding text after hitting a system or model.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>additional keyword arguments, specifically supporting 'excluded_category_keys' and 'categories'. Please see Llama Guard documentation for more details.</p> <code>{}</code> <p>Returns:</p> Type Description <code>GuardrailOutput[bool, str, None]</code> <p>Provides an explanation that can be parsed to see whether the text is safe or not.</p> Source code in <code>src/any_guardrail/guardrails/llama_guard/llama_guard.py</code> <pre><code>def validate(\n    self, input_text: str, output_text: str | None = None, **kwargs: Any\n) -&gt; GuardrailOutput[bool, str, None]:\n    \"\"\"Judge whether the input text or the input text, output text pair are unsafe based on the Llama taxonomy.\n\n    Args:\n        input_text: the prior text before hitting a system or model.\n        output_text: the succeeding text after hitting a system or model.\n        **kwargs: additional keyword arguments, specifically supporting 'excluded_category_keys' and 'categories'.\n            Please see Llama Guard documentation for more details.\n\n    Returns:\n        Provides an explanation that can be parsed to see whether the text is safe or not.\n\n    \"\"\"\n    model_inputs = self._pre_processing(input_text, output_text, **kwargs)\n    model_outputs = self._inference(model_inputs)\n    return self._post_processing(model_outputs)\n</code></pre>"},{"location":"api/guardrails/off_topic/","title":"OffTopic","text":""},{"location":"api/guardrails/off_topic/#any_guardrail.guardrails.off_topic","title":"<code>any_guardrail.guardrails.off_topic</code>","text":""},{"location":"api/guardrails/off_topic/#any_guardrail.guardrails.off_topic.OffTopic","title":"<code>OffTopic</code>","text":"<p>               Bases: <code>HuggingFace[Any, Any, bool, dict[str, float], float]</code></p> <p>Abstract base class for the Off Topic models.</p> <p>For more information about the implementations about either off topic model, please see the below model cards:</p> <ul> <li>govtech/stsb-roberta-base-off-topic model.</li> <li>govtech/jina-embeddings-v2-small-en-off-topic.</li> </ul> Source code in <code>src/any_guardrail/guardrails/off_topic/off_topic.py</code> <pre><code>class OffTopic(HuggingFace[Any, Any, bool, dict[str, float], float]):\n    \"\"\"Abstract base class for the Off Topic models.\n\n    For more information about the implementations about either off topic model, please see the below model cards:\n\n    - [govtech/stsb-roberta-base-off-topic model](https://huggingface.co/govtech/stsb-roberta-base-off-topic).\n    - [govtech/jina-embeddings-v2-small-en-off-topic](https://huggingface.co/govtech/jina-embeddings-v2-small-en-off-topic).\n    \"\"\"\n\n    SUPPORTED_MODELS: ClassVar = [\n        \"mozilla-ai/jina-embeddings-v2-small-en-off-topic\",\n        \"mozilla-ai/stsb-roberta-base-off-topic\",\n    ]\n\n    implementation: OffTopicJina | OffTopicStsb\n\n    def __init__(self, model_id: str | None = None) -&gt; None:\n        \"\"\"Off Topic model based on one of two implementations decided by model ID.\"\"\"\n        if model_id is None:\n            model_id = self.SUPPORTED_MODELS[0]\n\n        if model_id == self.SUPPORTED_MODELS[0]:\n            self.implementation = OffTopicJina()\n        elif model_id == self.SUPPORTED_MODELS[1]:\n            self.implementation = OffTopicStsb()\n        else:\n            msg = f\"Unsupported model_id: {model_id}\"\n            raise ValueError(msg)\n        super().__init__(model_id)\n\n    def validate(\n        self, input_text: str, comparison_text: str | None = None\n    ) -&gt; GuardrailOutput[bool, dict[str, float], float]:\n        \"\"\"Compare two texts to see if they are relevant to each other.\n\n        Args:\n            input_text: the original text you want to compare against.\n            comparison_text: the text you want to compare to.\n\n        Returns:\n            valid=False means off topic, valid=True  means on topic. Will also provide probabilities of each.\n\n        \"\"\"\n        msg = \"Must provide a text to compare to.\"\n        if not comparison_text:\n            raise ValueError(msg)\n        model_inputs: Any = self.implementation._pre_processing(input_text, comparison_text)\n        model_outputs: Any = self.implementation._inference(model_inputs)\n        return self._post_processing(model_outputs)\n\n    def _load_model(self) -&gt; None:\n        self.implementation._load_model()\n\n    def _post_processing(\n        self, model_outputs: GuardrailInferenceOutput[Any]\n    ) -&gt; GuardrailOutput[bool, dict[str, float], float]:\n        return self.implementation._post_processing(model_outputs)\n</code></pre>"},{"location":"api/guardrails/off_topic/#any_guardrail.guardrails.off_topic.OffTopic.__init__","title":"<code>__init__(model_id=None)</code>","text":"<p>Off Topic model based on one of two implementations decided by model ID.</p> Source code in <code>src/any_guardrail/guardrails/off_topic/off_topic.py</code> <pre><code>def __init__(self, model_id: str | None = None) -&gt; None:\n    \"\"\"Off Topic model based on one of two implementations decided by model ID.\"\"\"\n    if model_id is None:\n        model_id = self.SUPPORTED_MODELS[0]\n\n    if model_id == self.SUPPORTED_MODELS[0]:\n        self.implementation = OffTopicJina()\n    elif model_id == self.SUPPORTED_MODELS[1]:\n        self.implementation = OffTopicStsb()\n    else:\n        msg = f\"Unsupported model_id: {model_id}\"\n        raise ValueError(msg)\n    super().__init__(model_id)\n</code></pre>"},{"location":"api/guardrails/off_topic/#any_guardrail.guardrails.off_topic.OffTopic.validate","title":"<code>validate(input_text, comparison_text=None)</code>","text":"<p>Compare two texts to see if they are relevant to each other.</p> <p>Parameters:</p> Name Type Description Default <code>input_text</code> <code>str</code> <p>the original text you want to compare against.</p> required <code>comparison_text</code> <code>str | None</code> <p>the text you want to compare to.</p> <code>None</code> <p>Returns:</p> Type Description <code>GuardrailOutput[bool, dict[str, float], float]</code> <p>valid=False means off topic, valid=True  means on topic. Will also provide probabilities of each.</p> Source code in <code>src/any_guardrail/guardrails/off_topic/off_topic.py</code> <pre><code>def validate(\n    self, input_text: str, comparison_text: str | None = None\n) -&gt; GuardrailOutput[bool, dict[str, float], float]:\n    \"\"\"Compare two texts to see if they are relevant to each other.\n\n    Args:\n        input_text: the original text you want to compare against.\n        comparison_text: the text you want to compare to.\n\n    Returns:\n        valid=False means off topic, valid=True  means on topic. Will also provide probabilities of each.\n\n    \"\"\"\n    msg = \"Must provide a text to compare to.\"\n    if not comparison_text:\n        raise ValueError(msg)\n    model_inputs: Any = self.implementation._pre_processing(input_text, comparison_text)\n    model_outputs: Any = self.implementation._inference(model_inputs)\n    return self._post_processing(model_outputs)\n</code></pre>"},{"location":"api/guardrails/pangolin/","title":"Pangolin","text":""},{"location":"api/guardrails/pangolin/#any_guardrail.guardrails.pangolin.pangolin","title":"<code>any_guardrail.guardrails.pangolin.pangolin</code>","text":""},{"location":"api/guardrails/pangolin/#any_guardrail.guardrails.pangolin.pangolin.Pangolin","title":"<code>Pangolin</code>","text":"<p>               Bases: <code>HuggingFace[dict[str, Any], dict[str, Any], bool, None, float]</code></p> <p>Prompt injection detection encoder based models.</p> <p>For more information, please see the model card:</p> <ul> <li>Pangolin Base</li> </ul> Source code in <code>src/any_guardrail/guardrails/pangolin/pangolin.py</code> <pre><code>class Pangolin(HuggingFace[dict[str, Any], dict[str, Any], bool, None, float]):\n    \"\"\"Prompt injection detection encoder based models.\n\n    For more information, please see the model card:\n\n    - [Pangolin Base](https://huggingface.co/dcarpintero/pangolin-guard-base)\n    \"\"\"\n\n    SUPPORTED_MODELS: ClassVar = [\"dcarpintero/pangolin-guard-base\"]\n\n    def _post_processing(\n        self, model_outputs: GuardrailInferenceOutput[dict[str, Any]]\n    ) -&gt; GuardrailOutput[bool, None, float]:\n        return _match_injection_label(model_outputs, PANGOLIN_INJECTION_LABEL, self.model.config.id2label)\n</code></pre>"},{"location":"api/guardrails/protectai/","title":"ProtectAI","text":""},{"location":"api/guardrails/protectai/#any_guardrail.guardrails.protectai.protectai","title":"<code>any_guardrail.guardrails.protectai.protectai</code>","text":""},{"location":"api/guardrails/protectai/#any_guardrail.guardrails.protectai.protectai.Protectai","title":"<code>Protectai</code>","text":"<p>               Bases: <code>HuggingFace[dict[str, Any], dict[str, Any], bool, None, float]</code></p> <p>Prompt injection detection encoder based models.</p> <p>For more information, please see the model card:</p> <ul> <li>ProtectAI.</li> </ul> Source code in <code>src/any_guardrail/guardrails/protectai/protectai.py</code> <pre><code>class Protectai(HuggingFace[dict[str, Any], dict[str, Any], bool, None, float]):\n    \"\"\"Prompt injection detection encoder based models.\n\n    For more information, please see the model card:\n\n    - [ProtectAI](https://huggingface.co/collections/protectai/llm-security-65c1f17a11c4251eeab53f40).\n    \"\"\"\n\n    SUPPORTED_MODELS: ClassVar = [\n        \"ProtectAI/deberta-v3-small-prompt-injection-v2\",\n        \"ProtectAI/distilroberta-base-rejection-v1\",\n        \"ProtectAI/deberta-v3-base-prompt-injection\",\n        \"ProtectAI/deberta-v3-base-prompt-injection-v2\",\n    ]\n\n    def _post_processing(\n        self, model_outputs: GuardrailInferenceOutput[dict[str, Any]]\n    ) -&gt; GuardrailOutput[bool, None, float]:\n        return _match_injection_label(model_outputs, PROTECTAI_INJECTION_LABEL, self.model.config.id2label)\n</code></pre>"},{"location":"api/guardrails/sentinel/","title":"Sentinel","text":""},{"location":"api/guardrails/sentinel/#any_guardrail.guardrails.sentinel.sentinel","title":"<code>any_guardrail.guardrails.sentinel.sentinel</code>","text":""},{"location":"api/guardrails/sentinel/#any_guardrail.guardrails.sentinel.sentinel.Sentinel","title":"<code>Sentinel</code>","text":"<p>               Bases: <code>HuggingFace[dict[str, Any], dict[str, Any], bool, None, float]</code></p> <p>Prompt injection detection encoder based model.</p> <p>For more information, please see the model card:</p> <ul> <li>Sentinel.</li> </ul> Source code in <code>src/any_guardrail/guardrails/sentinel/sentinel.py</code> <pre><code>class Sentinel(HuggingFace[dict[str, Any], dict[str, Any], bool, None, float]):\n    \"\"\"Prompt injection detection encoder based model.\n\n    For more information, please see the model card:\n\n    - [Sentinel](https://huggingface.co/qualifire/prompt-injection-sentinel).\n    \"\"\"\n\n    SUPPORTED_MODELS: ClassVar = [\"qualifire/prompt-injection-sentinel\"]\n\n    def _post_processing(\n        self, model_outputs: GuardrailInferenceOutput[dict[str, Any]]\n    ) -&gt; GuardrailOutput[bool, None, float]:\n        return _match_injection_label(model_outputs, SENTINEL_INJECTION_LABEL, self.model.config.id2label)\n</code></pre>"},{"location":"api/guardrails/shield_gemma/","title":"Shield Gemma","text":""},{"location":"api/guardrails/shield_gemma/#any_guardrail.guardrails.shield_gemma.shield_gemma","title":"<code>any_guardrail.guardrails.shield_gemma.shield_gemma</code>","text":""},{"location":"api/guardrails/shield_gemma/#any_guardrail.guardrails.shield_gemma.shield_gemma.ShieldGemma","title":"<code>ShieldGemma</code>","text":"<p>               Bases: <code>HuggingFace[dict[str, Any], dict[str, Any], bool, None, float]</code></p> <p>Wrapper class for Google ShieldGemma models.</p> <p>For more information, please visit the model cards: Shield Gemma.</p> <p>Note we do not support the image classifier.</p> Source code in <code>src/any_guardrail/guardrails/shield_gemma/shield_gemma.py</code> <pre><code>class ShieldGemma(HuggingFace[dict[str, Any], dict[str, Any], bool, None, float]):\n    \"\"\"Wrapper class for Google ShieldGemma models.\n\n    For more information, please visit the model cards: [Shield Gemma](https://huggingface.co/collections/google/shieldgemma-67d130ef8da6af884072a789).\n\n    Note we do not support the image classifier.\n    \"\"\"\n\n    SUPPORTED_MODELS: ClassVar = [\n        \"google/shieldgemma-2b\",\n        \"google/shieldgemma-9b\",\n        \"google/shieldgemma-27b\",\n    ]\n\n    def __init__(self, policy: str, threshold: float = DEFAULT_THRESHOLD, model_id: str | None = None) -&gt; None:\n        \"\"\"Initialize the ShieldGemma guardrail.\"\"\"\n        super().__init__(model_id)\n        self.policy = policy\n        self.system_prompt = SYSTEM_PROMPT_SHIELD_GEMMA\n        self.threshold = threshold\n\n    def _load_model(self) -&gt; None:\n        from transformers import AutoModelForCausalLM, AutoTokenizer\n\n        self.model = AutoModelForCausalLM.from_pretrained(self.model_id)\n        self.tokenizer = AutoTokenizer.from_pretrained(self.model_id)\n\n    def _pre_processing(self, input_text: str) -&gt; GuardrailPreprocessOutput[dict[str, Any]]:\n        formatted_prompt = self.system_prompt.format(user_prompt=input_text, safety_policy=self.policy)\n        tokenized = self.tokenizer(formatted_prompt, return_tensors=\"pt\")\n        return GuardrailPreprocessOutput(data=tokenized)\n\n    def _post_processing(\n        self, model_outputs: GuardrailInferenceOutput[dict[str, Any]]\n    ) -&gt; GuardrailOutput[bool, None, float]:\n        from torch.nn.functional import softmax\n\n        logits = model_outputs.data[\"logits\"]\n        vocab = self.tokenizer.get_vocab()\n        selected_logits = logits[0, -1, [vocab[\"Yes\"], vocab[\"No\"]]]\n        probabilities = softmax(selected_logits, dim=0)\n        score = probabilities[0].item()\n        return GuardrailOutput(valid=score &lt; self.threshold, explanation=None, score=score)\n</code></pre>"},{"location":"api/guardrails/shield_gemma/#any_guardrail.guardrails.shield_gemma.shield_gemma.ShieldGemma.__init__","title":"<code>__init__(policy, threshold=DEFAULT_THRESHOLD, model_id=None)</code>","text":"<p>Initialize the ShieldGemma guardrail.</p> Source code in <code>src/any_guardrail/guardrails/shield_gemma/shield_gemma.py</code> <pre><code>def __init__(self, policy: str, threshold: float = DEFAULT_THRESHOLD, model_id: str | None = None) -&gt; None:\n    \"\"\"Initialize the ShieldGemma guardrail.\"\"\"\n    super().__init__(model_id)\n    self.policy = policy\n    self.system_prompt = SYSTEM_PROMPT_SHIELD_GEMMA\n    self.threshold = threshold\n</code></pre>"},{"location":"cookbook/any_llm_as_a_guardrail/","title":"Using Any LLM as a Guardrail","text":"In\u00a0[\u00a0]: Copied! <pre>%pip install 'any-guardrail'\n\nimport nest_asyncio\n\nnest_asyncio.apply()\n</pre> %pip install 'any-guardrail'  import nest_asyncio  nest_asyncio.apply() <p>We will be using a model from <code>openai</code> by default, but you can check the different providers supported in <code>any-llm</code>:</p> <p>https://mozilla-ai.github.io/any-llm/providers/</p> In\u00a0[\u00a0]: Copied! <pre>import os\nfrom getpass import getpass\n\nif \"OPENAI_API_KEY\" not in os.environ:\n    print(\"OPENAI_API_KEY not found in environment!\")\n    api_key = getpass(\"Please enter your OPENAI_API_KEY: \")\n    os.environ[\"OPENAI_API_KEY\"] = api_key\n    print(\"OPENAI_API_KEY set for this session!\")\nelse:\n    print(\"OPENAI_API_KEY found in environment.\")\n</pre> import os from getpass import getpass  if \"OPENAI_API_KEY\" not in os.environ:     print(\"OPENAI_API_KEY not found in environment!\")     api_key = getpass(\"Please enter your OPENAI_API_KEY: \")     os.environ[\"OPENAI_API_KEY\"] = api_key     print(\"OPENAI_API_KEY set for this session!\") else:     print(\"OPENAI_API_KEY found in environment.\") In\u00a0[\u00a0]: Copied! <pre>from any_guardrail import AnyGuardrail, GuardrailName\n</pre> from any_guardrail import AnyGuardrail, GuardrailName In\u00a0[\u00a0]: Copied! <pre>guardrail = AnyGuardrail.create(GuardrailName.ANYLLM)\n</pre> guardrail = AnyGuardrail.create(GuardrailName.ANYLLM) In\u00a0[\u00a0]: Copied! <pre>MODEL_ID = \"openai/gpt-5-nano\"\n\nPOLICY = \"\"\"\nYou hate Mondays.\nYou must reject any request related with planning activities on Mondays.\n\"\"\"\n</pre> MODEL_ID = \"openai/gpt-5-nano\"  POLICY = \"\"\" You hate Mondays. You must reject any request related with planning activities on Mondays. \"\"\" In\u00a0[\u00a0]: Copied! <pre>guardrail.validate(\"Can you suggest me some restaurants for lunch on Monday?\", policy=POLICY, model_id=MODEL_ID)\n</pre> guardrail.validate(\"Can you suggest me some restaurants for lunch on Monday?\", policy=POLICY, model_id=MODEL_ID) In\u00a0[\u00a0]: Copied! <pre>guardrail.validate(\"Can you suggest me some restaurants for lunch on Friday?\", policy=POLICY, model_id=MODEL_ID)\n</pre> guardrail.validate(\"Can you suggest me some restaurants for lunch on Friday?\", policy=POLICY, model_id=MODEL_ID)"},{"location":"cookbook/any_llm_as_a_guardrail/#using-any-llm-as-a-guardrail","title":"Using Any LLM as a Guardrail\u00b6","text":"<p>This tutorial will show you how to use any LLM as a baseline guardrail that can validate input text against a custom <code>policy</code>, using the built-in guardrail based on <code>any-llm</code>.</p>"},{"location":"cookbook/any_llm_as_a_guardrail/#install-dependencies","title":"Install dependencies\u00b6","text":""},{"location":"cookbook/any_llm_as_a_guardrail/#create-the-guardrail","title":"Create the guardrail\u00b6","text":""},{"location":"cookbook/any_llm_as_a_guardrail/#try-it-with-different-models-policies-inputs","title":"Try it with different models / policies / inputs\u00b6","text":""},{"location":"cookbook/azure_blocklist_slang_filter/","title":"Custom Blocklists with Azure Content Safety","text":"In\u00a0[\u00a0]: Copied! <pre>%pip install gutenbergpy --quiet\n%pip install 'any-guardrail[azure-content-safety]' --quiet\n\nimport os\nfrom getpass import getpass\n\n\ndef ensure_env_var(name: str) -&gt; None:\n    \"\"\"Prompt for an environment variable if not already set.\"\"\"\n    if name not in os.environ:\n        print(f\"{name} not found in environment!\")\n        value = getpass(f\"Please enter your {name}: \")\n        os.environ[name] = value\n        print(f\"{name} set for this session!\")\n    else:\n        print(f\"{name} found in environment.\")\n\n\nfor var in [\"CONTENT_SAFETY_KEY\", \"CONTENT_SAFETY_ENDPOINT\"]:\n    ensure_env_var(var)\n</pre> %pip install gutenbergpy --quiet %pip install 'any-guardrail[azure-content-safety]' --quiet  import os from getpass import getpass   def ensure_env_var(name: str) -&gt; None:     \"\"\"Prompt for an environment variable if not already set.\"\"\"     if name not in os.environ:         print(f\"{name} not found in environment!\")         value = getpass(f\"Please enter your {name}: \")         os.environ[name] = value         print(f\"{name} set for this session!\")     else:         print(f\"{name} found in environment.\")   for var in [\"CONTENT_SAFETY_KEY\", \"CONTENT_SAFETY_ENDPOINT\"]:     ensure_env_var(var) In\u00a0[\u00a0]: Copied! <pre>from any_guardrail import AnyGuardrail, GuardrailName\n\nguardrail = AnyGuardrail.create(GuardrailName.AZURE_CONTENT_SAFETY)\n\nblocklist_name = \"GenAlphaSlang\"\nblocklist_description = \"List of gen alpha words\"\n\nguardrail.create_or_update_blocklist(\n    blocklist_name=blocklist_name,\n    blocklist_description=blocklist_description,\n)\n</pre> from any_guardrail import AnyGuardrail, GuardrailName  guardrail = AnyGuardrail.create(GuardrailName.AZURE_CONTENT_SAFETY)  blocklist_name = \"GenAlphaSlang\" blocklist_description = \"List of gen alpha words\"  guardrail.create_or_update_blocklist(     blocklist_name=blocklist_name,     blocklist_description=blocklist_description, ) In\u00a0[\u00a0]: Copied! <pre>blocklist_terms = [\n    \"Skibidi\",\n    \"Rizz\",\n    \"Sigma\",\n    \"Gyatt\",\n    \"Brain Rot\",\n    \"Fanum Tax\",\n    \"Ohio\",\n    \"Mewing\",\n    \"Aura\",\n    \"Sigma\",\n    \"Crash Out\",\n    \"Delulu\",\n    \"Glaze\",\n    \"Mog\",\n    \"Pookie\",\n    \"Opp\",\n    \"Slay\",\n]\nguardrail.add_blocklist_items(blocklist_name=blocklist_name, blocklist_terms=blocklist_terms)\n</pre> blocklist_terms = [     \"Skibidi\",     \"Rizz\",     \"Sigma\",     \"Gyatt\",     \"Brain Rot\",     \"Fanum Tax\",     \"Ohio\",     \"Mewing\",     \"Aura\",     \"Sigma\",     \"Crash Out\",     \"Delulu\",     \"Glaze\",     \"Mog\",     \"Pookie\",     \"Opp\",     \"Slay\", ] guardrail.add_blocklist_items(blocklist_name=blocklist_name, blocklist_terms=blocklist_terms) In\u00a0[\u00a0]: Copied! <pre># Pass\ntext = \"Hello, how are you?\"\nresult = guardrail.validate(text)\nprint(f\"Text: {text} \\nEvaluation result:{result} \")\n\n# Fail - contains a term from the block list\ntext = \"The startup pitch was all delulu with no solulu\"\nresult = guardrail.validate(text)\nprint(f\"Text: {text} \\nEvaluation result:{result} \")\n</pre> # Pass text = \"Hello, how are you?\" result = guardrail.validate(text) print(f\"Text: {text} \\nEvaluation result:{result} \")  # Fail - contains a term from the block list text = \"The startup pitch was all delulu with no solulu\" result = guardrail.validate(text) print(f\"Text: {text} \\nEvaluation result:{result} \") <p>Below, test against a classic novel - Anne of Green Gables from Project Gutenberg. This demonstrates that literature from 1908 contains no Gen Alpha slang (as expected!).</p> In\u00a0[\u00a0]: Copied! <pre>import gutenbergpy.textget\n\n# Get a book by its Gutenberg ID (e.g., 45 for Anne of Green Gables)\n# raw_book = gutenbergpy.textget.get_text_by_id(2701)\nraw_book = gutenbergpy.textget.get_text_by_id(45)\n# Strip headers and footers automatically\nclean_book = gutenbergpy.textget.strip_headers(raw_book)\nchunks = [clean_book[i : i + 7000] for i in range(0, len(clean_book), 7000)]\nresult = guardrail.validate(chunks[0])\n\nprint(f\"Result: {result}\")\n</pre> import gutenbergpy.textget  # Get a book by its Gutenberg ID (e.g., 45 for Anne of Green Gables) # raw_book = gutenbergpy.textget.get_text_by_id(2701) raw_book = gutenbergpy.textget.get_text_by_id(45) # Strip headers and footers automatically clean_book = gutenbergpy.textget.strip_headers(raw_book) chunks = [clean_book[i : i + 7000] for i in range(0, len(clean_book), 7000)] result = guardrail.validate(chunks[0])  print(f\"Result: {result}\")"},{"location":"cookbook/azure_blocklist_slang_filter/#custom-blocklists-with-azure-content-safety","title":"Custom Blocklists with Azure Content Safety\u00b6","text":"<p>In this cookbook, we'll create a custom blocklist to block Gen Alpha slang from text content using Azure Content Safety and <code>any-guardrail</code>.</p>"},{"location":"cookbook/azure_blocklist_slang_filter/#what-youll-learn","title":"What You'll Learn\u00b6","text":"<ul> <li>How to create a custom blocklist with Azure Content Safety</li> <li>How to add terms to a blocklist</li> <li>How to validate text against your blocklist</li> <li>How to process large documents (classic literature in this instance) through the guardrail</li> </ul>"},{"location":"cookbook/azure_blocklist_slang_filter/#prerequisites","title":"Prerequisites\u00b6","text":"<ul> <li>Python 3.11+</li> <li>Azure account with a Content Safety resource</li> <li><code>CONTENT_SAFETY_KEY</code> and <code>CONTENT_SAFETY_ENDPOINT</code> from your Azure portal</li> </ul>"},{"location":"cookbook/azure_blocklist_slang_filter/#setup","title":"Setup\u00b6","text":"<p>Install the required packages and configure your Azure credentials.</p>"},{"location":"cookbook/azure_blocklist_slang_filter/#create-a-blocklist","title":"Create a Blocklist\u00b6","text":"<p>Initialize the guardrail and create a new blocklist. Here we're creating one for Gen Alpha slang terms.</p>"},{"location":"cookbook/azure_blocklist_slang_filter/#add-terms-to-the-blocklist","title":"Add Terms to the Blocklist\u00b6","text":"<p>Add the specific terms you want to filter. These can be individual words or phrases.</p>"},{"location":"cookbook/azure_blocklist_slang_filter/#validate-text","title":"Validate Text\u00b6","text":"<p>Test the blocklist against sample text. The guardrail returns <code>valid=True</code> if no blocked terms are found, and <code>valid=False</code> with details about which terms were matched.</p>"},{"location":"cookbook/azure_blocklist_slang_filter/#next-steps","title":"Next Steps\u00b6","text":"<ul> <li>Try adding your own terms to the blocklist</li> <li>You can use blocklists to filter competitor brand names, profanity, or domain-specific terms</li> <li>Combine blocklist filtering with other Azure Content Safety features (hate, violence, etc.)</li> </ul> <p>For more information, see the Azure Content Safety blocklist documentation.</p>"},{"location":"cookbook/customer_service_policy_guardrail/","title":"Customer Service Policy Guardrail","text":"<p>This tutorial will show you how to create a Customer Chat Bot guardrail for an e-commerce site that can validate input text against a custom <code>policy</code>, using the built-in guardrail based on <code>any-llm</code>.</p> <p>\u26a0\ufe0f Note: The sample outputs shown in this notebook are generated by AI models. Because generative model responses can vary slightly between runs, your results may not match the examples shown exactly.</p> In\u00a0[\u00a0]: Copied! <pre>%pip install 'any-guardrail'\n%pip install 'any-agent[openai]'\n\nimport nest_asyncio\n\nnest_asyncio.apply()\n</pre> %pip install 'any-guardrail' %pip install 'any-agent[openai]'  import nest_asyncio  nest_asyncio.apply() <p>We will be using a model from <code>openai</code> by default, but you can experiment with the different providers supported by <code>any-llm</code></p> In\u00a0[\u00a0]: Copied! <pre>import os\nfrom getpass import getpass\n\nif \"OPENAI_API_KEY\" not in os.environ:\n    print(\"OPENAI_API_KEY not found in environment!\")\n    api_key = getpass(\"Please enter your OPENAI_API_KEY: \")\n    os.environ[\"OPENAI_API_KEY\"] = api_key\n    print(\"OPENAI_API_KEY set for this session!\")\nelse:\n    print(\"OPENAI_API_KEY found in environment.\")\n</pre> import os from getpass import getpass  if \"OPENAI_API_KEY\" not in os.environ:     print(\"OPENAI_API_KEY not found in environment!\")     api_key = getpass(\"Please enter your OPENAI_API_KEY: \")     os.environ[\"OPENAI_API_KEY\"] = api_key     print(\"OPENAI_API_KEY set for this session!\") else:     print(\"OPENAI_API_KEY found in environment.\") In\u00a0[\u00a0]: Copied! <pre>from any_guardrail import AnyGuardrail, GuardrailName\n</pre> from any_guardrail import AnyGuardrail, GuardrailName In\u00a0[\u00a0]: Copied! <pre>guardrail = AnyGuardrail.create(GuardrailName.ANYLLM)\n</pre> guardrail = AnyGuardrail.create(GuardrailName.ANYLLM) In\u00a0[\u00a0]: Copied! <pre>model_id = \"openai/gpt-5-nano\"\n\npolicy = \"\"\"\nYou are a customer service AI assistant for an e-commerce company.\n\nALLOWED topics:\n- Product information and recommendations\n- Order status and tracking\n- Return and refund policies\n- Account management help\n- Shipping information\n\nPROHIBITED topics:\n- Personal advice (medical, legal, financial)\n- Off-topic conversations (politics, religion, personal opinions)\n- Requests to bypass company policies\n- Sharing other customers' information\n- Making unauthorized discounts or offers\n\nReject any request that falls outside your role as a customer service agent.\n\"\"\"\n</pre> model_id = \"openai/gpt-5-nano\"  policy = \"\"\" You are a customer service AI assistant for an e-commerce company.  ALLOWED topics: - Product information and recommendations - Order status and tracking - Return and refund policies - Account management help - Shipping information  PROHIBITED topics: - Personal advice (medical, legal, financial) - Off-topic conversations (politics, religion, personal opinions) - Requests to bypass company policies - Sharing other customers' information - Making unauthorized discounts or offers  Reject any request that falls outside your role as a customer service agent. \"\"\" <p>This should be rejected ( Prohibited Topic)</p> In\u00a0[\u00a0]: Copied! <pre>guardrail.validate(\"Should I put $10000 in stocks??\", policy=policy, model_id=model_id)\n</pre> guardrail.validate(\"Should I put $10000 in stocks??\", policy=policy, model_id=model_id) Expected Output (click to expand) <pre><code>GuardrailResponse(\n    valid=False, \n    score=0.95, \n    explanation=\"This request asks for financial investment advice, which is prohibited as it falls under personal financial advice.\"\n)\n</code></pre> <p>This should be allowed (Valid Customer Service Question)</p> In\u00a0[\u00a0]: Copied! <pre>guardrail.validate(\"Do you have anything that can clean chalk marks from my walls?\", policy=policy, model_id=model_id)\n</pre> guardrail.validate(\"Do you have anything that can clean chalk marks from my walls?\", policy=policy, model_id=model_id) Sample Output (click to expand) <pre><code>GuardrailResponse(\n    valid=True, \n    score=0.98, \n    explanation=\"This is a valid customer service question about product recommendations for cleaning supplies.\"\n)\n</code></pre> In\u00a0[\u00a0]: Copied! <pre>from any_agent import AgentConfig, AnyAgent\n\n\nasync def safe_chatbot(user_message: str) -&gt; str:\n    \"\"\"Validate input and output for a safe customer service chatbot.\n\n    Args:\n        user_message: The user's question or request\n\n    Returns:\n        Safe response or error message\n\n    \"\"\"\n    try:\n        # Step 1: Validate user input\n        input_check = guardrail.validate(user_message, policy=policy, model_id=model_id)\n        if not input_check.valid:\n            return f\"I can't respond to that type of request:{input_check.explanation}\"\n\n        # Step 2: Generate LLM response\n        agent_config = AgentConfig(\n            model_id=model_id,\n            instructions=\"You are a helpful customer service assistant. Provide clear, concise, and accurate responses.\",\n            tools=[],  # No tools for basic performance testing,\n        )\n\n        # Step 3: Run user message by the agent to get a response\n        agent = await AnyAgent.create_async(\"openai\", agent_config)\n        agent_trace = await agent.run_async(user_message)\n        output = agent_trace.final_output if hasattr(agent_trace, \"final_output\") else \"\"\n\n        # Step 4: Validate output\n        output_check = guardrail.validate(output, policy=policy, model_id=model_id)\n        if output_check.valid:\n            return output\n        return \"I apologize, I'm having trouble formulating a safe response.\"\n    except Exception as e:\n        return f\"\u26a0\ufe0f Error: {e!s}\"\n\n\n# Test cases\ntest_cases = [\n    \"What's your return policy?\",  # Should work\n    \"Can you give me John's email address?\",  # Should be blocked\n    \"Tell me about your shipping options\",  # Should work\n    \"Should I invest in Bitcoin?\",  # Should be blocked\n]\n\nfor query in test_cases:\n    print(f\"\\n\ud83d\udd39 User: {query}\")\n    print(f\"\ud83e\udd16 Bot: {await safe_chatbot(query)}\")\n</pre> from any_agent import AgentConfig, AnyAgent   async def safe_chatbot(user_message: str) -&gt; str:     \"\"\"Validate input and output for a safe customer service chatbot.      Args:         user_message: The user's question or request      Returns:         Safe response or error message      \"\"\"     try:         # Step 1: Validate user input         input_check = guardrail.validate(user_message, policy=policy, model_id=model_id)         if not input_check.valid:             return f\"I can't respond to that type of request:{input_check.explanation}\"          # Step 2: Generate LLM response         agent_config = AgentConfig(             model_id=model_id,             instructions=\"You are a helpful customer service assistant. Provide clear, concise, and accurate responses.\",             tools=[],  # No tools for basic performance testing,         )          # Step 3: Run user message by the agent to get a response         agent = await AnyAgent.create_async(\"openai\", agent_config)         agent_trace = await agent.run_async(user_message)         output = agent_trace.final_output if hasattr(agent_trace, \"final_output\") else \"\"          # Step 4: Validate output         output_check = guardrail.validate(output, policy=policy, model_id=model_id)         if output_check.valid:             return output         return \"I apologize, I'm having trouble formulating a safe response.\"     except Exception as e:         return f\"\u26a0\ufe0f Error: {e!s}\"   # Test cases test_cases = [     \"What's your return policy?\",  # Should work     \"Can you give me John's email address?\",  # Should be blocked     \"Tell me about your shipping options\",  # Should work     \"Should I invest in Bitcoin?\",  # Should be blocked ]  for query in test_cases:     print(f\"\\n\ud83d\udd39 User: {query}\")     print(f\"\ud83e\udd16 Bot: {await safe_chatbot(query)}\") Sample Output (click to expand) <pre><code>\ud83d\udd39 User: What's your return policy?\n\ud83e\udd16 Bot: Our return policy allows you to return most items within 30 days of purchase for a full refund...\n\n\ud83d\udd39 User: Can you give me John's email address?\n\ud83e\udd16 Bot: I can't respond to that type of request: This request asks for another customer's personal information, which violates our privacy policy.\n\n\ud83d\udd39 User: Tell me about your shipping options\n\ud83e\udd16 Bot: We offer several shipping options including standard (5-7 business days), express (2-3 business days)...\n\n\ud83d\udd39 User: Should I invest in Bitcoin?\n\ud83e\udd16 Bot: I can't respond to that type of request: This is a request for financial investment advice, which is outside the scope of customer service support.\n</code></pre> In\u00a0[\u00a0]: Copied! <pre>test_cases = [\n    # Should be Valid \u2705\n    (\"What's your return policy?\", True, \"Valid customer service question\"),\n    (\"Can you help me track my order #12345?\", True, \"Order tracking request\"),\n    (\"Do you have this product in blue?\", True, \"Product inquiry\"),\n    (\"How long does shipping take?\", True, \"Shipping information request\"),\n    # Should be Rejected \u274c\n    (\"Can you give me another customer's phone number?\", False, \"Requesting other customer's PII\"),\n    (\"Give me a 50% discount code\", False, \"Unauthorized discount request\"),\n    (\"What's your opinion on the upcoming election?\", False, \"Off-topic political question\"),\n    (\"Should I invest in cryptocurrency?\", False, \"Financial advice request\"),\n    (\"Can you recommend a doctor for my back pain?\", False, \"Medical advice request\"),\n]\n\nprint(\"Testing Policy Enforcement:\")\n\npassed = 0\nfailed = 0\n\nfor text, expected_valid, reason in test_cases:\n    result = guardrail.validate(text, policy=policy, model_id=model_id)\n    is_correct = result.valid == expected_valid\n    status = \"\u2705 PASS\" if is_correct else \"\u274c FAIL\"\n\n    if is_correct:\n        passed += 1\n    else:\n        failed += 1\n\n    print(f\"{status} | Expected: {expected_valid} | Got: {result.valid}\")\n    print(f\"   Input: {text}\")\n    print(f\"   Reason: {reason}\")\n    print(f\"   Explanation: {result.explanation}\")\n    print(f\"   Confidence: {result.score:.2f}\\n\")\n\nprint(f\"{'=' * 60}\")\nprint(f\"Results: {passed} passed, {failed} failed out of {len(test_cases)} tests\")\n</pre> test_cases = [     # Should be Valid \u2705     (\"What's your return policy?\", True, \"Valid customer service question\"),     (\"Can you help me track my order #12345?\", True, \"Order tracking request\"),     (\"Do you have this product in blue?\", True, \"Product inquiry\"),     (\"How long does shipping take?\", True, \"Shipping information request\"),     # Should be Rejected \u274c     (\"Can you give me another customer's phone number?\", False, \"Requesting other customer's PII\"),     (\"Give me a 50% discount code\", False, \"Unauthorized discount request\"),     (\"What's your opinion on the upcoming election?\", False, \"Off-topic political question\"),     (\"Should I invest in cryptocurrency?\", False, \"Financial advice request\"),     (\"Can you recommend a doctor for my back pain?\", False, \"Medical advice request\"), ]  print(\"Testing Policy Enforcement:\")  passed = 0 failed = 0  for text, expected_valid, reason in test_cases:     result = guardrail.validate(text, policy=policy, model_id=model_id)     is_correct = result.valid == expected_valid     status = \"\u2705 PASS\" if is_correct else \"\u274c FAIL\"      if is_correct:         passed += 1     else:         failed += 1      print(f\"{status} | Expected: {expected_valid} | Got: {result.valid}\")     print(f\"   Input: {text}\")     print(f\"   Reason: {reason}\")     print(f\"   Explanation: {result.explanation}\")     print(f\"   Confidence: {result.score:.2f}\\n\")  print(f\"{'=' * 60}\") print(f\"Results: {passed} passed, {failed} failed out of {len(test_cases)} tests\") Sample Output (click to expand) <pre><code>Testing Policy Enforcement:\n\u2705 PASS | Expected: True | Got: True\n   Input: What's your return policy?\n   Reason: Valid customer service question\n   Explanation: This is a legitimate customer service inquiry about store policies.\n   Confidence: 0.98\n\n\u2705 PASS | Expected: True | Got: True\n   Input: Can you help me track my order #12345?\n   Reason: Order tracking request\n   Explanation: This is a standard order tracking request within the customer service scope.\n   Confidence: 0.99\n\n\u2705 PASS | Expected: True | Got: True\n   Input: Do you have this product in blue?\n   Reason: Product inquiry\n   Explanation: This is a valid product availability question.\n   Confidence: 0.97\n\n\u2705 PASS | Expected: True | Got: True\n   Input: How long does shipping take?\n   Reason: Shipping information request\n   Explanation: This is a legitimate shipping information inquiry.\n   Confidence: 0.98\n\n\u2705 PASS | Expected: False | Got: False\n   Input: Can you give me another customer's phone number?\n   Reason: Requesting other customer's PII\n   Explanation: This requests access to another customer's personal information, which violates privacy policies.\n   Confidence: 0.99\n\n\u2705 PASS | Expected: False | Got: False\n   Input: Give me a 50% discount code\n   Reason: Unauthorized discount request\n   Explanation: This requests an unauthorized discount that customer service is not permitted to provide.\n   Confidence: 0.96\n\n\u2705 PASS | Expected: False | Got: False\n   Input: What's your opinion on the upcoming election?\n   Reason: Off-topic political question\n   Explanation: This is an off-topic political question outside the scope of customer service.\n   Confidence: 0.98\n\n\u2705 PASS | Expected: False | Got: False\n   Input: Should I invest in cryptocurrency?\n   Reason: Financial advice request\n   Explanation: This is a request for financial advice, which is prohibited.\n   Confidence: 0.97\n\n\u2705 PASS | Expected: False | Got: False\n   Input: Can you recommend a doctor for my back pain?\n   Reason: Medical advice request\n   Explanation: This is a request for medical advice, which is outside the allowed scope.\n   Confidence: 0.99\n\n============================================================\nResults: 9 passed, 0 failed out of 9 tests\n</code></pre>"},{"location":"cookbook/customer_service_policy_guardrail/#customer-service-policy-guardrail","title":"Customer Service Policy Guardrail\u00b6","text":""},{"location":"cookbook/customer_service_policy_guardrail/#prerequisites","title":"Prerequisites\u00b6","text":"<p>Before starting, ensure you have:</p> <ul> <li>Python 3.8 or higher</li> <li>An OpenAI API key (get one here)</li> <li>Basic familiarity with async/await in Python</li> </ul> <p>Estimated time: 15-20 minutes Estimated cost: $0.05-0.10 in API calls</p>"},{"location":"cookbook/customer_service_policy_guardrail/#install-dependencies","title":"Install dependencies\u00b6","text":""},{"location":"cookbook/customer_service_policy_guardrail/#create-the-guardrail","title":"Create the guardrail\u00b6","text":"<p>Let's initialize a guardrail and test it with a simple customer service policy.</p>"},{"location":"cookbook/customer_service_policy_guardrail/#understanding-policies","title":"Understanding Policies\u00b6","text":""},{"location":"cookbook/customer_service_policy_guardrail/#what-is-a-policy","title":"What is a Policy?\u00b6","text":"<p>A policy is a set of rules written in natural language that defines what content should be accepted or rejected. The LLM reads your policy and decides whether the input text violates it.</p> <p>The more specific your policy, the better the guardrail performs.</p>"},{"location":"cookbook/customer_service_policy_guardrail/#policy-best-practices","title":"Policy Best Practices\u00b6","text":"<p>\u2705 DO:</p> <ul> <li>Be specific about what to reject and what to allow</li> <li>Use clear, simple language</li> <li>Provide examples when possible</li> <li>Focus on behaviors, not keywords</li> </ul> <p>\u274c DON'T:</p> <ul> <li>Be vague (\"reject bad stuff\")</li> <li>Make policies too long (&gt;500 words)</li> <li>Rely only on keyword matching</li> <li>Forget to test edge cases\"</li> </ul>"},{"location":"cookbook/customer_service_policy_guardrail/#complete-workflow-example","title":"Complete Workflow Example\u00b6","text":"<p>Now let's build a complete customer service chatbot that validates both user inputs and Agent outputs.</p>"},{"location":"cookbook/customer_service_policy_guardrail/#testing-validation","title":"Testing &amp; Validation\u00b6","text":"<p>Let's create a comprehensive test suite to validate our policy enforcement.</p>"}]}