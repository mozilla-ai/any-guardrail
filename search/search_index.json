{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Intro","text":"<p><code>any-guardrail</code> is a Python library providing a single interface to different guardrails.</p>"},{"location":"#getting-started","title":"Getting Started","text":"<p>Refer to the Quickstart for instructions on installation and usage.</p>"},{"location":"#guardrails","title":"Guardrails","text":"<p>Refer to Guardrails for the parameters for each guardrail.</p> <p>Refer to AnyGuardrail for how to use the <code>AnyGuardrail</code> object.</p>"},{"location":"#for-ai-systems","title":"For AI Systems","text":"<p>This documentation is available in two AI-friendly formats:</p> <ul> <li>llms.txt - A structured overview with curated links to key documentation sections</li> <li>llms-full.txt - Complete documentation content concatenated into a single file</li> </ul>"},{"location":"quickstart/","title":"Quick Start","text":""},{"location":"quickstart/#requirements","title":"Requirements","text":"<ul> <li>Python 3.11 or newer</li> </ul>"},{"location":"quickstart/#installation","title":"Installation","text":"<p>You can install the bare bones library as follows (only [<code>any_guardrails.guardrails.any_llm.AnyLlm</code>] will be available):</p> <pre><code>pip install any-guardrail\n</code></pre> <p>Or you can install it with the required dependencies for different guardrails:</p> <pre><code>pip install any-guardrail[huggingface]\n</code></pre> <p>Refer to pyproject.toml for a list of the options available.</p>"},{"location":"quickstart/#basic-usage","title":"Basic Usage","text":"<p><code>AnyGuardrail</code> provides a seamless interface for interacting with the guardrail models. It allows you to see a list of all the supported guardrails, and to instantiate each supported guardrails. Here is a full example:</p> <pre><code>from any_guardrail import AnyGuardrail, GuardrailName, GuardrailOutput\n\nguardrail = AnyGuardrail.create(GuardrailName.DEEPSET)\n\nresult: GuardrailOutput = guardrail.validate(\"All smiles from me!\")\n\nassert result.valid\n</code></pre>"},{"location":"quickstart/#troubleshooting","title":"Troubleshooting","text":"<p>Some of the models on HuggingFace require extra permissions to use. To do this, you'll need to create a HuggingFace profile and manually go through the permissions. Then, you'll need to download the HuggingFace Hub and login. One way to do this is:</p> <pre><code>pip install --upgrade huggingface_hub\n\nhf auth login\n</code></pre> <p>More information can be found here: HuggingFace Hub</p>"},{"location":"api/any_guardrail/","title":"AnyGuardrail","text":""},{"location":"api/any_guardrail/#anyguardrail","title":"AnyGuardrail","text":""},{"location":"api/any_guardrail/#any_guardrail.api.AnyGuardrail","title":"<code>any_guardrail.api.AnyGuardrail</code>","text":"<p>Factory class for creating guardrail instances.</p> Source code in <code>src/any_guardrail/api.py</code> <pre><code>class AnyGuardrail:\n    \"\"\"Factory class for creating guardrail instances.\"\"\"\n\n    @classmethod\n    def get_supported_guardrails(cls) -&gt; list[GuardrailName]:\n        \"\"\"List all supported guardrails.\"\"\"\n        return list(GuardrailName)\n\n    @classmethod\n    def get_supported_model(cls, guardrail_name: GuardrailName) -&gt; list[str]:\n        \"\"\"Get the model IDs supported by a specific guardrail.\"\"\"\n        guardrail_class = cls._get_guardrail_class(guardrail_name)\n        return guardrail_class.SUPPORTED_MODELS\n\n    @classmethod\n    def get_all_supported_models(cls) -&gt; dict[str, list[str]]:\n        \"\"\"Get all model IDs supported by all guardrails.\"\"\"\n        model_ids = {}\n        for guardrail_name in cls.get_supported_guardrails():\n            model_ids[guardrail_name.value] = cls.get_supported_model(guardrail_name)\n        return model_ids\n\n    @classmethod\n    def create(cls, guardrail_name: GuardrailName, **kwargs: Any) -&gt; Guardrail:\n        \"\"\"Create a guardrail instance.\n\n        Args:\n            guardrail_name: The name of the guardrail to use.\n            **kwargs: Additional keyword arguments to pass to the guardrail constructor.\n\n        Returns:\n            A guardrail instance.\n\n        \"\"\"\n        guardrail_class = cls._get_guardrail_class(guardrail_name)\n        return guardrail_class(**kwargs)\n\n    @classmethod\n    def _get_guardrail_class(cls, guardrail_name: GuardrailName) -&gt; type[Guardrail]:\n        guardrail_module_name = f\"{guardrail_name.value}\"\n        module_path = f\"any_guardrail.guardrails.{guardrail_module_name}.{guardrail_module_name}\"\n\n        module = importlib.import_module(module_path)\n        parts = re.split(r\"[^A-Za-z0-9]+\", guardrail_module_name)\n        candidate_name = \"\".join(p.capitalize() for p in parts if p)\n        guardrail_class = getattr(module, candidate_name, None)\n        if inspect.isclass(guardrail_class) and issubclass(guardrail_class, Guardrail):\n            return guardrail_class\n        msg = f\"Could not resolve guardrail class for '{guardrail_module_name}' in {module.__name__}\"\n        raise ImportError(msg)\n</code></pre>"},{"location":"api/any_guardrail/#any_guardrail.api.AnyGuardrail.create","title":"<code>create(guardrail_name, **kwargs)</code>  <code>classmethod</code>","text":"<p>Create a guardrail instance.</p> <p>Parameters:</p> Name Type Description Default <code>guardrail_name</code> <code>GuardrailName</code> <p>The name of the guardrail to use.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the guardrail constructor.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Guardrail</code> <p>A guardrail instance.</p> Source code in <code>src/any_guardrail/api.py</code> <pre><code>@classmethod\ndef create(cls, guardrail_name: GuardrailName, **kwargs: Any) -&gt; Guardrail:\n    \"\"\"Create a guardrail instance.\n\n    Args:\n        guardrail_name: The name of the guardrail to use.\n        **kwargs: Additional keyword arguments to pass to the guardrail constructor.\n\n    Returns:\n        A guardrail instance.\n\n    \"\"\"\n    guardrail_class = cls._get_guardrail_class(guardrail_name)\n    return guardrail_class(**kwargs)\n</code></pre>"},{"location":"api/any_guardrail/#any_guardrail.api.AnyGuardrail.get_all_supported_models","title":"<code>get_all_supported_models()</code>  <code>classmethod</code>","text":"<p>Get all model IDs supported by all guardrails.</p> Source code in <code>src/any_guardrail/api.py</code> <pre><code>@classmethod\ndef get_all_supported_models(cls) -&gt; dict[str, list[str]]:\n    \"\"\"Get all model IDs supported by all guardrails.\"\"\"\n    model_ids = {}\n    for guardrail_name in cls.get_supported_guardrails():\n        model_ids[guardrail_name.value] = cls.get_supported_model(guardrail_name)\n    return model_ids\n</code></pre>"},{"location":"api/any_guardrail/#any_guardrail.api.AnyGuardrail.get_supported_guardrails","title":"<code>get_supported_guardrails()</code>  <code>classmethod</code>","text":"<p>List all supported guardrails.</p> Source code in <code>src/any_guardrail/api.py</code> <pre><code>@classmethod\ndef get_supported_guardrails(cls) -&gt; list[GuardrailName]:\n    \"\"\"List all supported guardrails.\"\"\"\n    return list(GuardrailName)\n</code></pre>"},{"location":"api/any_guardrail/#any_guardrail.api.AnyGuardrail.get_supported_model","title":"<code>get_supported_model(guardrail_name)</code>  <code>classmethod</code>","text":"<p>Get the model IDs supported by a specific guardrail.</p> Source code in <code>src/any_guardrail/api.py</code> <pre><code>@classmethod\ndef get_supported_model(cls, guardrail_name: GuardrailName) -&gt; list[str]:\n    \"\"\"Get the model IDs supported by a specific guardrail.\"\"\"\n    guardrail_class = cls._get_guardrail_class(guardrail_name)\n    return guardrail_class.SUPPORTED_MODELS\n</code></pre>"},{"location":"api/guardrails/","title":"Index","text":""},{"location":"api/guardrails/#guardrails","title":"Guardrails","text":"<p>This section documents the available guardrails and their parameters. Select a guardrail to view its API details.</p>"},{"location":"api/guardrails/any_llm/","title":"AnyLLM","text":""},{"location":"api/guardrails/any_llm/#any_guardrail.guardrails.any_llm.any_llm","title":"<code>any_guardrail.guardrails.any_llm.any_llm</code>","text":""},{"location":"api/guardrails/any_llm/#any_guardrail.guardrails.any_llm.any_llm.DEFAULT_MODEL_ID","title":"<code>DEFAULT_MODEL_ID = 'openai/gpt-5-nano'</code>  <code>module-attribute</code>","text":"<p>Will be used as default argument for <code>model_id</code></p>"},{"location":"api/guardrails/any_llm/#any_guardrail.guardrails.any_llm.any_llm.DEFAULT_SYSTEM_PROMPT","title":"<code>DEFAULT_SYSTEM_PROMPT = \"\\nYou are a guardrail designed to ensure that the input text adheres to a specific policy.\\nYour only task is to validate the input_text, don't try to answer the user query.\\n\\nHere is the policy: {policy}\\n\\nYou must return the following:\\n\\n- valid: bool\\n    If the input text provided by the user doesn't adhere to the policy, you must reject it (mark it as valid=False).\\n\\n- explanation: str\\n    A clear explanation of why the input text was rejected or not.\\n\\n- score: float (0-1)\\n    How confident you are about the validation.\\n\"</code>  <code>module-attribute</code>","text":"<p>Will be used as default argument for <code>system_prompt</code></p>"},{"location":"api/guardrails/any_llm/#any_guardrail.guardrails.any_llm.any_llm.AnyLlm","title":"<code>AnyLlm</code>","text":"<p>               Bases: <code>Guardrail</code></p> <p>A guardrail using <code>any-llm</code>.</p> Source code in <code>src/any_guardrail/guardrails/any_llm/any_llm.py</code> <pre><code>class AnyLlm(Guardrail):\n    \"\"\"A guardrail using `any-llm`.\"\"\"\n\n    def validate(\n        self,\n        input_text: str,\n        policy: str,\n        model_id: str = DEFAULT_MODEL_ID,\n        system_prompt: str = DEFAULT_SYSTEM_PROMPT,\n        **kwargs: Any,\n    ) -&gt; GuardrailOutput:\n        \"\"\"Validate the `input_text` against the given `policy`.\n\n        Args:\n            input_text (str): The text to validate.\n            policy (str): The policy to validate against.\n            model_id (str, optional): The model ID to use.\n            system_prompt (str, optional): The system prompt to use.\n                Expected to have a `{policy}` placeholder.\n            **kwargs: Additional keyword arguments to pass to `any_llm.completion` function.\n\n        Returns:\n            GuardrailOutput: The output of the validation.\n\n        \"\"\"\n        result: ChatCompletion = completion(  # type: ignore[assignment]\n            model=model_id,\n            messages=[\n                {\"role\": \"system\", \"content\": system_prompt.format(policy=policy)},\n                {\"role\": \"user\", \"content\": input_text},\n            ],\n            response_format=GuardrailOutput,\n            **kwargs,\n        )\n        return GuardrailOutput(**json.loads(result.choices[0].message.content))  # type: ignore[arg-type]\n</code></pre>"},{"location":"api/guardrails/any_llm/#any_guardrail.guardrails.any_llm.any_llm.AnyLlm.validate","title":"<code>validate(input_text, policy, model_id=DEFAULT_MODEL_ID, system_prompt=DEFAULT_SYSTEM_PROMPT, **kwargs)</code>","text":"<p>Validate the <code>input_text</code> against the given <code>policy</code>.</p> <p>Parameters:</p> Name Type Description Default <code>input_text</code> <code>str</code> <p>The text to validate.</p> required <code>policy</code> <code>str</code> <p>The policy to validate against.</p> required <code>model_id</code> <code>str</code> <p>The model ID to use.</p> <code>DEFAULT_MODEL_ID</code> <code>system_prompt</code> <code>str</code> <p>The system prompt to use. Expected to have a <code>{policy}</code> placeholder.</p> <code>DEFAULT_SYSTEM_PROMPT</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to <code>any_llm.completion</code> function.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>GuardrailOutput</code> <code>GuardrailOutput</code> <p>The output of the validation.</p> Source code in <code>src/any_guardrail/guardrails/any_llm/any_llm.py</code> <pre><code>def validate(\n    self,\n    input_text: str,\n    policy: str,\n    model_id: str = DEFAULT_MODEL_ID,\n    system_prompt: str = DEFAULT_SYSTEM_PROMPT,\n    **kwargs: Any,\n) -&gt; GuardrailOutput:\n    \"\"\"Validate the `input_text` against the given `policy`.\n\n    Args:\n        input_text (str): The text to validate.\n        policy (str): The policy to validate against.\n        model_id (str, optional): The model ID to use.\n        system_prompt (str, optional): The system prompt to use.\n            Expected to have a `{policy}` placeholder.\n        **kwargs: Additional keyword arguments to pass to `any_llm.completion` function.\n\n    Returns:\n        GuardrailOutput: The output of the validation.\n\n    \"\"\"\n    result: ChatCompletion = completion(  # type: ignore[assignment]\n        model=model_id,\n        messages=[\n            {\"role\": \"system\", \"content\": system_prompt.format(policy=policy)},\n            {\"role\": \"user\", \"content\": input_text},\n        ],\n        response_format=GuardrailOutput,\n        **kwargs,\n    )\n    return GuardrailOutput(**json.loads(result.choices[0].message.content))  # type: ignore[arg-type]\n</code></pre>"},{"location":"api/guardrails/deepset/","title":"Deepset","text":""},{"location":"api/guardrails/deepset/#any_guardrail.guardrails.deepset.deepset","title":"<code>any_guardrail.guardrails.deepset.deepset</code>","text":""},{"location":"api/guardrails/deepset/#any_guardrail.guardrails.deepset.deepset.Deepset","title":"<code>Deepset</code>","text":"<p>               Bases: <code>HuggingFace</code></p> <p>Wrapper for prompt injection detection model from Deepset.</p> <p>For more information, please see the model card:</p> <ul> <li>Deepset.</li> </ul> Source code in <code>src/any_guardrail/guardrails/deepset/deepset.py</code> <pre><code>class Deepset(HuggingFace):\n    \"\"\"Wrapper for prompt injection detection model from Deepset.\n\n    For more information, please see the model card:\n\n    - [Deepset](https://huggingface.co/deepset/deberta-v3-base-injection).\n    \"\"\"\n\n    SUPPORTED_MODELS: ClassVar = [\"deepset/deberta-v3-base-injection\"]\n\n    def _post_processing(self, model_outputs: dict[str, Any]) -&gt; GuardrailOutput:\n        return _match_injection_label(model_outputs, DEEPSET_INJECTION_LABEL, self.model.config.id2label)\n</code></pre>"},{"location":"api/guardrails/duo_guard/","title":"Duo Guard","text":""},{"location":"api/guardrails/duo_guard/#any_guardrail.guardrails.duo_guard.duo_guard","title":"<code>any_guardrail.guardrails.duo_guard.duo_guard</code>","text":""},{"location":"api/guardrails/duo_guard/#any_guardrail.guardrails.duo_guard.duo_guard.DuoGuard","title":"<code>DuoGuard</code>","text":"<p>               Bases: <code>HuggingFace</code></p> <p>Guardrail that classifies text based on the categories in DUOGUARD_CATEGORIES.</p> <p>For more information, please see the model card:</p> <ul> <li>DuoGuard.</li> </ul> Source code in <code>src/any_guardrail/guardrails/duo_guard/duo_guard.py</code> <pre><code>class DuoGuard(HuggingFace):\n    \"\"\"Guardrail that classifies text based on the categories in DUOGUARD_CATEGORIES.\n\n    For more information, please see the model card:\n\n    - [DuoGuard](https://huggingface.co/collections/DuoGuard/duoguard-models-67a29ad8bd579a404e504d21).\n    \"\"\"\n\n    SUPPORTED_MODELS: ClassVar = [\n        \"DuoGuard/DuoGuard-0.5B\",\n        \"DuoGuard/DuoGuard-1B-Llama-3.2-transfer\",\n        \"DuoGuard/DuoGuard-1.5B-transfer\",\n    ]\n\n    MODELS_TO_TOKENIZER: ClassVar = {\n        \"DuoGuard/DuoGuard-0.5B\": \"Qwen/Qwen2.5-0.5B\",\n        \"DuoGuard/DuoGuard-1B-Llama-3.2-transfer\": \"meta-llama/Llama-3.2-1B\",\n        \"DuoGuard/DuoGuard-1.5B-transfer\": \"Qwen/Qwen2.5-1.5B\",\n    }\n\n    def __init__(self, model_id: str | None = None, threshold: float = DUOGUARD_DEFAULT_THRESHOLD) -&gt; None:\n        \"\"\"Initialize the DuoGuard model.\"\"\"\n        super().__init__(model_id)\n        self.threshold = threshold\n\n    def _load_model(self) -&gt; None:\n        from transformers import AutoModelForSequenceClassification, AutoTokenizer\n\n        self.model = AutoModelForSequenceClassification.from_pretrained(self.model_id)\n        self.tokenizer = AutoTokenizer.from_pretrained(self.MODELS_TO_TOKENIZER[self.model_id])  # type: ignore[no-untyped-call]\n        self.tokenizer.pad_token = self.tokenizer.eos_token\n\n    def _post_processing(self, model_outputs: dict[str, Any]) -&gt; GuardrailOutput:\n        from torch.nn.functional import sigmoid\n\n        probabilities = sigmoid(model_outputs[\"logits\"][0]).tolist()\n        predicted_labels = {\n            category: prob &gt; self.threshold for category, prob in zip(DUOGUARD_CATEGORIES, probabilities, strict=True)\n        }\n        return GuardrailOutput(\n            valid=not any(predicted_labels.values()), explanation=predicted_labels, score=max(probabilities)\n        )\n</code></pre>"},{"location":"api/guardrails/duo_guard/#any_guardrail.guardrails.duo_guard.duo_guard.DuoGuard.__init__","title":"<code>__init__(model_id=None, threshold=DUOGUARD_DEFAULT_THRESHOLD)</code>","text":"<p>Initialize the DuoGuard model.</p> Source code in <code>src/any_guardrail/guardrails/duo_guard/duo_guard.py</code> <pre><code>def __init__(self, model_id: str | None = None, threshold: float = DUOGUARD_DEFAULT_THRESHOLD) -&gt; None:\n    \"\"\"Initialize the DuoGuard model.\"\"\"\n    super().__init__(model_id)\n    self.threshold = threshold\n</code></pre>"},{"location":"api/guardrails/flowjudge/","title":"FlowJudge","text":""},{"location":"api/guardrails/flowjudge/#any_guardrail.guardrails.flowjudge.flowjudge","title":"<code>any_guardrail.guardrails.flowjudge.flowjudge</code>","text":""},{"location":"api/guardrails/flowjudge/#any_guardrail.guardrails.flowjudge.flowjudge.Flowjudge","title":"<code>Flowjudge</code>","text":"<p>               Bases: <code>Guardrail</code></p> <p>Wrapper around FlowJudge, allowing for custom guardrailing based on user defined criteria, metrics, and rubric.</p> <p>Please see the model card for more information: FlowJudge.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>User defined metric name.</p> required <code>criteria</code> <code>str</code> <p>User defined question that they want answered by FlowJudge model.</p> required <code>rubric</code> <code>dict[int, str]</code> <p>A scoring rubric in a likert scale fashion, providing an integer score and then a description of what the value means.</p> required <code>required_inputs</code> <code>list[str]</code> <p>A list of what is required for the judge to consider.</p> required <code>required_output</code> <code>str</code> <p>What is the expected output from the judge.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>Only supports FlowJudge keywords to instantiate FlowJudge.</p> Source code in <code>src/any_guardrail/guardrails/flowjudge/flowjudge.py</code> <pre><code>class Flowjudge(Guardrail):\n    \"\"\"Wrapper around FlowJudge, allowing for custom guardrailing based on user defined criteria, metrics, and rubric.\n\n    Please see the model card for more information: [FlowJudge](https://huggingface.co/flowaicom/Flow-Judge-v0.1).\n\n    Args:\n        name: User defined metric name.\n        criteria: User defined question that they want answered by FlowJudge model.\n        rubric: A scoring rubric in a likert scale fashion, providing an integer score and then a description of what the\n            value means.\n        required_inputs: A list of what is required for the judge to consider.\n        required_output: What is the expected output from the judge.\n\n    Raises:\n        ValueError: Only supports FlowJudge keywords to instantiate FlowJudge.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        name: str,\n        criteria: str,\n        rubric: dict[int, str],\n        required_inputs: list[str],\n        required_output: str,\n    ) -&gt; None:\n        \"\"\"Initialize the FlowJudgeClass.\"\"\"\n        if MISSING_PACKAGES_ERROR is not None:\n            msg = \"Missing packages for FlowJudge guardrail. You can try `pip install 'any-guardrail[flowjudge]'`\"\n            raise ImportError(msg) from MISSING_PACKAGES_ERROR\n\n        self.metric_name = name\n        self.criteria = criteria\n        self.rubric = rubric\n        self.required_inputs = required_inputs\n        self.required_output = required_output\n        self.metric_prompt = self._define_metric_prompt()\n        self.model = self._load_model()\n\n    def validate(self, inputs: list[dict[str, str]], output: dict[str, str]) -&gt; GuardrailOutput:\n        \"\"\"Classifies the desired input and output according to the associated metric provided to the judge.\n\n        Args:\n            inputs: A dictionary mapping the required input names to the inputs.\n            output: A dictionary mapping the required output name to the output.\n\n        Return:\n            A score from the RubricItems and feedback related to the rubric and criteria.\n\n        \"\"\"\n        eval_input = self._pre_processing(inputs, output)\n        result = self._inference(eval_input)\n        return GuardrailOutput(explanation=result.feedback, score=result.score)\n\n    def _load_model(self) -&gt; FlowJudge:\n        \"\"\"Construct the FlowJudge model using the defined metric prompt that contains the rubric, criteria, and metric.\n\n        Returns:\n            judge: The evaluation model.\n\n        \"\"\"\n        model = Hf(flash_attn=False)\n        return FlowJudge(metric=self.metric_prompt, model=model)\n\n    def _define_metric_prompt(self) -&gt; Metric:\n        \"\"\"Construct the Metric object needed to instantiate the FlowJudge model.\n\n        Returns:\n            The Metric object used to construct the FlowJudge model.\n\n        \"\"\"\n        processed_rubric = self._construct_rubric()\n        return Metric(\n            name=self.metric_name,\n            criteria=self.criteria,\n            rubric=processed_rubric,\n            required_inputs=self.required_inputs,\n            required_output=self.required_output,\n        )\n\n    def _construct_rubric(self) -&gt; list[RubricItem]:\n        \"\"\"Construct the rubric from a user defined rubric dicitionary to construct the Metric object.\n\n        Returns:\n            List of RubricItem objects.\n\n        \"\"\"\n        processed_rubric = []\n        for key, value in self.rubric.items():\n            rubric_item = RubricItem(score=key, description=value)\n            processed_rubric.append(rubric_item)\n        return processed_rubric\n\n    def _pre_processing(self, inputs: list[dict[str, str]], output: dict[str, str]) -&gt; EvalInput:\n        return EvalInput(inputs=inputs, output=output)\n\n    def _inference(self, eval_input: EvalInput) -&gt; EvalOutput:\n        return self.model.evaluate(eval_input, save_results=False)\n</code></pre>"},{"location":"api/guardrails/flowjudge/#any_guardrail.guardrails.flowjudge.flowjudge.Flowjudge.__init__","title":"<code>__init__(name, criteria, rubric, required_inputs, required_output)</code>","text":"<p>Initialize the FlowJudgeClass.</p> Source code in <code>src/any_guardrail/guardrails/flowjudge/flowjudge.py</code> <pre><code>def __init__(\n    self,\n    name: str,\n    criteria: str,\n    rubric: dict[int, str],\n    required_inputs: list[str],\n    required_output: str,\n) -&gt; None:\n    \"\"\"Initialize the FlowJudgeClass.\"\"\"\n    if MISSING_PACKAGES_ERROR is not None:\n        msg = \"Missing packages for FlowJudge guardrail. You can try `pip install 'any-guardrail[flowjudge]'`\"\n        raise ImportError(msg) from MISSING_PACKAGES_ERROR\n\n    self.metric_name = name\n    self.criteria = criteria\n    self.rubric = rubric\n    self.required_inputs = required_inputs\n    self.required_output = required_output\n    self.metric_prompt = self._define_metric_prompt()\n    self.model = self._load_model()\n</code></pre>"},{"location":"api/guardrails/flowjudge/#any_guardrail.guardrails.flowjudge.flowjudge.Flowjudge.validate","title":"<code>validate(inputs, output)</code>","text":"<p>Classifies the desired input and output according to the associated metric provided to the judge.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>list[dict[str, str]]</code> <p>A dictionary mapping the required input names to the inputs.</p> required <code>output</code> <code>dict[str, str]</code> <p>A dictionary mapping the required output name to the output.</p> required Return <p>A score from the RubricItems and feedback related to the rubric and criteria.</p> Source code in <code>src/any_guardrail/guardrails/flowjudge/flowjudge.py</code> <pre><code>def validate(self, inputs: list[dict[str, str]], output: dict[str, str]) -&gt; GuardrailOutput:\n    \"\"\"Classifies the desired input and output according to the associated metric provided to the judge.\n\n    Args:\n        inputs: A dictionary mapping the required input names to the inputs.\n        output: A dictionary mapping the required output name to the output.\n\n    Return:\n        A score from the RubricItems and feedback related to the rubric and criteria.\n\n    \"\"\"\n    eval_input = self._pre_processing(inputs, output)\n    result = self._inference(eval_input)\n    return GuardrailOutput(explanation=result.feedback, score=result.score)\n</code></pre>"},{"location":"api/guardrails/glider/","title":"Glider","text":""},{"location":"api/guardrails/glider/#any_guardrail.guardrails.glider.glider","title":"<code>any_guardrail.guardrails.glider.glider</code>","text":""},{"location":"api/guardrails/glider/#any_guardrail.guardrails.glider.glider.Glider","title":"<code>Glider</code>","text":"<p>               Bases: <code>HuggingFace</code></p> <p>A prompt based guardrail from Patronus AI that utilizes pass criteria and a rubric to judge text.</p> <p>For more information, see the model card:GLIDER. It outputs its reasoning, highlights for what determined the score, and an integer score.</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str | None</code> <p>HuggingFace path to model.</p> <code>None</code> <code>pass_criteria</code> <code>str</code> <p>A question or description of what you are validating.</p> required <code>rubric</code> <code>str</code> <p>A scoring rubric, describing to the model how to score the provided data.</p> required Raise <p>ValueError: Can only use model path to GLIDER from HuggingFace.</p> Source code in <code>src/any_guardrail/guardrails/glider/glider.py</code> <pre><code>class Glider(HuggingFace):\n    \"\"\"A prompt based guardrail from Patronus AI that utilizes pass criteria and a rubric to judge text.\n\n    For more information, see the model card:[GLIDER](https://huggingface.co/PatronusAI/glider). It outputs its reasoning,\n    highlights for what determined the score, and an integer score.\n\n    Args:\n        model_id: HuggingFace path to model.\n        pass_criteria: A question or description of what you are validating.\n        rubric: A scoring rubric, describing to the model how to score the provided data.\n\n    Raise:\n        ValueError: Can only use model path to GLIDER from HuggingFace.\n\n    \"\"\"\n\n    SUPPORTED_MODELS: ClassVar = [\"PatronusAI/glider\"]\n\n    def __init__(self, pass_criteria: str, rubric: str, model_id: str | None = None) -&gt; None:\n        \"\"\"Initialize the GLIDER guardrail.\"\"\"\n        super().__init__(model_id)\n        self.pass_criteria = pass_criteria\n        self.rubric = rubric\n        self.system_prompt = SYSTEM_PROMPT_GLIDER\n\n    def validate(self, input_text: str, output_text: str | None = None) -&gt; GuardrailOutput:\n        \"\"\"Use the provided pass criteria and rubric to judge the input and output text provided.\n\n        Args:\n            input_text: the initial text.\n            output_text: the subsequent text.\n\n        Returns:\n            An explanation in the format provided by the system prompt.\n\n        \"\"\"\n        message = self._pre_processing(input_text, output_text)\n        result = self._inference(message)\n        return self._post_processing(result)\n\n    def _load_model(self) -&gt; None:\n        from transformers import pipeline\n\n        pipe = pipeline(\"text-generation\", self.model_id, max_new_tokens=2048, return_full_text=False)\n        self.model = pipe\n\n    def _pre_processing(self, input_text: str, output_text: str | None = None) -&gt; list[dict[str, str]]:\n        if output_text is None:\n            data = INPUT_DATA_FORMAT.format(input_text=input_text)\n        else:\n            data = INPUT_OUTPUT_DATA_FORMAT.format(input_text=input_text, output_text=output_text)\n        prompt = self.system_prompt.format(data=data, pass_criteria=self.pass_criteria, rubric=self.rubric)\n        return [{\"role\": \"user\", \"content\": prompt}]\n\n    def _inference(self, message: list[dict[str, str]]) -&gt; Any:\n        return self.model(message)[0][\"generated_text\"]\n\n    def _post_processing(self, model_outputs: Any) -&gt; GuardrailOutput:\n        score = re.findall(r\"&lt;score&gt;\\n(\\d+)\\n&lt;/score&gt;\", model_outputs)\n        if len(score) != 0 and score[0].isdigit():\n            final_score = int(score[0])\n        else:\n            final_score = None\n\n        return GuardrailOutput(explanation=model_outputs, score=final_score)\n</code></pre>"},{"location":"api/guardrails/glider/#any_guardrail.guardrails.glider.glider.Glider.__init__","title":"<code>__init__(pass_criteria, rubric, model_id=None)</code>","text":"<p>Initialize the GLIDER guardrail.</p> Source code in <code>src/any_guardrail/guardrails/glider/glider.py</code> <pre><code>def __init__(self, pass_criteria: str, rubric: str, model_id: str | None = None) -&gt; None:\n    \"\"\"Initialize the GLIDER guardrail.\"\"\"\n    super().__init__(model_id)\n    self.pass_criteria = pass_criteria\n    self.rubric = rubric\n    self.system_prompt = SYSTEM_PROMPT_GLIDER\n</code></pre>"},{"location":"api/guardrails/glider/#any_guardrail.guardrails.glider.glider.Glider.validate","title":"<code>validate(input_text, output_text=None)</code>","text":"<p>Use the provided pass criteria and rubric to judge the input and output text provided.</p> <p>Parameters:</p> Name Type Description Default <code>input_text</code> <code>str</code> <p>the initial text.</p> required <code>output_text</code> <code>str | None</code> <p>the subsequent text.</p> <code>None</code> <p>Returns:</p> Type Description <code>GuardrailOutput</code> <p>An explanation in the format provided by the system prompt.</p> Source code in <code>src/any_guardrail/guardrails/glider/glider.py</code> <pre><code>def validate(self, input_text: str, output_text: str | None = None) -&gt; GuardrailOutput:\n    \"\"\"Use the provided pass criteria and rubric to judge the input and output text provided.\n\n    Args:\n        input_text: the initial text.\n        output_text: the subsequent text.\n\n    Returns:\n        An explanation in the format provided by the system prompt.\n\n    \"\"\"\n    message = self._pre_processing(input_text, output_text)\n    result = self._inference(message)\n    return self._post_processing(result)\n</code></pre>"},{"location":"api/guardrails/harm_guard/","title":"Harm Guard","text":""},{"location":"api/guardrails/harm_guard/#any_guardrail.guardrails.harm_guard.harm_guard","title":"<code>any_guardrail.guardrails.harm_guard.harm_guard</code>","text":""},{"location":"api/guardrails/harm_guard/#any_guardrail.guardrails.harm_guard.harm_guard.HarmGuard","title":"<code>HarmGuard</code>","text":"<p>               Bases: <code>HuggingFace</code></p> <p>Prompt injection detection encoder based model.</p> <p>For more information, please see the model card:</p> <ul> <li>HarmGuard.</li> </ul> Source code in <code>src/any_guardrail/guardrails/harm_guard/harm_guard.py</code> <pre><code>class HarmGuard(HuggingFace):\n    \"\"\"Prompt injection detection encoder based model.\n\n    For more information, please see the model card:\n\n    - [HarmGuard](https://huggingface.co/hbseong/HarmAug-Guard).\n    \"\"\"\n\n    SUPPORTED_MODELS: ClassVar = [\"hbseong/HarmAug-Guard\"]\n\n    def __init__(self, model_id: str | None = None, threshold: float = HARMGUARD_DEFAULT_THRESHOLD) -&gt; None:\n        \"\"\"Initialize the HarmGuard guardrail.\"\"\"\n        super().__init__(model_id)\n        self.threshold = threshold\n\n    def _post_processing(self, model_outputs: dict[str, Any]) -&gt; GuardrailOutput:\n        logits = model_outputs[\"logits\"][0].numpy()\n        scores = _softmax(logits)  # type: ignore[no-untyped-call]\n        final_score = float(scores[1])\n        return GuardrailOutput(valid=final_score &lt; self.threshold, score=final_score)\n</code></pre>"},{"location":"api/guardrails/harm_guard/#any_guardrail.guardrails.harm_guard.harm_guard.HarmGuard.__init__","title":"<code>__init__(model_id=None, threshold=HARMGUARD_DEFAULT_THRESHOLD)</code>","text":"<p>Initialize the HarmGuard guardrail.</p> Source code in <code>src/any_guardrail/guardrails/harm_guard/harm_guard.py</code> <pre><code>def __init__(self, model_id: str | None = None, threshold: float = HARMGUARD_DEFAULT_THRESHOLD) -&gt; None:\n    \"\"\"Initialize the HarmGuard guardrail.\"\"\"\n    super().__init__(model_id)\n    self.threshold = threshold\n</code></pre>"},{"location":"api/guardrails/injec_guard/","title":"InjecGuard","text":""},{"location":"api/guardrails/injec_guard/#any_guardrail.guardrails.injec_guard.injec_guard","title":"<code>any_guardrail.guardrails.injec_guard.injec_guard</code>","text":""},{"location":"api/guardrails/injec_guard/#any_guardrail.guardrails.injec_guard.injec_guard.InjecGuard","title":"<code>InjecGuard</code>","text":"<p>               Bases: <code>HuggingFace</code></p> <p>Prompt injection detection encoder based model.</p> <p>For more information, please see the model card:</p> <ul> <li>InjecGuard.</li> </ul> Source code in <code>src/any_guardrail/guardrails/injec_guard/injec_guard.py</code> <pre><code>class InjecGuard(HuggingFace):\n    \"\"\"Prompt injection detection encoder based model.\n\n    For more information, please see the model card:\n\n    - [InjecGuard](https://huggingface.co/leolee99/InjecGuard).\n    \"\"\"\n\n    SUPPORTED_MODELS: ClassVar = [\"leolee99/InjecGuard\"]\n\n    def _load_model(self) -&gt; None:\n        from transformers import AutoModelForSequenceClassification, AutoTokenizer\n\n        self.model = AutoModelForSequenceClassification.from_pretrained(self.model_id, trust_remote_code=True)\n        self.tokenizer = AutoTokenizer.from_pretrained(self.model_id)  # type: ignore[no-untyped-call]\n\n    def _post_processing(self, model_outputs: dict[str, Any]) -&gt; GuardrailOutput:\n        return _match_injection_label(model_outputs, INJECGUARD_LABEL, self.model.config.id2label)\n</code></pre>"},{"location":"api/guardrails/jasper/","title":"Jasper","text":""},{"location":"api/guardrails/jasper/#any_guardrail.guardrails.jasper.jasper","title":"<code>any_guardrail.guardrails.jasper.jasper</code>","text":""},{"location":"api/guardrails/jasper/#any_guardrail.guardrails.jasper.jasper.Jasper","title":"<code>Jasper</code>","text":"<p>               Bases: <code>HuggingFace</code></p> <p>Prompt injection detection encoder based models.</p> <p>For more information, please see the model card:</p> <ul> <li>Jasper Deberta</li> <li>Jasper Gelectra.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str | None</code> <p>HuggingFace path to model.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>Can only use model paths for Jasper models from HuggingFace.</p> Source code in <code>src/any_guardrail/guardrails/jasper/jasper.py</code> <pre><code>class Jasper(HuggingFace):\n    \"\"\"Prompt injection detection encoder based models.\n\n    For more information, please see the model card:\n\n    - [Jasper Deberta](https://huggingface.co/JasperLS/deberta-v3-base-injection)\n    - [Jasper Gelectra](https://huggingface.co/JasperLS/gelectra-base-injection).\n\n    Args:\n        model_id: HuggingFace path to model.\n\n    Raises:\n        ValueError: Can only use model paths for Jasper models from HuggingFace.\n\n    \"\"\"\n\n    SUPPORTED_MODELS: ClassVar = [\"JasperLS/gelectra-base-injection\", \"JasperLS/deberta-v3-base-injection\"]\n\n    def _post_processing(self, model_outputs: dict[str, Any]) -&gt; GuardrailOutput:\n        return _match_injection_label(model_outputs, JASPER_INJECTION_LABEL, self.model.config.id2label)\n</code></pre>"},{"location":"api/guardrails/llama_guard/","title":"Llama Guard","text":""},{"location":"api/guardrails/llama_guard/#any_guardrail.guardrails.llama_guard.llama_guard","title":"<code>any_guardrail.guardrails.llama_guard.llama_guard</code>","text":""},{"location":"api/guardrails/llama_guard/#any_guardrail.guardrails.llama_guard.llama_guard.LlamaGuard","title":"<code>LlamaGuard</code>","text":"<p>               Bases: <code>HuggingFace</code></p> <p>Wrapper class for Llama Guard 3 &amp; 4 implementations.</p> <p>For more information about the implementations about either off topic model, please see the below model cards:</p> <ul> <li>Meta Llama Guard 3 Docs</li> <li>HuggingFace Llama Guard 3 Docs</li> <li>Meta Llama Guard 4 Docs</li> <li>HuggingFace Llama Guard 4 Docs</li> </ul> Source code in <code>src/any_guardrail/guardrails/llama_guard/llama_guard.py</code> <pre><code>class LlamaGuard(HuggingFace):\n    \"\"\"Wrapper class for Llama Guard 3 &amp; 4 implementations.\n\n    For more information about the implementations about either off topic model, please see the below model cards:\n\n    - [Meta Llama Guard 3 Docs](https://www.llama.com/docs/model-cards-and-prompt-formats/llama-guard-3/)\n    - [HuggingFace Llama Guard 3 Docs](https://huggingface.co/meta-llama/Llama-Guard-3-1B)\n    - [Meta Llama Guard 4 Docs](https://www.llama.com/docs/model-cards-and-prompt-formats/llama-guard-4/)\n    - [HuggingFace Llama Guard 4 Docs](https://huggingface.co/meta-llama/Llama-Guard-4-12B)\n    \"\"\"\n\n    SUPPORTED_MODELS: ClassVar = [\n        \"meta-llama/Llama-Guard-3-1B\",\n        \"meta-llama/Llama-Guard-3-8B\",\n        \"hf-internal-testing/tiny-random-LlamaForCausalLM\",\n        \"meta-llama/Llama-Guard-4-12B\",\n    ]\n\n    def __init__(self, model_id: str | None = None) -&gt; None:\n        \"\"\"Llama guard model. Either Llama Guard 3 or 4 depending on the model id. Defaults to Llama Guard 3.\"\"\"\n        self.model_id = model_id or self.SUPPORTED_MODELS[0]\n        if self._is_version_4:\n            self.tokenizer_class = AutoProcessor\n            self.model_class = Llama4ForConditionalGeneration\n            self.tokenizer_params = {\n                \"return_tensors\": \"pt\",\n                \"add_generation_prompt\": True,\n                \"tokenize\": True,\n                \"return_dict\": True,\n            }\n        elif self.model_id in self.SUPPORTED_MODELS:\n            self.tokenizer_class = AutoTokenizer  # type: ignore[assignment]\n            self.model_class = AutoModelForCausalLM  # type: ignore[assignment]\n            self.tokenizer_params = {\n                \"return_tensors\": \"pt\",\n            }\n        else:\n            msg = f\"Unsupported model_id: {self.model_id}\"\n            raise ValueError(msg)\n        super().__init__(model_id)\n\n    def validate(self, input_text: str, output_text: str | None = None, **kwargs: Any) -&gt; GuardrailOutput:\n        \"\"\"Judge whether the input text or the input text, output text pair are unsafe based on the Llama taxonomy.\n\n        Args:\n            input_text: the prior text before hitting a system or model.\n            output_text: the succeeding text after hitting a system or model.\n            **kwargs: additional keyword arguments, specifically supporting 'excluded_category_keys' and 'categories'.\n                Please see Llama Guard documentation for more details.\n\n        Returns:\n            Provides an explanation that can be parsed to see whether the text is safe or not.\n\n        \"\"\"\n        model_inputs = self._pre_processing(input_text, output_text, **kwargs)\n        model_outputs = self._inference(model_inputs)\n        return self._post_processing(model_outputs)\n\n    def _load_model(self) -&gt; None:\n        self.tokenizer = self.tokenizer_class.from_pretrained(self.model_id)  # type: ignore[no-untyped-call]\n        self.model = self.model_class.from_pretrained(self.model_id)\n\n    def _pre_processing(self, input_text: str, output_text: str | None = None, **kwargs: Any) -&gt; Any:\n        if output_text:\n            if self.model_id == self.SUPPORTED_MODELS[0] or self._is_version_4:\n                conversation = [\n                    {\n                        \"role\": \"user\",\n                        \"content\": [\n                            {\"type\": \"text\", \"text\": input_text},\n                        ],\n                    },\n                    {\n                        \"role\": \"assistant\",\n                        \"content\": [\n                            {\"type\": \"text\", \"text\": output_text},\n                        ],\n                    },\n                ]\n            else:\n                conversation = [\n                    {\n                        \"role\": \"user\",\n                        \"content\": input_text,\n                    },\n                    {\n                        \"role\": \"assistant\",\n                        \"content\": output_text,\n                    },\n                ]\n        else:\n            if self.model_id == self.SUPPORTED_MODELS[0] or self._is_version_4:\n                conversation = [\n                    {\n                        \"role\": \"user\",\n                        \"content\": [\n                            {\"type\": \"text\", \"text\": input_text},\n                        ],\n                    },\n                ]\n            else:\n                conversation = [\n                    {\n                        \"role\": \"user\",\n                        \"content\": input_text,\n                    },\n                ]\n        self.model_inputs = self.tokenizer.apply_chat_template(conversation, **self.tokenizer_params, **kwargs)\n        return self.model_inputs\n\n    def _inference(self, model_inputs: Any) -&gt; Any:\n        if self._is_version_4:\n            return self.model.generate(**model_inputs, max_new_tokens=10, do_sample=False)\n        return self.model.generate(\n            model_inputs,\n            max_new_tokens=20,\n            pad_token_id=0,\n        )\n\n    def _post_processing(self, model_outputs: Any) -&gt; GuardrailOutput:\n        if self._is_version_4:\n            explanation = self.tokenizer.batch_decode(\n                model_outputs[:, self.model_inputs[\"input_ids\"].shape[-1] :], skip_special_tokens=True\n            )[0]\n\n            if \"unsafe\" in explanation.lower():\n                return GuardrailOutput(valid=False, explanation=explanation)\n            return GuardrailOutput(valid=True, explanation=explanation)\n\n        prompt_len = self.model_inputs.shape[1]\n        output = model_outputs[:, prompt_len:]\n        explanation = self.tokenizer.decode(output[0])\n\n        if \"unsafe\" in explanation.lower():\n            return GuardrailOutput(valid=False, explanation=explanation)\n        return GuardrailOutput(valid=True, explanation=explanation)\n\n    @property\n    def _is_version_4(self) -&gt; bool:\n        return self.model_id == self.SUPPORTED_MODELS[-1]\n</code></pre>"},{"location":"api/guardrails/llama_guard/#any_guardrail.guardrails.llama_guard.llama_guard.LlamaGuard.__init__","title":"<code>__init__(model_id=None)</code>","text":"<p>Llama guard model. Either Llama Guard 3 or 4 depending on the model id. Defaults to Llama Guard 3.</p> Source code in <code>src/any_guardrail/guardrails/llama_guard/llama_guard.py</code> <pre><code>def __init__(self, model_id: str | None = None) -&gt; None:\n    \"\"\"Llama guard model. Either Llama Guard 3 or 4 depending on the model id. Defaults to Llama Guard 3.\"\"\"\n    self.model_id = model_id or self.SUPPORTED_MODELS[0]\n    if self._is_version_4:\n        self.tokenizer_class = AutoProcessor\n        self.model_class = Llama4ForConditionalGeneration\n        self.tokenizer_params = {\n            \"return_tensors\": \"pt\",\n            \"add_generation_prompt\": True,\n            \"tokenize\": True,\n            \"return_dict\": True,\n        }\n    elif self.model_id in self.SUPPORTED_MODELS:\n        self.tokenizer_class = AutoTokenizer  # type: ignore[assignment]\n        self.model_class = AutoModelForCausalLM  # type: ignore[assignment]\n        self.tokenizer_params = {\n            \"return_tensors\": \"pt\",\n        }\n    else:\n        msg = f\"Unsupported model_id: {self.model_id}\"\n        raise ValueError(msg)\n    super().__init__(model_id)\n</code></pre>"},{"location":"api/guardrails/llama_guard/#any_guardrail.guardrails.llama_guard.llama_guard.LlamaGuard.validate","title":"<code>validate(input_text, output_text=None, **kwargs)</code>","text":"<p>Judge whether the input text or the input text, output text pair are unsafe based on the Llama taxonomy.</p> <p>Parameters:</p> Name Type Description Default <code>input_text</code> <code>str</code> <p>the prior text before hitting a system or model.</p> required <code>output_text</code> <code>str | None</code> <p>the succeeding text after hitting a system or model.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>additional keyword arguments, specifically supporting 'excluded_category_keys' and 'categories'. Please see Llama Guard documentation for more details.</p> <code>{}</code> <p>Returns:</p> Type Description <code>GuardrailOutput</code> <p>Provides an explanation that can be parsed to see whether the text is safe or not.</p> Source code in <code>src/any_guardrail/guardrails/llama_guard/llama_guard.py</code> <pre><code>def validate(self, input_text: str, output_text: str | None = None, **kwargs: Any) -&gt; GuardrailOutput:\n    \"\"\"Judge whether the input text or the input text, output text pair are unsafe based on the Llama taxonomy.\n\n    Args:\n        input_text: the prior text before hitting a system or model.\n        output_text: the succeeding text after hitting a system or model.\n        **kwargs: additional keyword arguments, specifically supporting 'excluded_category_keys' and 'categories'.\n            Please see Llama Guard documentation for more details.\n\n    Returns:\n        Provides an explanation that can be parsed to see whether the text is safe or not.\n\n    \"\"\"\n    model_inputs = self._pre_processing(input_text, output_text, **kwargs)\n    model_outputs = self._inference(model_inputs)\n    return self._post_processing(model_outputs)\n</code></pre>"},{"location":"api/guardrails/off_topic/","title":"OffTopic","text":""},{"location":"api/guardrails/off_topic/#any_guardrail.guardrails.off_topic","title":"<code>any_guardrail.guardrails.off_topic</code>","text":""},{"location":"api/guardrails/off_topic/#any_guardrail.guardrails.off_topic.OffTopic","title":"<code>OffTopic</code>","text":"<p>               Bases: <code>HuggingFace</code></p> <p>Abstract base class for the Off Topic models.</p> <p>For more information about the implementations about either off topic model, please see the below model cards:</p> <ul> <li>govtech/stsb-roberta-base-off-topic model.</li> <li>govtech/jina-embeddings-v2-small-en-off-topic.</li> </ul> Source code in <code>src/any_guardrail/guardrails/off_topic/off_topic.py</code> <pre><code>class OffTopic(HuggingFace):\n    \"\"\"Abstract base class for the Off Topic models.\n\n    For more information about the implementations about either off topic model, please see the below model cards:\n\n    - [govtech/stsb-roberta-base-off-topic model](https://huggingface.co/govtech/stsb-roberta-base-off-topic).\n    - [govtech/jina-embeddings-v2-small-en-off-topic](https://huggingface.co/govtech/jina-embeddings-v2-small-en-off-topic).\n    \"\"\"\n\n    SUPPORTED_MODELS: ClassVar = [\n        \"mozilla-ai/jina-embeddings-v2-small-en-off-topic\",\n        \"mozilla-ai/stsb-roberta-base-off-topic\",\n    ]\n\n    implementation: OffTopicJina | OffTopicStsb\n\n    def __init__(self, model_id: str | None = None) -&gt; None:\n        \"\"\"Off Topic model based on one of two implementations decided by model ID.\"\"\"\n        super().__init__(model_id)\n        if self.model_id == self.SUPPORTED_MODELS[0]:\n            self.implementation = OffTopicJina()\n        elif self.model_id == self.SUPPORTED_MODELS[1]:\n            self.implementation = OffTopicStsb()\n        else:\n            msg = f\"Unsupported model_id: {self.model_id}\"\n            raise ValueError(msg)\n        super().__init__()\n\n    def validate(self, input_text: str, comparison_text: str | None = None) -&gt; GuardrailOutput:\n        \"\"\"Compare two texts to see if they are relevant to each other.\n\n        Args:\n            input_text: the original text you want to compare against.\n            comparison_text: the text you want to compare to.\n\n        Returns:\n            valid=False means off topic, valid=True  means on topic. Will also provide probabilities of each.\n\n        \"\"\"\n        msg = \"Must provide a text to compare to.\"\n        if comparison_text:\n            raise ValueError(msg)\n        model_inputs = self.implementation._pre_processing(input_text, comparison_text)\n        model_outputs = self.implementation._inference(model_inputs)\n        return self._post_processing(model_outputs)\n\n    def _load_model(self) -&gt; None:\n        self.implementation._load_model()\n\n    def _post_processing(self, model_outputs: Any) -&gt; GuardrailOutput:\n        return self.implementation._post_processing(model_outputs)\n</code></pre>"},{"location":"api/guardrails/off_topic/#any_guardrail.guardrails.off_topic.OffTopic.__init__","title":"<code>__init__(model_id=None)</code>","text":"<p>Off Topic model based on one of two implementations decided by model ID.</p> Source code in <code>src/any_guardrail/guardrails/off_topic/off_topic.py</code> <pre><code>def __init__(self, model_id: str | None = None) -&gt; None:\n    \"\"\"Off Topic model based on one of two implementations decided by model ID.\"\"\"\n    super().__init__(model_id)\n    if self.model_id == self.SUPPORTED_MODELS[0]:\n        self.implementation = OffTopicJina()\n    elif self.model_id == self.SUPPORTED_MODELS[1]:\n        self.implementation = OffTopicStsb()\n    else:\n        msg = f\"Unsupported model_id: {self.model_id}\"\n        raise ValueError(msg)\n    super().__init__()\n</code></pre>"},{"location":"api/guardrails/off_topic/#any_guardrail.guardrails.off_topic.OffTopic.validate","title":"<code>validate(input_text, comparison_text=None)</code>","text":"<p>Compare two texts to see if they are relevant to each other.</p> <p>Parameters:</p> Name Type Description Default <code>input_text</code> <code>str</code> <p>the original text you want to compare against.</p> required <code>comparison_text</code> <code>str | None</code> <p>the text you want to compare to.</p> <code>None</code> <p>Returns:</p> Type Description <code>GuardrailOutput</code> <p>valid=False means off topic, valid=True  means on topic. Will also provide probabilities of each.</p> Source code in <code>src/any_guardrail/guardrails/off_topic/off_topic.py</code> <pre><code>def validate(self, input_text: str, comparison_text: str | None = None) -&gt; GuardrailOutput:\n    \"\"\"Compare two texts to see if they are relevant to each other.\n\n    Args:\n        input_text: the original text you want to compare against.\n        comparison_text: the text you want to compare to.\n\n    Returns:\n        valid=False means off topic, valid=True  means on topic. Will also provide probabilities of each.\n\n    \"\"\"\n    msg = \"Must provide a text to compare to.\"\n    if comparison_text:\n        raise ValueError(msg)\n    model_inputs = self.implementation._pre_processing(input_text, comparison_text)\n    model_outputs = self.implementation._inference(model_inputs)\n    return self._post_processing(model_outputs)\n</code></pre>"},{"location":"api/guardrails/pangolin/","title":"Pangolin","text":""},{"location":"api/guardrails/pangolin/#any_guardrail.guardrails.pangolin.pangolin","title":"<code>any_guardrail.guardrails.pangolin.pangolin</code>","text":""},{"location":"api/guardrails/pangolin/#any_guardrail.guardrails.pangolin.pangolin.Pangolin","title":"<code>Pangolin</code>","text":"<p>               Bases: <code>HuggingFace</code></p> <p>Prompt injection detection encoder based models.</p> <p>For more information, please see the model card:</p> <ul> <li>Pangolin Base</li> </ul> Source code in <code>src/any_guardrail/guardrails/pangolin/pangolin.py</code> <pre><code>class Pangolin(HuggingFace):\n    \"\"\"Prompt injection detection encoder based models.\n\n    For more information, please see the model card:\n\n    - [Pangolin Base](https://huggingface.co/dcarpintero/pangolin-guard-base)\n    \"\"\"\n\n    SUPPORTED_MODELS: ClassVar = [\"dcarpintero/pangolin-guard-base\"]\n\n    def _post_processing(self, model_outputs: dict[str, Any]) -&gt; GuardrailOutput:\n        return _match_injection_label(model_outputs, PANGOLIN_INJECTION_LABEL, self.model.config.id2label)\n</code></pre>"},{"location":"api/guardrails/protectai/","title":"ProtectAI","text":""},{"location":"api/guardrails/protectai/#any_guardrail.guardrails.protectai.protectai","title":"<code>any_guardrail.guardrails.protectai.protectai</code>","text":""},{"location":"api/guardrails/protectai/#any_guardrail.guardrails.protectai.protectai.Protectai","title":"<code>Protectai</code>","text":"<p>               Bases: <code>HuggingFace</code></p> <p>Prompt injection detection encoder based models.</p> <p>For more information, please see the model card:</p> <ul> <li>ProtectAI.</li> </ul> Source code in <code>src/any_guardrail/guardrails/protectai/protectai.py</code> <pre><code>class Protectai(HuggingFace):\n    \"\"\"Prompt injection detection encoder based models.\n\n    For more information, please see the model card:\n\n    - [ProtectAI](https://huggingface.co/collections/protectai/llm-security-65c1f17a11c4251eeab53f40).\n    \"\"\"\n\n    SUPPORTED_MODELS: ClassVar = [\n        \"ProtectAI/deberta-v3-small-prompt-injection-v2\",\n        \"ProtectAI/distilroberta-base-rejection-v1\",\n        \"ProtectAI/deberta-v3-base-prompt-injection\",\n        \"ProtectAI/deberta-v3-base-prompt-injection-v2\",\n    ]\n\n    def _post_processing(self, model_outputs: dict[str, Any]) -&gt; GuardrailOutput:\n        return _match_injection_label(model_outputs, PROTECTAI_INJECTION_LABEL, self.model.config.id2label)\n</code></pre>"},{"location":"api/guardrails/sentinel/","title":"Sentinel","text":""},{"location":"api/guardrails/sentinel/#any_guardrail.guardrails.sentinel.sentinel","title":"<code>any_guardrail.guardrails.sentinel.sentinel</code>","text":""},{"location":"api/guardrails/sentinel/#any_guardrail.guardrails.sentinel.sentinel.Sentinel","title":"<code>Sentinel</code>","text":"<p>               Bases: <code>HuggingFace</code></p> <p>Prompt injection detection encoder based model.</p> <p>For more information, please see the model card:</p> <ul> <li>Sentinel.</li> </ul> Source code in <code>src/any_guardrail/guardrails/sentinel/sentinel.py</code> <pre><code>class Sentinel(HuggingFace):\n    \"\"\"Prompt injection detection encoder based model.\n\n    For more information, please see the model card:\n\n    - [Sentinel](https://huggingface.co/qualifire/prompt-injection-sentinel).\n    \"\"\"\n\n    SUPPORTED_MODELS: ClassVar = [\"qualifire/prompt-injection-sentinel\"]\n\n    def _post_processing(self, model_outputs: dict[str, Any]) -&gt; GuardrailOutput:\n        return _match_injection_label(model_outputs, SENTINEL_INJECTION_LABEL, self.model.config.id2label)\n</code></pre>"},{"location":"api/guardrails/shield_gemma/","title":"Shield Gemma","text":""},{"location":"api/guardrails/shield_gemma/#any_guardrail.guardrails.shield_gemma.shield_gemma","title":"<code>any_guardrail.guardrails.shield_gemma.shield_gemma</code>","text":""},{"location":"api/guardrails/shield_gemma/#any_guardrail.guardrails.shield_gemma.shield_gemma.ShieldGemma","title":"<code>ShieldGemma</code>","text":"<p>               Bases: <code>HuggingFace</code></p> <p>Wrapper class for Google ShieldGemma models.</p> <p>For more information, please visit the model cards: Shield Gemma.</p> <p>Note we do not support the image classifier.</p> Source code in <code>src/any_guardrail/guardrails/shield_gemma/shield_gemma.py</code> <pre><code>class ShieldGemma(HuggingFace):\n    \"\"\"Wrapper class for Google ShieldGemma models.\n\n    For more information, please visit the model cards: [Shield Gemma](https://huggingface.co/collections/google/shieldgemma-67d130ef8da6af884072a789).\n\n    Note we do not support the image classifier.\n    \"\"\"\n\n    SUPPORTED_MODELS: ClassVar = [\n        \"google/shieldgemma-2b\",\n        \"google/shieldgemma-9b\",\n        \"google/shieldgemma-27b\",\n        \"hf-internal-testing/tiny-random-Gemma3ForCausalLM\",\n    ]\n\n    def __init__(self, policy: str, threshold: float = DEFAULT_THRESHOLD, model_id: str | None = None) -&gt; None:\n        \"\"\"Initialize the ShieldGemma guardrail.\"\"\"\n        super().__init__(model_id)\n        self.policy = policy\n        self.system_prompt = SYSTEM_PROMPT_SHIELD_GEMMA\n        self.threshold = threshold\n\n    def _load_model(self) -&gt; None:\n        from transformers import AutoModelForCausalLM, AutoTokenizer\n\n        self.model = AutoModelForCausalLM.from_pretrained(self.model_id)\n        self.tokenizer = AutoTokenizer.from_pretrained(self.model_id)  # type: ignore[no-untyped-call]\n\n    def _pre_processing(self, input_text: str) -&gt; Any:\n        formatted_prompt = self.system_prompt.format(user_prompt=input_text, safety_policy=self.policy)\n        return super()._pre_processing(formatted_prompt)\n\n    def _post_processing(self, model_outputs: dict[str, Any]) -&gt; GuardrailOutput:\n        from torch.nn.functional import softmax\n\n        logits = model_outputs[\"logits\"]\n        vocab = self.tokenizer.get_vocab()\n        selected_logits = logits[0, -1, [vocab[\"Yes\"], vocab[\"No\"]]]\n        probabilities = softmax(selected_logits, dim=0)\n        score = probabilities[0].item()\n        return GuardrailOutput(valid=score &lt; self.threshold, explanation=None, score=score)\n</code></pre>"},{"location":"api/guardrails/shield_gemma/#any_guardrail.guardrails.shield_gemma.shield_gemma.ShieldGemma.__init__","title":"<code>__init__(policy, threshold=DEFAULT_THRESHOLD, model_id=None)</code>","text":"<p>Initialize the ShieldGemma guardrail.</p> Source code in <code>src/any_guardrail/guardrails/shield_gemma/shield_gemma.py</code> <pre><code>def __init__(self, policy: str, threshold: float = DEFAULT_THRESHOLD, model_id: str | None = None) -&gt; None:\n    \"\"\"Initialize the ShieldGemma guardrail.\"\"\"\n    super().__init__(model_id)\n    self.policy = policy\n    self.system_prompt = SYSTEM_PROMPT_SHIELD_GEMMA\n    self.threshold = threshold\n</code></pre>"},{"location":"cookbook/any_llm_as_a_guardrail/","title":"Using Any LLM as a Guardrail","text":"In\u00a0[\u00a0]: Copied! <pre>%pip install 'any-guardrail'\n\nimport nest_asyncio\n\nnest_asyncio.apply()\n</pre> %pip install 'any-guardrail'  import nest_asyncio  nest_asyncio.apply() <p>We will be using a model from <code>openai</code> by default, but you can check the different providers supported in <code>any-llm</code>:</p> <p>https://mozilla-ai.github.io/any-llm/providers/</p> In\u00a0[8]: Copied! <pre>import os\nfrom getpass import getpass\n\nif \"OPENAI_API_KEY\" not in os.environ:\n    print(\"OPENAI_API_KEY not found in environment!\")\n    api_key = getpass(\"Please enter your OPENAI_API_KEY: \")\n    os.environ[\"OPENAI_API_KEY\"] = api_key\n    print(\"OPENAI_API_KEY set for this session!\")\nelse:\n    print(\"OPENAI_API_KEY found in environment.\")\n</pre> import os from getpass import getpass  if \"OPENAI_API_KEY\" not in os.environ:     print(\"OPENAI_API_KEY not found in environment!\")     api_key = getpass(\"Please enter your OPENAI_API_KEY: \")     os.environ[\"OPENAI_API_KEY\"] = api_key     print(\"OPENAI_API_KEY set for this session!\") else:     print(\"OPENAI_API_KEY found in environment.\") <pre>OPENAI_API_KEY found in environment.\n</pre> In\u00a0[3]: Copied! <pre>from any_guardrail import AnyGuardrail, GuardrailName\n</pre> from any_guardrail import AnyGuardrail, GuardrailName In\u00a0[4]: Copied! <pre>guardrail = AnyGuardrail.create(GuardrailName.ANYLLM)\n</pre> guardrail = AnyGuardrail.create(GuardrailName.ANYLLM) In\u00a0[5]: Copied! <pre>MODEL_ID = \"openai/gpt-5-nano\"\n\nPOLICY = \"\"\"\nYou hate Mondays.\nYou must reject any request related with planning activities on Mondays.\n\"\"\"\n</pre> MODEL_ID = \"openai/gpt-5-nano\"  POLICY = \"\"\" You hate Mondays. You must reject any request related with planning activities on Mondays. \"\"\" In\u00a0[6]: Copied! <pre>guardrail.validate(\"Can you suggest me some restaurants for lunch on Monday?\", policy=POLICY, model_id=MODEL_ID)\n</pre> guardrail.validate(\"Can you suggest me some restaurants for lunch on Monday?\", policy=POLICY, model_id=MODEL_ID) Out[6]: <pre>GuardrailOutput(valid=False, explanation='The user requested planning lunch options specifically for Monday, which is planning activities on Monday. The policy requires rejecting Monday planning requests.', score=0.92)</pre> In\u00a0[7]: Copied! <pre>guardrail.validate(\"Can you suggest me some restaurants for lunch on Friday?\", policy=POLICY, model_id=MODEL_ID)\n</pre> guardrail.validate(\"Can you suggest me some restaurants for lunch on Friday?\", policy=POLICY, model_id=MODEL_ID) Out[7]: <pre>GuardrailOutput(valid=True, explanation='The user asks for lunch restaurant suggestions on Friday. The policy forbids planning activities on Mondays, which does not apply here.', score=0.87)</pre>"},{"location":"cookbook/any_llm_as_a_guardrail/#using-any-llm-as-a-guardrail","title":"Using Any LLM as a Guardrail\u00b6","text":"<p>This tutorial will show you how to use any LLM as a baseline guardrail that can validate input text against a custom <code>policy</code>, using the built-in guardrail based on <code>any-llm</code>.</p>"},{"location":"cookbook/any_llm_as_a_guardrail/#install-dependencies","title":"Install dependencies\u00b6","text":""},{"location":"cookbook/any_llm_as_a_guardrail/#create-the-guardrail","title":"Create the guardrail\u00b6","text":""},{"location":"cookbook/any_llm_as_a_guardrail/#try-it-with-different-models-policies-inputs","title":"Try it with different models / policies / inputs\u00b6","text":""}]}