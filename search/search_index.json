{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Intro","text":"<p><code>any-guardrail</code> is a Python library providing a single interface to different guardrails.</p>"},{"location":"#getting-started","title":"Getting Started","text":"<p>Refer to the Quickstart for instructions on installation and usage.</p>"},{"location":"#guardrails","title":"Guardrails","text":"<p>Refer to Guardrails for the parameters for each guardrail.</p> <p>Refer to GuardrailFactory for how to use the <code>GuardrailFactory</code> object.</p>"},{"location":"#for-ai-systems","title":"For AI Systems","text":"<p>This documentation is available in two AI-friendly formats:</p> <ul> <li>llms.txt - A structured overview with curated links to key documentation sections</li> <li>llms-full.txt - Complete documentation content concatenated into a single file</li> </ul>"},{"location":"quickstart/","title":"Quick Start","text":""},{"location":"quickstart/#quickstart","title":"Quickstart","text":""},{"location":"quickstart/#requirements","title":"Requirements","text":"<ul> <li>Python 3.11 or newer</li> <li>For guardrails that need permission granted on HuggingFace, make sure to get a HuggingFace access token as well. Then log into HuggingFace Hub</li> </ul>"},{"location":"quickstart/#installation","title":"Installation","text":"<p>To install, you can use <code>pip</code>:</p> <pre><code>pip install any-guardrail\n</code></pre> <p>If you plan to use HuggingFace models that require extra permissions, please log into the HuggingFace Hub:</p> <pre><code>pip install --upgrade huggingface_hub\n\nhf auth login\n</code></pre> <p>Make sure to agree to the terms and conditions of the model you are trying to access as well.</p>"},{"location":"quickstart/#basic-usage","title":"Basic Usage","text":"<p><code>GuardrailFactory</code> provides a seamless interface for interacting with the guardrail models. It allows you to see a list of all the supported guardrails, and to instantiate each supported guardrails. Here is a full example:</p> <pre><code>from any_guardrail import GuardrailFactory\nsupported_guardrails = GuardrailFactory.list_all_supported_guardrails() # This will out a list of all guardrail identifiers\nguardrail = GuardrailFactory.create_guardrail(support_guardrails[0])  # will create Deepset's deberta prompt injection defense model\nresult = guardrail.safety_review(\"All smiles from me!\")\nassert result.unsafe == False\n</code></pre>"},{"location":"api/guardrail_factory/","title":"Guardrail Factory","text":""},{"location":"api/guardrail_factory/#guardrail-factory","title":"Guardrail Factory","text":""},{"location":"api/guardrail_factory/#any_guardrail.api","title":"<code>any_guardrail.api</code>","text":""},{"location":"api/guardrail_factory/#any_guardrail.api.GuardrailFactory","title":"<code>GuardrailFactory</code>","text":"<p>Factory class for creating and managing guardrail instances.</p> Source code in <code>src/any_guardrail/api.py</code> <pre><code>class GuardrailFactory:\n    \"\"\"Factory class for creating and managing guardrail instances.\"\"\"\n\n    # This is a class variable so that it can be accessed by the create_guardrail method\n    # and the list_all_supported_guardrails method\n    model_registry = model_registry\n\n    @classmethod\n    def list_all_supported_guardrails(cls) -&gt; List[str]:\n        \"\"\"List all supported guardrails.\"\"\"\n        return list(cls.model_registry.keys())\n\n    @classmethod\n    def create_guardrail(cls, model_id: str, **kwargs: Any) -&gt; Guardrail:\n        \"\"\"Create a guardrail instance.\n\n        Args:\n            model_id: The identifier of the model to use, which will be mapped to the guardrail that uses it.\n            **kwargs: Additional keyword arguments to pass to the guardrail constructor.\n\n        Returns:\n            A guardrail instance.\n        \"\"\"\n        if model_id in cls.model_registry.keys():\n            registry = cls.model_registry[model_id](model_id=model_id, **kwargs)\n            if not isinstance(registry, Guardrail):\n                raise ValueError(f\"{model_id} is not of type Guardrail. Please use the correct model identifier.\")\n            return registry\n        else:\n            raise ValueError(\n                f\"You tried to instantiate {model_id}, which is not a supported guardrail. \"\n                f\"Use list_all_supported_guardrails to see which guardrails are supported.\"\n            )\n</code></pre>"},{"location":"api/guardrail_factory/#any_guardrail.api.GuardrailFactory.create_guardrail","title":"<code>create_guardrail(model_id, **kwargs)</code>  <code>classmethod</code>","text":"<p>Create a guardrail instance.</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>The identifier of the model to use, which will be mapped to the guardrail that uses it.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the guardrail constructor.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Guardrail</code> <p>A guardrail instance.</p> Source code in <code>src/any_guardrail/api.py</code> <pre><code>@classmethod\ndef create_guardrail(cls, model_id: str, **kwargs: Any) -&gt; Guardrail:\n    \"\"\"Create a guardrail instance.\n\n    Args:\n        model_id: The identifier of the model to use, which will be mapped to the guardrail that uses it.\n        **kwargs: Additional keyword arguments to pass to the guardrail constructor.\n\n    Returns:\n        A guardrail instance.\n    \"\"\"\n    if model_id in cls.model_registry.keys():\n        registry = cls.model_registry[model_id](model_id=model_id, **kwargs)\n        if not isinstance(registry, Guardrail):\n            raise ValueError(f\"{model_id} is not of type Guardrail. Please use the correct model identifier.\")\n        return registry\n    else:\n        raise ValueError(\n            f\"You tried to instantiate {model_id}, which is not a supported guardrail. \"\n            f\"Use list_all_supported_guardrails to see which guardrails are supported.\"\n        )\n</code></pre>"},{"location":"api/guardrail_factory/#any_guardrail.api.GuardrailFactory.list_all_supported_guardrails","title":"<code>list_all_supported_guardrails()</code>  <code>classmethod</code>","text":"<p>List all supported guardrails.</p> Source code in <code>src/any_guardrail/api.py</code> <pre><code>@classmethod\ndef list_all_supported_guardrails(cls) -&gt; List[str]:\n    \"\"\"List all supported guardrails.\"\"\"\n    return list(cls.model_registry.keys())\n</code></pre>"},{"location":"api/guardrails/","title":"Guardrails","text":""},{"location":"api/guardrails/#guardrails","title":"Guardrails","text":""},{"location":"api/guardrails/#any_guardrail.guardrails.deepset","title":"<code>any_guardrail.guardrails.deepset</code>","text":""},{"location":"api/guardrails/#any_guardrail.guardrails.deepset.Deepset","title":"<code>Deepset</code>","text":"<p>               Bases: <code>Guardrail</code></p> <p>Wrapper for prompt injection detection model from Deepset. Please see model card for more information: Deepset</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>HuggingFace path to model</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>Only supports Deepset models from HuggingFace</p> Source code in <code>src/any_guardrail/guardrails/deepset.py</code> <pre><code>class Deepset(Guardrail):\n    \"\"\"\n    Wrapper for prompt injection detection model from Deepset. Please see model card for more information:\n    [Deepset](https://huggingface.co/deepset/deberta-v3-base-injection)\n\n    Args:\n        model_id: HuggingFace path to model\n\n    Raises:\n        ValueError: Only supports Deepset models from HuggingFace\n    \"\"\"\n\n    def __init__(self, model_id: str) -&gt; None:\n        super().__init__(model_id)\n        if self.model_id in [\"deepset/deberta-v3-base-injection\"]:\n            self.guardrail = self._model_instantiation()\n        else:\n            raise ValueError(\n                \"Only supports deepset/deberta-v3-base-injection. Please use this path to instantiate model.\"\n            )\n\n    def safety_review(self, input_text: str) -&gt; ClassificationOutput:\n        \"\"\"\n        Classifies whether the provided text is a prompt injection attack or not.\n\n        Args:\n            input_text: the text that you want to check for prompt injection attacks\n        Returns:\n            True if there is a prompt injection attack, False otherwise\n        \"\"\"\n        if isinstance(self.guardrail.model, Pipeline):\n            classification = self.guardrail.model(input_text)\n            return ClassificationOutput(unsafe=classification[0][\"label\"] == DEEPSET_INJECTION_LABEL)\n        else:\n            raise TypeError(\"Using incorrect model type for Deepset.\")\n\n    def _model_instantiation(self) -&gt; GuardrailModel:\n        tokenizer = AutoTokenizer.from_pretrained(self.model_id)  # type: ignore[no-untyped-call]\n        model = AutoModelForSequenceClassification.from_pretrained(self.model_id)\n        pipe = pipeline(\"text-classification\", model=model, tokenizer=tokenizer)\n        return GuardrailModel(model=pipe)\n</code></pre>"},{"location":"api/guardrails/#any_guardrail.guardrails.deepset.Deepset.safety_review","title":"<code>safety_review(input_text)</code>","text":"<p>Classifies whether the provided text is a prompt injection attack or not.</p> <p>Parameters:</p> Name Type Description Default <code>input_text</code> <code>str</code> <p>the text that you want to check for prompt injection attacks</p> required <p>Returns:     True if there is a prompt injection attack, False otherwise</p> Source code in <code>src/any_guardrail/guardrails/deepset.py</code> <pre><code>def safety_review(self, input_text: str) -&gt; ClassificationOutput:\n    \"\"\"\n    Classifies whether the provided text is a prompt injection attack or not.\n\n    Args:\n        input_text: the text that you want to check for prompt injection attacks\n    Returns:\n        True if there is a prompt injection attack, False otherwise\n    \"\"\"\n    if isinstance(self.guardrail.model, Pipeline):\n        classification = self.guardrail.model(input_text)\n        return ClassificationOutput(unsafe=classification[0][\"label\"] == DEEPSET_INJECTION_LABEL)\n    else:\n        raise TypeError(\"Using incorrect model type for Deepset.\")\n</code></pre>"},{"location":"api/guardrails/#any_guardrail.guardrails.duoguard","title":"<code>any_guardrail.guardrails.duoguard</code>","text":""},{"location":"api/guardrails/#any_guardrail.guardrails.duoguard.DuoGuard","title":"<code>DuoGuard</code>","text":"<p>               Bases: <code>Guardrail</code></p> <p>Guardrail that classifies text based on the categories in DUOGUARD_CATEGORIES. For more information, please see the model cards: DuoGuard</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>HuggingFace path to model.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>Only supports DuoGuard models from HuggingFace.</p> Source code in <code>src/any_guardrail/guardrails/duoguard.py</code> <pre><code>class DuoGuard(Guardrail):\n    \"\"\"\n    Guardrail that classifies text based on the categories in DUOGUARD_CATEGORIES. For more information, please see the\n    model cards: [DuoGuard](https://huggingface.co/collections/DuoGuard/duoguard-models-67a29ad8bd579a404e504d21)\n\n    Args:\n        model_id: HuggingFace path to model.\n\n    Raises:\n        ValueError: Only supports DuoGuard models from HuggingFace.\n    \"\"\"\n\n    def __init__(self, model_id: str, threshold: float = DUOGUARD_DEFAULT_THRESHOLD) -&gt; None:\n        super().__init__(model_id)\n        if self.model_id in [\n            \"DuoGuard/DuoGuard-0.5B\",\n            \"DuoGuard/DuoGuard-1B-Llama-3.2-transfer\",\n            \"DuoGuard/DuoGuard-1.5B-transfer\",\n        ]:\n            self.guardrail = self._model_instantiation()\n        else:\n            raise ValueError(\n                \"Must instantiate model using one of the following paths: \"\n                \"\\n\\n DuoGuard/DuoGuard-0.5B \\n DuoGuard/DuoGuard-1B-Llama-3.2-transfer \\n DuoGuard/DuoGuard-1.5B-transfer\"\n            )\n        self.threshold = threshold\n\n    def safety_review(self, input_text: str) -&gt; ClassificationOutput:\n        \"\"\"\n        Classifies text based on DuoGuard categories.\n\n        Args:\n            input_text: text that you want to safety_review.\n        Returns:\n            Whether the output is generally true (bool) and dictionary object with classifications for each category supported by\n            DuoGuard.\n        \"\"\"\n        prob_vector = self._get_probabilities(input_text)\n        overall_label, predicted_labels = self._classification_decision(prob_vector)\n        return ClassificationOutput(unsafe=overall_label, explanation=predicted_labels)\n\n    def _model_instantiation(self) -&gt; GuardrailModel:\n        tokenizer = AutoTokenizer.from_pretrained(self.model_id)  # type: ignore[no-untyped-call]\n        tokenizer.pad_token = tokenizer.eos_token\n        model = AutoModelForSequenceClassification.from_pretrained(self.model_id)\n        return GuardrailModel(model=model, tokenizer=tokenizer)\n\n    def _get_probabilities(self, input_text: str) -&gt; List[float]:\n        \"\"\"\n        Processes the input text to obtain probabilities for each of the DuoGuard categories. It does this by looking at the\n        logits for each category name, and then converting the logits to probabilities. These probabilities will then be used\n        to determine whether, given a threshold, the input text violates any of the safety categories.\n\n        Args:\n            input_text: text that you want to safety_review.\n        Returns:\n            A list of probabilities for each category.\n        \"\"\"\n        if self.guardrail.tokenizer:\n            inputs = self.guardrail.tokenizer(\n                input_text,\n                return_tensors=\"pt\",\n            )\n            if isinstance(self.guardrail.model, PreTrainedModel):\n                with torch.no_grad():\n                    outputs = self.guardrail.model(**inputs)\n                    logits = outputs.logits\n                    probabilities = torch.sigmoid(logits)\n                prob_vector = probabilities[0].tolist()\n            else:\n                raise TypeError(\"Using the incorrect model type for DuoGuard\")\n            return prob_vector\n        else:\n            raise TypeError(\"Did not instantiate tokenizer.\")\n\n    def _classification_decision(self, prob_vector: List[float]) -&gt; Tuple[bool, Dict[str, bool]]:\n        \"\"\"\n        Performs the decision function, as described in the DuoGuard paper (see the huggingface documentation). Can be\n        overridden to define a new decision function using the probability vector.\n\n        Args:\n            prob_vector: a list of each probability that a category has been violated\n        Returns:\n            overall_label: True if one of the safety categories is violated, False otherwise\n            predicted_labels: A mapping between the categories and which are violated. Follows the same schema as overall_label.\n        \"\"\"\n        categories = DUOGUARD_CATEGORIES\n        predicted_labels = {}\n        for cat_name, prob in zip(categories, prob_vector):\n            label = prob &gt; self.threshold\n            predicted_labels[cat_name] = label\n        max_prob = max(prob_vector)\n        overall_label = max_prob &gt; self.threshold\n        return overall_label, predicted_labels\n</code></pre>"},{"location":"api/guardrails/#any_guardrail.guardrails.duoguard.DuoGuard.safety_review","title":"<code>safety_review(input_text)</code>","text":"<p>Classifies text based on DuoGuard categories.</p> <p>Parameters:</p> Name Type Description Default <code>input_text</code> <code>str</code> <p>text that you want to safety_review.</p> required <p>Returns:     Whether the output is generally true (bool) and dictionary object with classifications for each category supported by     DuoGuard.</p> Source code in <code>src/any_guardrail/guardrails/duoguard.py</code> <pre><code>def safety_review(self, input_text: str) -&gt; ClassificationOutput:\n    \"\"\"\n    Classifies text based on DuoGuard categories.\n\n    Args:\n        input_text: text that you want to safety_review.\n    Returns:\n        Whether the output is generally true (bool) and dictionary object with classifications for each category supported by\n        DuoGuard.\n    \"\"\"\n    prob_vector = self._get_probabilities(input_text)\n    overall_label, predicted_labels = self._classification_decision(prob_vector)\n    return ClassificationOutput(unsafe=overall_label, explanation=predicted_labels)\n</code></pre>"},{"location":"api/guardrails/#any_guardrail.guardrails.flowjudge","title":"<code>any_guardrail.guardrails.flowjudge</code>","text":""},{"location":"api/guardrails/#any_guardrail.guardrails.flowjudge.FlowJudgeClass","title":"<code>FlowJudgeClass</code>","text":"<p>               Bases: <code>Guardrail</code></p> <p>Wrapper around FlowJudge, allowing for custom guardrailing based on user defined criteria, metrics, and rubric. Please see the model card for more information: FlowJudge</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>Name of model. Only used for instantiation of FlowJudge.</p> required <code>name</code> <code>str</code> <p>User defined metric name.</p> required <code>criteria</code> <code>str</code> <p>User defined question that they want answered by FlowJudge model.</p> required <code>rubric</code> <code>Dict[int, str]</code> <p>A scoring rubric in a likert scale fashion, providing an integer score and then a description of what the value means.</p> required <code>required_inputs</code> <code>List[str]</code> <p>A list of what is required for the judge to consider.</p> required <code>required_output</code> <code>str</code> <p>What is the expected output from the judge.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>Only supports FlowJudge keywords to instantiate FlowJudge.</p> Source code in <code>src/any_guardrail/guardrails/flowjudge.py</code> <pre><code>class FlowJudgeClass(Guardrail):\n    \"\"\"\n    Wrapper around FlowJudge, allowing for custom guardrailing based on user defined criteria, metrics, and rubric. Please see\n    the model card for more information: [FlowJudge](https://huggingface.co/flowaicom/Flow-Judge-v0.1)\n\n    Args:\n        model_id: Name of model. Only used for instantiation of FlowJudge.\n        name: User defined metric name.\n        criteria: User defined question that they want answered by FlowJudge model.\n        rubric: A scoring rubric in a likert scale fashion, providing an integer score and then a description of what the\n            value means.\n        required_inputs: A list of what is required for the judge to consider.\n        required_output: What is the expected output from the judge.\n\n    Raises:\n        ValueError: Only supports FlowJudge keywords to instantiate FlowJudge.\n    \"\"\"\n\n    def __init__(\n        self,\n        model_id: str,\n        name: str,\n        criteria: str,\n        rubric: Dict[int, str],\n        required_inputs: List[str],\n        required_output: str,\n    ) -&gt; None:\n        super().__init__(model_id)\n        self.metric_name = name\n        self.criteria = criteria\n        self.rubric = rubric\n        self.required_inputs = required_inputs\n        self.required_output = required_output\n        self.metric_prompt = self.define_metric_prompt\n        if model_id in [\"FlowJudge\", \"Flowjudge\", \"flowjudge\"]:\n            self.guardrail = self._model_instantiation()\n        else:\n            raise ValueError(\"You must use one of the following key word arguments: FlowJudge, Flowjudge, flowjudge.\")\n\n    def safety_review(self, inputs: List[Dict[str, str]], output: Dict[str, str]) -&gt; ClassificationOutput:\n        \"\"\"\n        Classifies the desired input and output according to the associated metric provided to the judge.\n\n        Args:\n            inputs: A dictionary mapping the required input names to the inputs.\n            output: A dictionary mapping the required output name to the output.\n        Return:\n            A score from the RubricItems and feedback related to the rubric and criteria.\n        \"\"\"\n        eval_input = EvalInput(inputs=inputs, output=output)\n        if isinstance(self.guardrail.model, FlowJudge):\n            result = self.guardrail.model.evaluate(eval_input, save_results=False)\n        else:\n            raise TypeError(\"Using the wrong GuardrailModel type for FlowJudge.\")\n        return ClassificationOutput(explanation=result.feedback, score=result.score)\n\n    def _model_instantiation(self) -&gt; GuardrailModel:\n        \"\"\"\n        Constructs the FlowJudge model using the defined metric prompt that contains the rubric, criteria, and metric.\n        Returns:\n            judge (FlowJudge): The evaluation model.\n        \"\"\"\n        model = Hf(flash_attention=False)\n        judge = FlowJudge(metric=self.metric_prompt, model=model)  # type: ignore[arg-type]\n        return GuardrailModel(model=judge)\n\n    def define_metric_prompt(self) -&gt; Metric:\n        \"\"\"\n        Constructs the Metric object needed to instantiate the FlowJudge model.\n        Returns:\n            The Metric object used to construct the FlowJudge model.\n        \"\"\"\n        processed_rubric = self._construct_rubric()\n        metric_prompt = Metric(\n            name=self.metric_name,\n            criteria=self.criteria,\n            rubric=processed_rubric,\n            required_inputs=self.required_inputs,\n            required_output=self.required_output,\n        )\n        return metric_prompt\n\n    def _construct_rubric(self) -&gt; List[RubricItem]:\n        \"\"\"\n        Construct the rubric from a user defined rubric dicitionary to construct the Metric object.\n        Returns:\n            List of RubricItem objects.\n        \"\"\"\n        processed_rubric = []\n        for key, value in self.rubric.items():\n            rubric_item = RubricItem(score=key, description=value)\n            processed_rubric.append(rubric_item)\n        return processed_rubric\n</code></pre>"},{"location":"api/guardrails/#any_guardrail.guardrails.flowjudge.FlowJudgeClass.define_metric_prompt","title":"<code>define_metric_prompt()</code>","text":"<p>Constructs the Metric object needed to instantiate the FlowJudge model. Returns:     The Metric object used to construct the FlowJudge model.</p> Source code in <code>src/any_guardrail/guardrails/flowjudge.py</code> <pre><code>def define_metric_prompt(self) -&gt; Metric:\n    \"\"\"\n    Constructs the Metric object needed to instantiate the FlowJudge model.\n    Returns:\n        The Metric object used to construct the FlowJudge model.\n    \"\"\"\n    processed_rubric = self._construct_rubric()\n    metric_prompt = Metric(\n        name=self.metric_name,\n        criteria=self.criteria,\n        rubric=processed_rubric,\n        required_inputs=self.required_inputs,\n        required_output=self.required_output,\n    )\n    return metric_prompt\n</code></pre>"},{"location":"api/guardrails/#any_guardrail.guardrails.flowjudge.FlowJudgeClass.safety_review","title":"<code>safety_review(inputs, output)</code>","text":"<p>Classifies the desired input and output according to the associated metric provided to the judge.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>List[Dict[str, str]]</code> <p>A dictionary mapping the required input names to the inputs.</p> required <code>output</code> <code>Dict[str, str]</code> <p>A dictionary mapping the required output name to the output.</p> required <p>Return:     A score from the RubricItems and feedback related to the rubric and criteria.</p> Source code in <code>src/any_guardrail/guardrails/flowjudge.py</code> <pre><code>def safety_review(self, inputs: List[Dict[str, str]], output: Dict[str, str]) -&gt; ClassificationOutput:\n    \"\"\"\n    Classifies the desired input and output according to the associated metric provided to the judge.\n\n    Args:\n        inputs: A dictionary mapping the required input names to the inputs.\n        output: A dictionary mapping the required output name to the output.\n    Return:\n        A score from the RubricItems and feedback related to the rubric and criteria.\n    \"\"\"\n    eval_input = EvalInput(inputs=inputs, output=output)\n    if isinstance(self.guardrail.model, FlowJudge):\n        result = self.guardrail.model.evaluate(eval_input, save_results=False)\n    else:\n        raise TypeError(\"Using the wrong GuardrailModel type for FlowJudge.\")\n    return ClassificationOutput(explanation=result.feedback, score=result.score)\n</code></pre>"},{"location":"api/guardrails/#any_guardrail.guardrails.glider","title":"<code>any_guardrail.guardrails.glider</code>","text":""},{"location":"api/guardrails/#any_guardrail.guardrails.glider.GLIDER","title":"<code>GLIDER</code>","text":"<p>               Bases: <code>Guardrail</code></p> <p>A prompt based guardrail from Patronus AI that utilizes pass criteria and a rubric to judge text. It outputs its reasoning, highlights for what determined the score, and an integer score. For more information, see the model card: GLIDER</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>HuggingFace path to model.</p> required <code>pass_criteria</code> <code>str</code> <p>A question or description of what you are safety_reviewing.</p> required <code>rubric</code> <code>str</code> <p>A scoring rubric, describing to the model how to score the provided data.</p> required Raise <p>ValueError: Can only use model path to GLIDER from HuggingFace.</p> Source code in <code>src/any_guardrail/guardrails/glider.py</code> <pre><code>class GLIDER(Guardrail):\n    \"\"\"\n    A prompt based guardrail from Patronus AI that utilizes pass criteria and a rubric to judge text. It outputs its reasoning,\n    highlights for what determined the score, and an integer score. For more information, see the model card:\n    [GLIDER](https://huggingface.co/PatronusAI/glider)\n\n    Args:\n        model_id: HuggingFace path to model.\n        pass_criteria: A question or description of what you are safety_reviewing.\n        rubric: A scoring rubric, describing to the model how to score the provided data.\n\n    Raise:\n        ValueError: Can only use model path to GLIDER from HuggingFace.\n    \"\"\"\n\n    def __init__(self, model_id: str, pass_criteria: str, rubric: str) -&gt; None:\n        super().__init__(model_id)\n        if self.model_id in [\"PatronusAI/glider\"]:\n            self.guardrail = self._model_instantiation()\n        else:\n            raise ValueError(\"You must use the following model path: PatronusAI/glider\")\n        self.pass_criteria = pass_criteria\n        self.rubric = rubric\n        self.system_prompt = SYSTEM_PROMPT_GLIDER\n\n    def safety_review(self, input_text: str, output_text: str) -&gt; ClassificationOutput:\n        \"\"\"\n        Uses the provided pass criteria and rubric to just the input and output text provided.\n\n        Args:\n            input_text: the initial text\n            output_text: the subsequent text\n        Returns:\n            An explanation in the format provided by the system prompt\n        \"\"\"\n        data = \"\"\"\n            &lt;INPUT&gt;\n            {input_text}\n            &lt;/INPUT&gt;\n\n            &lt;OUTPUT&gt;\n            {output_text}\n            &lt;/OUTPUT&gt;\n            \"\"\".format(input_text=input_text, output_text=output_text)\n\n        prompt = self.system_prompt.format(data=data, pass_criteria=self.pass_criteria, rubric=self.rubric)\n\n        message = [{\"role\": \"user\", \"content\": prompt}]\n        if isinstance(self.guardrail.model, Pipeline):\n            result = self.guardrail.model(message)\n            return ClassificationOutput(explanation=result)\n        else:\n            raise TypeError(\"Using incorrect model type for GLIDER.\")\n\n    def _model_instantiation(self) -&gt; GuardrailModel:\n        pipe = pipeline(\"text-classification\", self.model_id)\n        return GuardrailModel(model=pipe)\n</code></pre>"},{"location":"api/guardrails/#any_guardrail.guardrails.glider.GLIDER.safety_review","title":"<code>safety_review(input_text, output_text)</code>","text":"<p>Uses the provided pass criteria and rubric to just the input and output text provided.</p> <p>Parameters:</p> Name Type Description Default <code>input_text</code> <code>str</code> <p>the initial text</p> required <code>output_text</code> <code>str</code> <p>the subsequent text</p> required <p>Returns:     An explanation in the format provided by the system prompt</p> Source code in <code>src/any_guardrail/guardrails/glider.py</code> <pre><code>def safety_review(self, input_text: str, output_text: str) -&gt; ClassificationOutput:\n    \"\"\"\n    Uses the provided pass criteria and rubric to just the input and output text provided.\n\n    Args:\n        input_text: the initial text\n        output_text: the subsequent text\n    Returns:\n        An explanation in the format provided by the system prompt\n    \"\"\"\n    data = \"\"\"\n        &lt;INPUT&gt;\n        {input_text}\n        &lt;/INPUT&gt;\n\n        &lt;OUTPUT&gt;\n        {output_text}\n        &lt;/OUTPUT&gt;\n        \"\"\".format(input_text=input_text, output_text=output_text)\n\n    prompt = self.system_prompt.format(data=data, pass_criteria=self.pass_criteria, rubric=self.rubric)\n\n    message = [{\"role\": \"user\", \"content\": prompt}]\n    if isinstance(self.guardrail.model, Pipeline):\n        result = self.guardrail.model(message)\n        return ClassificationOutput(explanation=result)\n    else:\n        raise TypeError(\"Using incorrect model type for GLIDER.\")\n</code></pre>"},{"location":"api/guardrails/#any_guardrail.guardrails.harmguard","title":"<code>any_guardrail.guardrails.harmguard</code>","text":""},{"location":"api/guardrails/#any_guardrail.guardrails.harmguard.HarmGuard","title":"<code>HarmGuard</code>","text":"<p>               Bases: <code>Guardrail</code></p> <p>Prompt injection detection encoder based model. For more information, please see the model card: HarmGuard</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>HuggingFace path to model.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>Can only use model path for HarmGuard from HuggingFace</p> Source code in <code>src/any_guardrail/guardrails/harmguard.py</code> <pre><code>class HarmGuard(Guardrail):\n    \"\"\"\n    Prompt injection detection encoder based model. For more information, please see the model card:\n    [HarmGuard](https://huggingface.co/hbseong/HarmAug-Guard)\n\n    Args:\n        model_id: HuggingFace path to model.\n\n    Raises:\n        ValueError: Can only use model path for HarmGuard from HuggingFace\n    \"\"\"\n\n    def __init__(self, model_id: str, threshold: float = HARMGUARD_DEFAULT_THRESHOLD) -&gt; None:\n        super().__init__(model_id)\n        if self.model_id in [\"hbseong/HarmAug-Guard\"]:\n            self.guardrail = self._model_instantiation()\n        else:\n            raise ValueError(\"Must use the following keyword argument to instantiate model: hbseong/HarmAug-Guard\")\n        self.threshold = threshold\n\n    def safety_review(self, input_text: str, output_text: str = \"\") -&gt; ClassificationOutput:\n        \"\"\"\n        Classifies input text and, optionally, output text for prompt injection detection.\n\n        Args:\n            input_text: the initial text that you want to safety_review\n            output_text: the subsequent text that you want to safety_review\n\n        Returns:\n            True if it is a prompt injection attack, False otherwise, and the associated final score.\n        \"\"\"\n        if self.guardrail.tokenizer:\n            if output_text:\n                inputs = self.guardrail.tokenizer(input_text, return_tensors=\"pt\")\n            else:\n                inputs = self.guardrail.tokenizer(input_text, output_text, return_tensors=\"pt\")\n            if isinstance(self.guardrail.model, PreTrainedModel):\n                with torch.no_grad():\n                    outputs = self.guardrail.model(**inputs)\n                    unsafe_prob = F.softmax(outputs.logits, dim=-1)[:, 1]\n                final_score = unsafe_prob.item()\n            else:\n                raise TypeError(\"Using incorrect model type for HarmGuard.\")\n\n            return ClassificationOutput(unsafe=final_score &gt; self.threshold, score=final_score)\n        else:\n            raise TypeError(\"Did not instantiate tokenizer.\")\n\n    def _model_instantiation(self) -&gt; GuardrailModel:\n        tokenizer = AutoTokenizer.from_pretrained(self.model_id)  # type: ignore[no-untyped-call]\n        model = AutoModelForSequenceClassification.from_pretrained(self.model_id)\n        model.eval()\n        return GuardrailModel(model=model, tokenizer=tokenizer)\n</code></pre>"},{"location":"api/guardrails/#any_guardrail.guardrails.harmguard.HarmGuard.safety_review","title":"<code>safety_review(input_text, output_text='')</code>","text":"<p>Classifies input text and, optionally, output text for prompt injection detection.</p> <p>Parameters:</p> Name Type Description Default <code>input_text</code> <code>str</code> <p>the initial text that you want to safety_review</p> required <code>output_text</code> <code>str</code> <p>the subsequent text that you want to safety_review</p> <code>''</code> <p>Returns:</p> Type Description <code>ClassificationOutput</code> <p>True if it is a prompt injection attack, False otherwise, and the associated final score.</p> Source code in <code>src/any_guardrail/guardrails/harmguard.py</code> <pre><code>def safety_review(self, input_text: str, output_text: str = \"\") -&gt; ClassificationOutput:\n    \"\"\"\n    Classifies input text and, optionally, output text for prompt injection detection.\n\n    Args:\n        input_text: the initial text that you want to safety_review\n        output_text: the subsequent text that you want to safety_review\n\n    Returns:\n        True if it is a prompt injection attack, False otherwise, and the associated final score.\n    \"\"\"\n    if self.guardrail.tokenizer:\n        if output_text:\n            inputs = self.guardrail.tokenizer(input_text, return_tensors=\"pt\")\n        else:\n            inputs = self.guardrail.tokenizer(input_text, output_text, return_tensors=\"pt\")\n        if isinstance(self.guardrail.model, PreTrainedModel):\n            with torch.no_grad():\n                outputs = self.guardrail.model(**inputs)\n                unsafe_prob = F.softmax(outputs.logits, dim=-1)[:, 1]\n            final_score = unsafe_prob.item()\n        else:\n            raise TypeError(\"Using incorrect model type for HarmGuard.\")\n\n        return ClassificationOutput(unsafe=final_score &gt; self.threshold, score=final_score)\n    else:\n        raise TypeError(\"Did not instantiate tokenizer.\")\n</code></pre>"},{"location":"api/guardrails/#any_guardrail.guardrails.injecguard","title":"<code>any_guardrail.guardrails.injecguard</code>","text":""},{"location":"api/guardrails/#any_guardrail.guardrails.injecguard.InjecGuard","title":"<code>InjecGuard</code>","text":"<p>               Bases: <code>Guardrail</code></p> <p>Prompt injection detection encoder based model. For more information, please see the model card: InjecGuard</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>HuggingFace path to model.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>Can only use the model path for InjecGuard from HuggingFace</p> Source code in <code>src/any_guardrail/guardrails/injecguard.py</code> <pre><code>class InjecGuard(Guardrail):\n    \"\"\"\n    Prompt injection detection encoder based model. For more information, please see the model card:\n    [InjecGuard](https://huggingface.co/leolee99/InjecGuard)\n\n    Args:\n        model_id: HuggingFace path to model.\n\n    Raises:\n        ValueError: Can only use the model path for InjecGuard from HuggingFace\n    \"\"\"\n\n    def __init__(self, model_id: str) -&gt; None:\n        super().__init__(model_id)\n        if self.model_id in [\"leolee99/InjecGuard\"]:\n            self.guardrail = self._model_instantiation()\n        else:\n            raise ValueError(\"Must use the following keyword argument to instantiate model: leolee99/InjecGuard\")\n\n    def safety_review(self, input_text: str) -&gt; ClassificationOutput:\n        \"\"\"\n        Classify some text to see if it contains a prompt injection attack.\n\n        Args:\n            input_text: the text to safety_review for prompt injection attacks\n        Returns:\n            True if there is a prompt injection attack, False otherwise\n        \"\"\"\n        if isinstance(self.guardrail.model, Pipeline):\n            classification = self.guardrail.model(input_text)\n            return ClassificationOutput(unsafe=classification[0][\"label\"] == INJECGUARD_LABEL)\n        else:\n            raise TypeError(\"Using incorrect model type for InjecGuard.\")\n\n    def _model_instantiation(self) -&gt; GuardrailModel:\n        tokenizer = AutoTokenizer.from_pretrained(self.model_id)  # type: ignore[no-untyped-call]\n        model = AutoModelForSequenceClassification.from_pretrained(self.model_id)\n        pipe = pipeline(\"text-classification\", model=model, tokenizer=tokenizer)\n        return GuardrailModel(model=pipe)\n</code></pre>"},{"location":"api/guardrails/#any_guardrail.guardrails.injecguard.InjecGuard.safety_review","title":"<code>safety_review(input_text)</code>","text":"<p>Classify some text to see if it contains a prompt injection attack.</p> <p>Parameters:</p> Name Type Description Default <code>input_text</code> <code>str</code> <p>the text to safety_review for prompt injection attacks</p> required <p>Returns:     True if there is a prompt injection attack, False otherwise</p> Source code in <code>src/any_guardrail/guardrails/injecguard.py</code> <pre><code>def safety_review(self, input_text: str) -&gt; ClassificationOutput:\n    \"\"\"\n    Classify some text to see if it contains a prompt injection attack.\n\n    Args:\n        input_text: the text to safety_review for prompt injection attacks\n    Returns:\n        True if there is a prompt injection attack, False otherwise\n    \"\"\"\n    if isinstance(self.guardrail.model, Pipeline):\n        classification = self.guardrail.model(input_text)\n        return ClassificationOutput(unsafe=classification[0][\"label\"] == INJECGUARD_LABEL)\n    else:\n        raise TypeError(\"Using incorrect model type for InjecGuard.\")\n</code></pre>"},{"location":"api/guardrails/#any_guardrail.guardrails.jasper","title":"<code>any_guardrail.guardrails.jasper</code>","text":""},{"location":"api/guardrails/#any_guardrail.guardrails.jasper.Jasper","title":"<code>Jasper</code>","text":"<p>               Bases: <code>Guardrail</code></p> <p>Prompt injection detection encoder based models. For more information, please see the model card: Jasper Deberta Jasper Gelectra</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>HuggingFace path to model.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>Can only use model paths for Jasper models from HuggingFace.</p> Source code in <code>src/any_guardrail/guardrails/jasper.py</code> <pre><code>class Jasper(Guardrail):\n    \"\"\"\n    Prompt injection detection encoder based models. For more information, please see the model card:\n    [Jasper Deberta](https://huggingface.co/JasperLS/deberta-v3-base-injection)\n    [Jasper Gelectra](https://huggingface.co/JasperLS/gelectra-base-injection)\n\n    Args:\n        model_id: HuggingFace path to model.\n\n    Raises:\n        ValueError: Can only use model paths for Jasper models from HuggingFace.\n    \"\"\"\n\n    def __init__(self, model_id: str) -&gt; None:\n        super().__init__(model_id)\n        if self.model_id in [\"JasperLS/deberta-v3-base-injection\", \"JasperLS/gelectra-base-injection\"]:\n            self.guardrail = self._model_instantiation()\n        else:\n            raise ValueError(\n                \"Must use one of the following keyword arguments to instantiate model: \"\n                \"\\n\\n JasperLS/deberta-v3-base-injection \\n JasperLS/gelectra-base-injection\"\n            )\n\n    def safety_review(self, input_text: str) -&gt; ClassificationOutput:\n        \"\"\"\n        Classify some text to see if it contains a prompt injection attack.\n\n        Args:\n            input_text: the text to safety_review for prompt injection attacks\n        Returns:\n            True if there is a prompt injection attack, False otherwise\n        \"\"\"\n        if isinstance(self.guardrail.model, Pipeline):\n            classification = self.guardrail.model(input_text)\n            return ClassificationOutput(unsafe=classification[0][\"label\"] == JASPER_INJECTION_LABEL)\n        else:\n            raise TypeError(\"Using incorrect model type for Jasper models.\")\n\n    def _model_instantiation(self) -&gt; GuardrailModel:\n        tokenizer = AutoTokenizer.from_pretrained(self.model_id)  # type: ignore[no-untyped-call]\n        model = AutoModelForSequenceClassification.from_pretrained(self.model_id)\n        pipe = pipeline(\"text-classification\", model=model, tokenizer=tokenizer)\n        return GuardrailModel(model=pipe)\n</code></pre>"},{"location":"api/guardrails/#any_guardrail.guardrails.jasper.Jasper.safety_review","title":"<code>safety_review(input_text)</code>","text":"<p>Classify some text to see if it contains a prompt injection attack.</p> <p>Parameters:</p> Name Type Description Default <code>input_text</code> <code>str</code> <p>the text to safety_review for prompt injection attacks</p> required <p>Returns:     True if there is a prompt injection attack, False otherwise</p> Source code in <code>src/any_guardrail/guardrails/jasper.py</code> <pre><code>def safety_review(self, input_text: str) -&gt; ClassificationOutput:\n    \"\"\"\n    Classify some text to see if it contains a prompt injection attack.\n\n    Args:\n        input_text: the text to safety_review for prompt injection attacks\n    Returns:\n        True if there is a prompt injection attack, False otherwise\n    \"\"\"\n    if isinstance(self.guardrail.model, Pipeline):\n        classification = self.guardrail.model(input_text)\n        return ClassificationOutput(unsafe=classification[0][\"label\"] == JASPER_INJECTION_LABEL)\n    else:\n        raise TypeError(\"Using incorrect model type for Jasper models.\")\n</code></pre>"},{"location":"api/guardrails/#any_guardrail.guardrails.pangolin","title":"<code>any_guardrail.guardrails.pangolin</code>","text":""},{"location":"api/guardrails/#any_guardrail.guardrails.pangolin.Pangolin","title":"<code>Pangolin</code>","text":"<p>               Bases: <code>Guardrail</code></p> <p>Prompt injection detection encoder based models. For more information, please see the model card: Pangolin Base Pangolin Large</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>HuggingFace path to model.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>Can only use model paths for Pangolin from HuggingFace</p> Source code in <code>src/any_guardrail/guardrails/pangolin.py</code> <pre><code>class Pangolin(Guardrail):\n    \"\"\"\n    Prompt injection detection encoder based models. For more information, please see the model card:\n    [Pangolin Base](https://huggingface.co/dcarpintero/pangolin-guard-base)\n    [Pangolin Large](https://huggingface.co/dcarpintero/pangolin-guard-large)\n\n    Args:\n        model_id: HuggingFace path to model.\n\n    Raises:\n        ValueError: Can only use model paths for Pangolin from HuggingFace\n    \"\"\"\n\n    def __init__(self, model_id: str) -&gt; None:\n        super().__init__(model_id)\n        if self.model_id in [\"dcarpintero/pangolin-guard-large\", \"dcarpintero/pangolin-guard-base\"]:\n            self.guardrail = self._model_instantiation()\n        else:\n            raise ValueError(\n                \"Must use one of the following keyword arguments to instantiate model: \"\n                \"\\n\\n dcarpintero/pangolin-guard-large \\n dcarpintero/pangolin-guard-base\"\n            )\n\n    def safety_review(self, input_text: str) -&gt; ClassificationOutput:\n        \"\"\"\n        Classify some text to see if it contains a prompt injection attack.\n\n        Args:\n            input_text: the text to safety_review for prompt injection attacks\n        Returns:\n            True if there is a prompt injection attack, False otherwise\n        \"\"\"\n        if isinstance(self.guardrail.model, Pipeline):\n            classification = self.guardrail.model(input_text)\n            return ClassificationOutput(unsafe=classification[0][\"label\"] == PANGOLIN_INJECTION_LABEL)\n        else:\n            raise TypeError(\"Using incorrect model type for Pangolin.\")\n\n    def _model_instantiation(self) -&gt; GuardrailModel:\n        pipe = pipeline(\"text-classification\", self.model_id)\n        return GuardrailModel(model=pipe)\n</code></pre>"},{"location":"api/guardrails/#any_guardrail.guardrails.pangolin.Pangolin.safety_review","title":"<code>safety_review(input_text)</code>","text":"<p>Classify some text to see if it contains a prompt injection attack.</p> <p>Parameters:</p> Name Type Description Default <code>input_text</code> <code>str</code> <p>the text to safety_review for prompt injection attacks</p> required <p>Returns:     True if there is a prompt injection attack, False otherwise</p> Source code in <code>src/any_guardrail/guardrails/pangolin.py</code> <pre><code>def safety_review(self, input_text: str) -&gt; ClassificationOutput:\n    \"\"\"\n    Classify some text to see if it contains a prompt injection attack.\n\n    Args:\n        input_text: the text to safety_review for prompt injection attacks\n    Returns:\n        True if there is a prompt injection attack, False otherwise\n    \"\"\"\n    if isinstance(self.guardrail.model, Pipeline):\n        classification = self.guardrail.model(input_text)\n        return ClassificationOutput(unsafe=classification[0][\"label\"] == PANGOLIN_INJECTION_LABEL)\n    else:\n        raise TypeError(\"Using incorrect model type for Pangolin.\")\n</code></pre>"},{"location":"api/guardrails/#any_guardrail.guardrails.protectai","title":"<code>any_guardrail.guardrails.protectai</code>","text":""},{"location":"api/guardrails/#any_guardrail.guardrails.protectai.ProtectAI","title":"<code>ProtectAI</code>","text":"<p>               Bases: <code>Guardrail</code></p> <p>Prompt injection detection encoder based models. For more information, please see the model cards: ProtectA</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>HuggingFace path to model.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>Can only use model paths for ProtectAI from HuggingFace.</p> Source code in <code>src/any_guardrail/guardrails/protectai.py</code> <pre><code>class ProtectAI(Guardrail):\n    \"\"\"\n    Prompt injection detection encoder based models. For more information, please see the model cards:\n    [ProtectA](https://huggingface.co/collections/protectai/llm-security-65c1f17a11c4251eeab53f40)\n\n    Args:\n        model_id: HuggingFace path to model.\n\n    Raises:\n        ValueError: Can only use model paths for ProtectAI from HuggingFace.\n    \"\"\"\n\n    def __init__(self, model_id: str) -&gt; None:\n        super().__init__(model_id)\n        if self.model_id in [\n            \"protectai/deberta-v3-base-prompt-injection\",\n            \"protectai/deberta-v3-small-prompt-injection-v2\",\n            \"protectai/deberta-v3-base-prompt-injection-v2\",\n        ]:\n            self.guardrail = self._model_instantiation()\n        else:\n            raise ValueError(\n                \"Must use one of the following keyword arguments to instantiate model: \"\n                \"\\n\\n protectai/deberta-v3-base-prompt-injection \\n protectai/deberta-v3-small-prompt-injection-v2 \\n\"\n                \"protectai/deberta-v3-base-prompt-injection-v2\"\n            )\n\n    def safety_review(self, input_text: str) -&gt; ClassificationOutput:\n        \"\"\"\n        Classify some text to see if it contains a prompt injection attack.\n\n        Args:\n            input_text: the text to safety_review for prompt injection attacks\n        Returns:\n            True if there is a prompt injection attack, False otherwise\n        \"\"\"\n        if isinstance(self.guardrail.model, Pipeline):\n            classification = self.guardrail.model(input_text)\n            return ClassificationOutput(unsafe=classification[0][\"label\"] == PROTECTAI_INJECTION_LABEL)\n        else:\n            raise TypeError(\"Using incorrect model type to call ProtectAI.\")\n\n    def _model_instantiation(self) -&gt; GuardrailModel:\n        tokenizer = AutoTokenizer.from_pretrained(self.model_id)  # type: ignore[no-untyped-call]\n        model = AutoModelForSequenceClassification.from_pretrained(self.model_id)\n        pipe = pipeline(\"text-classification\", model=model, tokenizer=tokenizer)\n        return GuardrailModel(model=pipe)\n</code></pre>"},{"location":"api/guardrails/#any_guardrail.guardrails.protectai.ProtectAI.safety_review","title":"<code>safety_review(input_text)</code>","text":"<p>Classify some text to see if it contains a prompt injection attack.</p> <p>Parameters:</p> Name Type Description Default <code>input_text</code> <code>str</code> <p>the text to safety_review for prompt injection attacks</p> required <p>Returns:     True if there is a prompt injection attack, False otherwise</p> Source code in <code>src/any_guardrail/guardrails/protectai.py</code> <pre><code>def safety_review(self, input_text: str) -&gt; ClassificationOutput:\n    \"\"\"\n    Classify some text to see if it contains a prompt injection attack.\n\n    Args:\n        input_text: the text to safety_review for prompt injection attacks\n    Returns:\n        True if there is a prompt injection attack, False otherwise\n    \"\"\"\n    if isinstance(self.guardrail.model, Pipeline):\n        classification = self.guardrail.model(input_text)\n        return ClassificationOutput(unsafe=classification[0][\"label\"] == PROTECTAI_INJECTION_LABEL)\n    else:\n        raise TypeError(\"Using incorrect model type to call ProtectAI.\")\n</code></pre>"},{"location":"api/guardrails/#any_guardrail.guardrails.sentinel","title":"<code>any_guardrail.guardrails.sentinel</code>","text":""},{"location":"api/guardrails/#any_guardrail.guardrails.sentinel.Sentinel","title":"<code>Sentinel</code>","text":"<p>               Bases: <code>Guardrail</code></p> <p>Prompt injection detection encoder based model. For more information, please see the model card: Sentinel</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>HuggingFace path to model.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>Can only use model path for Sentinel from HuggingFace.</p> Source code in <code>src/any_guardrail/guardrails/sentinel.py</code> <pre><code>class Sentinel(Guardrail):\n    \"\"\"\n    Prompt injection detection encoder based model. For more information, please see the model card:\n    [Sentinel](https://huggingface.co/qualifire/prompt-injection-sentinel)\n\n    Args:\n        model_id: HuggingFace path to model.\n\n    Raises:\n        ValueError: Can only use model path for Sentinel from HuggingFace.\n    \"\"\"\n\n    def __init__(self, model_id: str) -&gt; None:\n        super().__init__(model_id)\n        if self.model_id in [\"qualifire/prompt-injection-sentinel\"]:\n            self.guardrail = self._model_instantiation()\n        else:\n            raise ValueError(\n                \"Must use the following keyword argument to instantiate model: qualifire/prompt-injection-sentinel\"\n            )\n\n    def safety_review(self, input_text: str) -&gt; ClassificationOutput:\n        \"\"\"\n        Classify some text to see if it contains a prompt injection attack.\n\n        Args:\n            input_text: the text to safety_review for prompt injection attacks\n        Returns:\n            True if there is a prompt injection attack, False otherwise\n        \"\"\"\n        if isinstance(self.guardrail.model, Pipeline):\n            classification = self.guardrail.model(input_text)\n            return ClassificationOutput(unsafe=classification[0][\"label\"] == SENTINEL_INJECTION_LABEL)\n        else:\n            raise TypeError(\"Using incorrect model type for Sentinel.\")\n\n    def _model_instantiation(self) -&gt; GuardrailModel:\n        tokenizer = AutoTokenizer.from_pretrained(self.model_id)  # type: ignore[no-untyped-call]\n        model = AutoModelForSequenceClassification.from_pretrained(self.model_id)\n        pipe = pipeline(\"text-classification\", model=model, tokenizer=tokenizer)\n        return GuardrailModel(model=pipe)\n</code></pre>"},{"location":"api/guardrails/#any_guardrail.guardrails.sentinel.Sentinel.safety_review","title":"<code>safety_review(input_text)</code>","text":"<p>Classify some text to see if it contains a prompt injection attack.</p> <p>Parameters:</p> Name Type Description Default <code>input_text</code> <code>str</code> <p>the text to safety_review for prompt injection attacks</p> required <p>Returns:     True if there is a prompt injection attack, False otherwise</p> Source code in <code>src/any_guardrail/guardrails/sentinel.py</code> <pre><code>def safety_review(self, input_text: str) -&gt; ClassificationOutput:\n    \"\"\"\n    Classify some text to see if it contains a prompt injection attack.\n\n    Args:\n        input_text: the text to safety_review for prompt injection attacks\n    Returns:\n        True if there is a prompt injection attack, False otherwise\n    \"\"\"\n    if isinstance(self.guardrail.model, Pipeline):\n        classification = self.guardrail.model(input_text)\n        return ClassificationOutput(unsafe=classification[0][\"label\"] == SENTINEL_INJECTION_LABEL)\n    else:\n        raise TypeError(\"Using incorrect model type for Sentinel.\")\n</code></pre>"},{"location":"api/guardrails/#any_guardrail.guardrails.shield_gemma","title":"<code>any_guardrail.guardrails.shield_gemma</code>","text":""},{"location":"api/guardrails/#any_guardrail.guardrails.shield_gemma.ShieldGemma","title":"<code>ShieldGemma</code>","text":"<p>               Bases: <code>Guardrail</code></p> <p>Wrapper class for Google ShieldGemma models. For more information, please visit the model cards: Shield Gemma</p> <p>Note we do not support the image classifier.</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>HuggingFace path to model.</p> required <code>policy</code> <code>str</code> <p>The safety policy to enforce.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>Can only use model_ids to ShieldGemma from HuggingFace.</p> Source code in <code>src/any_guardrail/guardrails/shield_gemma.py</code> <pre><code>class ShieldGemma(Guardrail):\n    \"\"\"\n    Wrapper class for Google ShieldGemma models. For more information, please visit the model cards:\n    [Shield Gemma](https://huggingface.co/collections/google/shieldgemma-67d130ef8da6af884072a789)\n\n    Note we do not support the image classifier.\n\n    Args:\n        model_id: HuggingFace path to model.\n        policy: The safety policy to enforce.\n\n    Raises:\n        ValueError: Can only use model_ids to ShieldGemma from HuggingFace.\n    \"\"\"\n\n    def __init__(self, model_id: str, policy: str, threshold: float = DEFAULT_THRESHOLD) -&gt; None:\n        super().__init__(model_id)\n        supported_models = [\n            \"google/shieldgemma-2b\",\n            \"google/shieldgemma-9b\",\n            \"google/shieldgemma-27b\",\n            \"hf-internal-testing/tiny-random-Gemma3ForCausalLM\",\n        ]\n        if self.model_id in supported_models:\n            self.guardrail = self._model_instantiation()\n        else:\n            raise ValueError(\n                f\"Must use one of the following keyword arguments to instantiate model: \\n\\n {supported_models}\"\n            )\n        self.policy = policy\n        self.system_prompt = SYSTEM_PROMPT_SHIELD_GEMMA\n        self.threshold = threshold\n\n    def safety_review(self, input_text: str) -&gt; ClassificationOutput:\n        \"\"\"\n        Classify input_text according to the safety policy.\n\n        Args:\n            input_text: the text you want to safety_review based on the policy\n        Returns:\n            True if the text violates the policy, False otherwise\n        \"\"\"\n        formatted_prompt = self.system_prompt.format(user_prompt=input_text, safety_policy=self.policy)\n        if self.guardrail.tokenizer:\n            if isinstance(self.guardrail.model, PreTrainedModel):\n                inputs = self.guardrail.tokenizer(formatted_prompt, return_tensors=\"pt\").to(self.guardrail.model.device)\n                with torch.no_grad():\n                    logits = self.guardrail.model(**inputs).logits\n            else:\n                raise TypeError(\"Using wrong model type to instantiate Shield Gemma models.\")\n            vocab = self.guardrail.tokenizer.get_vocab()\n            selected_logits = logits[0, -1, [vocab[\"Yes\"], vocab[\"No\"]]]\n            probabilities = softmax(selected_logits, dim=0)\n            score = probabilities[0].item()\n\n            return ClassificationOutput(unsafe=score &gt; self.threshold)\n        else:\n            raise TypeError(\"Did not instantiate tokenizer.\")\n\n    def _model_instantiation(self) -&gt; GuardrailModel:\n        tokenizer = AutoTokenizer.from_pretrained(self.model_id)  # type: ignore[no-untyped-call]\n        model = AutoModelForCausalLM.from_pretrained(self.model_id, device_map=\"auto\", torch_dtype=torch.bfloat16)\n        return GuardrailModel(model=model, tokenizer=tokenizer)\n</code></pre>"},{"location":"api/guardrails/#any_guardrail.guardrails.shield_gemma.ShieldGemma.safety_review","title":"<code>safety_review(input_text)</code>","text":"<p>Classify input_text according to the safety policy.</p> <p>Parameters:</p> Name Type Description Default <code>input_text</code> <code>str</code> <p>the text you want to safety_review based on the policy</p> required <p>Returns:     True if the text violates the policy, False otherwise</p> Source code in <code>src/any_guardrail/guardrails/shield_gemma.py</code> <pre><code>def safety_review(self, input_text: str) -&gt; ClassificationOutput:\n    \"\"\"\n    Classify input_text according to the safety policy.\n\n    Args:\n        input_text: the text you want to safety_review based on the policy\n    Returns:\n        True if the text violates the policy, False otherwise\n    \"\"\"\n    formatted_prompt = self.system_prompt.format(user_prompt=input_text, safety_policy=self.policy)\n    if self.guardrail.tokenizer:\n        if isinstance(self.guardrail.model, PreTrainedModel):\n            inputs = self.guardrail.tokenizer(formatted_prompt, return_tensors=\"pt\").to(self.guardrail.model.device)\n            with torch.no_grad():\n                logits = self.guardrail.model(**inputs).logits\n        else:\n            raise TypeError(\"Using wrong model type to instantiate Shield Gemma models.\")\n        vocab = self.guardrail.tokenizer.get_vocab()\n        selected_logits = logits[0, -1, [vocab[\"Yes\"], vocab[\"No\"]]]\n        probabilities = softmax(selected_logits, dim=0)\n        score = probabilities[0].item()\n\n        return ClassificationOutput(unsafe=score &gt; self.threshold)\n    else:\n        raise TypeError(\"Did not instantiate tokenizer.\")\n</code></pre>"}]}