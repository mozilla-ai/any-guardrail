{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Intro","text":"<p><code>any-guardrail</code> is a Python library providing a single interface to different guardrails.</p>"},{"location":"#getting-started","title":"Getting Started","text":"<p>Refer to the Quickstart for instructions on installation and usage.</p>"},{"location":"#guardrails","title":"Guardrails","text":"<p>Refer to Guardrails for the parameters for each guardrail.</p> <p>Refer to AnyGuardrail for how to use the <code>AnyGuardrail</code> object.</p>"},{"location":"#for-ai-systems","title":"For AI Systems","text":"<p>This documentation is available in two AI-friendly formats:</p> <ul> <li>llms.txt - A structured overview with curated links to key documentation sections</li> <li>llms-full.txt - Complete documentation content concatenated into a single file</li> </ul>"},{"location":"quickstart/","title":"Quick Start","text":""},{"location":"quickstart/#requirements","title":"Requirements","text":"<ul> <li>Python 3.11 or newer</li> </ul>"},{"location":"quickstart/#installation","title":"Installation","text":"<p>Install with <code>pip</code>:</p> <pre><code>pip install any-guardrail\n</code></pre>"},{"location":"quickstart/#basic-usage","title":"Basic Usage","text":"<p><code>AnyGuardrail</code> provides a seamless interface for interacting with the guardrail models. It allows you to see a list of all the supported guardrails, and to instantiate each supported guardrails. Here is a full example:</p> <pre><code>from any_guardrail import AnyGuardrail, GuardrailName, GuardrailOutput\nsupported_guardrails = AnyGuardrail.list_all_supported_guardrails() # This will out a list of all guardrail identifiers\nguardrail = AnyGuardrail.create_guardrail(model_id=\"google/shieldgemma-2b\", guardrail_name=GuardrailName.SHIELD_GEMMA)\nresult: GuardrailOutput = guardrail.validate(\"All smiles from me!\")\nassert result.unsafe == False\n</code></pre>"},{"location":"quickstart/#troubleshooting","title":"Troubleshooting","text":"<p>Some of the models on HuggingFace require extra permissions to use. To do this, you'll need to create a HuggingFace profile and manually go through the permissions. Then, you'll need to download the HuggingFace Hub and login. One way to do this is:</p> <pre><code>pip install --upgrade huggingface_hub\n\nhf auth login\n</code></pre> <p>More information can be found here: HuggingFace Hub</p>"},{"location":"api/any_guardrail/","title":"AnyGuardrail","text":""},{"location":"api/any_guardrail/#anyguardrail","title":"AnyGuardrail","text":""},{"location":"api/any_guardrail/#any_guardrail.api.AnyGuardrail","title":"<code>any_guardrail.api.AnyGuardrail</code>","text":"<p>Factory class for creating guardrail instances.</p> Source code in <code>src/any_guardrail/api.py</code> <pre><code>class AnyGuardrail:\n    \"\"\"Factory class for creating guardrail instances.\"\"\"\n\n    @classmethod\n    def get_supported_guardrails(cls) -&gt; List[GuardrailName]:\n        \"\"\"List all supported guardrails.\"\"\"\n        return list(GuardrailName)\n\n    @classmethod\n    def create_guardrail(cls, guardrail_name: GuardrailName, model_id: str, **kwargs: Any) -&gt; Guardrail:\n        \"\"\"Create a guardrail instance.\n\n        Args:\n            guardrail_name: The name of the guardrail to use.\n            model_id: The identifier of the model to use, which will be mapped to the guardrail that uses it.\n            **kwargs: Additional keyword arguments to pass to the guardrail constructor.\n\n        Returns:\n            A guardrail instance.\n        \"\"\"\n        guardrail_module_name = f\"{guardrail_name.value}\"\n        module_path = f\"any_guardrail.guardrails.{guardrail_module_name}\"\n\n        module = importlib.import_module(module_path)\n        parts = re.split(r\"[^A-Za-z0-9]+\", guardrail_module_name)\n        candidate_name = \"\".join(p.capitalize() for p in parts if p)\n        guardrail_class = getattr(module, candidate_name, None)\n        if inspect.isclass(guardrail_class) and issubclass(guardrail_class, Guardrail):\n            return guardrail_class(model_id=model_id, **kwargs)\n\n        raise ImportError(f\"Could not resolve guardrail class for '{guardrail_module_name}' in {module.__name__}\")\n</code></pre>"},{"location":"api/any_guardrail/#any_guardrail.api.AnyGuardrail.create_guardrail","title":"<code>create_guardrail(guardrail_name, model_id, **kwargs)</code>  <code>classmethod</code>","text":"<p>Create a guardrail instance.</p> <p>Parameters:</p> Name Type Description Default <code>guardrail_name</code> <code>GuardrailName</code> <p>The name of the guardrail to use.</p> required <code>model_id</code> <code>str</code> <p>The identifier of the model to use, which will be mapped to the guardrail that uses it.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the guardrail constructor.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Guardrail</code> <p>A guardrail instance.</p> Source code in <code>src/any_guardrail/api.py</code> <pre><code>@classmethod\ndef create_guardrail(cls, guardrail_name: GuardrailName, model_id: str, **kwargs: Any) -&gt; Guardrail:\n    \"\"\"Create a guardrail instance.\n\n    Args:\n        guardrail_name: The name of the guardrail to use.\n        model_id: The identifier of the model to use, which will be mapped to the guardrail that uses it.\n        **kwargs: Additional keyword arguments to pass to the guardrail constructor.\n\n    Returns:\n        A guardrail instance.\n    \"\"\"\n    guardrail_module_name = f\"{guardrail_name.value}\"\n    module_path = f\"any_guardrail.guardrails.{guardrail_module_name}\"\n\n    module = importlib.import_module(module_path)\n    parts = re.split(r\"[^A-Za-z0-9]+\", guardrail_module_name)\n    candidate_name = \"\".join(p.capitalize() for p in parts if p)\n    guardrail_class = getattr(module, candidate_name, None)\n    if inspect.isclass(guardrail_class) and issubclass(guardrail_class, Guardrail):\n        return guardrail_class(model_id=model_id, **kwargs)\n\n    raise ImportError(f\"Could not resolve guardrail class for '{guardrail_module_name}' in {module.__name__}\")\n</code></pre>"},{"location":"api/any_guardrail/#any_guardrail.api.AnyGuardrail.get_supported_guardrails","title":"<code>get_supported_guardrails()</code>  <code>classmethod</code>","text":"<p>List all supported guardrails.</p> Source code in <code>src/any_guardrail/api.py</code> <pre><code>@classmethod\ndef get_supported_guardrails(cls) -&gt; List[GuardrailName]:\n    \"\"\"List all supported guardrails.\"\"\"\n    return list(GuardrailName)\n</code></pre>"},{"location":"api/guardrails/","title":"Guardrails","text":""},{"location":"api/guardrails/#any_guardrail.guardrails.deepset","title":"<code>any_guardrail.guardrails.deepset</code>","text":""},{"location":"api/guardrails/#any_guardrail.guardrails.deepset.Deepset","title":"<code>Deepset</code>","text":"<p>               Bases: <code>Guardrail</code></p> <p>Wrapper for prompt injection detection model from Deepset. Please see model card for more information: Deepset</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>HuggingFace path to model</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>Only supports Deepset models from HuggingFace</p> Source code in <code>src/any_guardrail/guardrails/deepset.py</code> <pre><code>class Deepset(Guardrail):\n    \"\"\"\n    Wrapper for prompt injection detection model from Deepset. Please see model card for more information:\n    [Deepset](https://huggingface.co/deepset/deberta-v3-base-injection)\n\n    Args:\n        model_id: HuggingFace path to model\n\n    Raises:\n        ValueError: Only supports Deepset models from HuggingFace\n    \"\"\"\n\n    SUPPORTED_MODELS = [\"deepset/deberta-v3-base-injection\"]\n\n    def validate(self, input_text: str) -&gt; GuardrailOutput:\n        \"\"\"\n        Classifies whether the provided text is a prompt injection attack or not.\n\n        Args:\n            input_text: the text that you want to check for prompt injection attacks\n        Returns:\n            True if there is a prompt injection attack, False otherwise\n        \"\"\"\n        classification = self._inference(input_text)\n        return GuardrailOutput(unsafe=self._post_processing(classification))\n\n    def _load_model(self) -&gt; None:\n        tokenizer = AutoTokenizer.from_pretrained(self.model_id)  # type: ignore[no-untyped-call]\n        model = AutoModelForSequenceClassification.from_pretrained(self.model_id)\n        pipe = pipeline(\"text-classification\", model=model, tokenizer=tokenizer)\n        self.model = pipe\n        self.tokenizer = tokenizer\n\n    def _inference(self, input_text: str) -&gt; List[Dict[str, str | float]]:\n        return self.model(input_text)\n\n    def _post_processing(self, classification: List[Dict[str, str | float]]) -&gt; bool:\n        return classification[0][\"label\"] == DEEPSET_INJECTION_LABEL\n</code></pre>"},{"location":"api/guardrails/#any_guardrail.guardrails.deepset.Deepset.validate","title":"<code>validate(input_text)</code>","text":"<p>Classifies whether the provided text is a prompt injection attack or not.</p> <p>Parameters:</p> Name Type Description Default <code>input_text</code> <code>str</code> <p>the text that you want to check for prompt injection attacks</p> required <p>Returns:     True if there is a prompt injection attack, False otherwise</p> Source code in <code>src/any_guardrail/guardrails/deepset.py</code> <pre><code>def validate(self, input_text: str) -&gt; GuardrailOutput:\n    \"\"\"\n    Classifies whether the provided text is a prompt injection attack or not.\n\n    Args:\n        input_text: the text that you want to check for prompt injection attacks\n    Returns:\n        True if there is a prompt injection attack, False otherwise\n    \"\"\"\n    classification = self._inference(input_text)\n    return GuardrailOutput(unsafe=self._post_processing(classification))\n</code></pre>"},{"location":"api/guardrails/#any_guardrail.guardrails.duoguard","title":"<code>any_guardrail.guardrails.duoguard</code>","text":""},{"location":"api/guardrails/#any_guardrail.guardrails.duoguard.DuoGuard","title":"<code>DuoGuard</code>","text":"<p>               Bases: <code>Guardrail</code></p> <p>Guardrail that classifies text based on the categories in DUOGUARD_CATEGORIES. For more information, please see the model cards: DuoGuard</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>HuggingFace path to model.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>Only supports DuoGuard models from HuggingFace.</p> Source code in <code>src/any_guardrail/guardrails/duoguard.py</code> <pre><code>class DuoGuard(Guardrail):\n    \"\"\"\n    Guardrail that classifies text based on the categories in DUOGUARD_CATEGORIES. For more information, please see the\n    model cards: [DuoGuard](https://huggingface.co/collections/DuoGuard/duoguard-models-67a29ad8bd579a404e504d21)\n\n    Args:\n        model_id: HuggingFace path to model.\n\n    Raises:\n        ValueError: Only supports DuoGuard models from HuggingFace.\n    \"\"\"\n\n    SUPPORTED_MODELS = [\n        \"DuoGuard/DuoGuard-0.5B\",\n        \"DuoGuard/DuoGuard-1B-Llama-3.2-transfer\",\n        \"DuoGuard/DuoGuard-1.5B-transfer\",\n    ]\n\n    def __init__(self, model_id: str, threshold: float = DUOGUARD_DEFAULT_THRESHOLD) -&gt; None:\n        super().__init__(model_id)\n        self.threshold = threshold\n\n    def validate(self, input_text: str) -&gt; GuardrailOutput:\n        \"\"\"\n        Classifies text based on DuoGuard categories.\n\n        Args:\n            input_text: text that you want to validate.\n        Returns:\n            Whether the output is generally true (bool) and dictionary object with classifications for each category supported by\n            DuoGuard.\n        \"\"\"\n        input_tensors = self._pre_processing(input_text)\n        prob_vector = self._inference(input_tensors)\n        overall_label, predicted_labels = self._post_processing(prob_vector)\n        return GuardrailOutput(unsafe=overall_label, explanation=predicted_labels)\n\n    def _load_model(self) -&gt; None:\n        tokenizer = AutoTokenizer.from_pretrained(self.model_id)  # type: ignore[no-untyped-call]\n        tokenizer.pad_token = tokenizer.eos_token\n        model = AutoModelForSequenceClassification.from_pretrained(self.model_id)\n        self.model = model\n        self.tokenizer = tokenizer\n\n    def _pre_processing(self, input_text: str) -&gt; torch.Tensor:\n        inputs = self.tokenizer(\n            input_text,\n            return_tensors=\"pt\",\n        )\n        return inputs\n\n    def _inference(self, inputs: torch.Tensor) -&gt; List[float]:\n        \"\"\"\n        Processes the input text to obtain probabilities for each of the DuoGuard categories. It does this by looking at the\n        logits for each category name, and then converting the logits to probabilities. These probabilities will then be used\n        to determine whether, given a threshold, the input text violates any of the safety categories.\n\n        Args:\n            input_text: text that you want to validate.\n        Returns:\n            A list of probabilities for each category.\n        \"\"\"\n        with torch.no_grad():\n            outputs = self.model(**inputs)\n            logits = outputs.logits\n            probabilities = torch.sigmoid(logits)\n        prob_vector = probabilities[0].tolist()\n        return prob_vector\n\n    def _post_processing(self, prob_vector: List[float]) -&gt; Tuple[bool, Dict[str, bool]]:\n        \"\"\"\n        Performs the decision function, as described in the DuoGuard paper (see the huggingface documentation). Can be\n        overridden to define a new decision function using the probability vector.\n\n        Args:\n            prob_vector: a list of each probability that a category has been violated\n        Returns:\n            overall_label: True if one of the safety categories is violated, False otherwise\n            predicted_labels: A mapping between the categories and which are violated. Follows the same schema as overall_label.\n        \"\"\"\n        categories = DUOGUARD_CATEGORIES\n        predicted_labels = {}\n        for cat_name, prob in zip(categories, prob_vector):\n            label = prob &gt; self.threshold\n            predicted_labels[cat_name] = label\n        max_prob = max(prob_vector)\n        overall_label = max_prob &gt; self.threshold\n        return overall_label, predicted_labels\n</code></pre>"},{"location":"api/guardrails/#any_guardrail.guardrails.duoguard.DuoGuard.validate","title":"<code>validate(input_text)</code>","text":"<p>Classifies text based on DuoGuard categories.</p> <p>Parameters:</p> Name Type Description Default <code>input_text</code> <code>str</code> <p>text that you want to validate.</p> required <p>Returns:     Whether the output is generally true (bool) and dictionary object with classifications for each category supported by     DuoGuard.</p> Source code in <code>src/any_guardrail/guardrails/duoguard.py</code> <pre><code>def validate(self, input_text: str) -&gt; GuardrailOutput:\n    \"\"\"\n    Classifies text based on DuoGuard categories.\n\n    Args:\n        input_text: text that you want to validate.\n    Returns:\n        Whether the output is generally true (bool) and dictionary object with classifications for each category supported by\n        DuoGuard.\n    \"\"\"\n    input_tensors = self._pre_processing(input_text)\n    prob_vector = self._inference(input_tensors)\n    overall_label, predicted_labels = self._post_processing(prob_vector)\n    return GuardrailOutput(unsafe=overall_label, explanation=predicted_labels)\n</code></pre>"},{"location":"api/guardrails/#any_guardrail.guardrails.flowjudge","title":"<code>any_guardrail.guardrails.flowjudge</code>","text":""},{"location":"api/guardrails/#any_guardrail.guardrails.flowjudge.FlowJudgeClass","title":"<code>FlowJudgeClass</code>","text":"<p>               Bases: <code>Guardrail</code></p> <p>Wrapper around FlowJudge, allowing for custom guardrailing based on user defined criteria, metrics, and rubric. Please see the model card for more information: FlowJudge</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>Name of model. Only used for instantiation of FlowJudge.</p> required <code>name</code> <code>str</code> <p>User defined metric name.</p> required <code>criteria</code> <code>str</code> <p>User defined question that they want answered by FlowJudge model.</p> required <code>rubric</code> <code>Dict[int, str]</code> <p>A scoring rubric in a likert scale fashion, providing an integer score and then a description of what the value means.</p> required <code>required_inputs</code> <code>List[str]</code> <p>A list of what is required for the judge to consider.</p> required <code>required_output</code> <code>str</code> <p>What is the expected output from the judge.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>Only supports FlowJudge keywords to instantiate FlowJudge.</p> Source code in <code>src/any_guardrail/guardrails/flowjudge.py</code> <pre><code>class FlowJudgeClass(Guardrail):\n    \"\"\"\n    Wrapper around FlowJudge, allowing for custom guardrailing based on user defined criteria, metrics, and rubric. Please see\n    the model card for more information: [FlowJudge](https://huggingface.co/flowaicom/Flow-Judge-v0.1)\n\n    Args:\n        model_id: Name of model. Only used for instantiation of FlowJudge.\n        name: User defined metric name.\n        criteria: User defined question that they want answered by FlowJudge model.\n        rubric: A scoring rubric in a likert scale fashion, providing an integer score and then a description of what the\n            value means.\n        required_inputs: A list of what is required for the judge to consider.\n        required_output: What is the expected output from the judge.\n\n    Raises:\n        ValueError: Only supports FlowJudge keywords to instantiate FlowJudge.\n    \"\"\"\n\n    SUPPORTED_MODELS = [\"FlowJudge\"]\n\n    def __init__(\n        self,\n        model_id: str,\n        name: str,\n        criteria: str,\n        rubric: Dict[int, str],\n        required_inputs: List[str],\n        required_output: str,\n    ) -&gt; None:\n        self.metric_name = name\n        self.criteria = criteria\n        self.rubric = rubric\n        self.required_inputs = required_inputs\n        self.required_output = required_output\n        self.metric_prompt = self._define_metric_prompt()\n        super().__init__(model_id)\n\n    def validate(self, inputs: List[Dict[str, str]], output: Dict[str, str]) -&gt; GuardrailOutput:\n        \"\"\"\n        Classifies the desired input and output according to the associated metric provided to the judge.\n\n        Args:\n            inputs: A dictionary mapping the required input names to the inputs.\n            output: A dictionary mapping the required output name to the output.\n        Return:\n            A score from the RubricItems and feedback related to the rubric and criteria.\n        \"\"\"\n        eval_input = self._pre_processing(inputs, output)\n        result = self._inference(eval_input)\n        return GuardrailOutput(explanation=result.feedback, score=result.score)\n\n    def _load_model(self) -&gt; None:\n        \"\"\"\n        Constructs the FlowJudge model using the defined metric prompt that contains the rubric, criteria, and metric.\n        Returns:\n            judge: The evaluation model.\n        \"\"\"\n        model = Hf(flash_attention=False)\n        judge = FlowJudge(metric=self.metric_prompt, model=model)  # type: ignore[arg-type]\n        self.model = judge\n\n    def _define_metric_prompt(self) -&gt; Metric:\n        \"\"\"\n        Constructs the Metric object needed to instantiate the FlowJudge model.\n        Returns:\n            The Metric object used to construct the FlowJudge model.\n        \"\"\"\n        processed_rubric = self._construct_rubric()\n        metric_prompt = Metric(\n            name=self.metric_name,\n            criteria=self.criteria,\n            rubric=processed_rubric,\n            required_inputs=self.required_inputs,\n            required_output=self.required_output,\n        )\n        return metric_prompt\n\n    def _construct_rubric(self) -&gt; List[RubricItem]:\n        \"\"\"\n        Construct the rubric from a user defined rubric dicitionary to construct the Metric object.\n        Returns:\n            List of RubricItem objects.\n        \"\"\"\n        processed_rubric = []\n        for key, value in self.rubric.items():\n            rubric_item = RubricItem(score=key, description=value)\n            processed_rubric.append(rubric_item)\n        return processed_rubric\n\n    def _pre_processing(self, inputs: List[Dict[str, str]], output: Dict[str, str]) -&gt; EvalInput:\n        return EvalInput(inputs=inputs, output=output)\n\n    def _inference(self, eval_input: EvalInput) -&gt; EvalOutput:\n        return self.model.evaluate(eval_input, save_results=False)\n</code></pre>"},{"location":"api/guardrails/#any_guardrail.guardrails.flowjudge.FlowJudgeClass.validate","title":"<code>validate(inputs, output)</code>","text":"<p>Classifies the desired input and output according to the associated metric provided to the judge.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>List[Dict[str, str]]</code> <p>A dictionary mapping the required input names to the inputs.</p> required <code>output</code> <code>Dict[str, str]</code> <p>A dictionary mapping the required output name to the output.</p> required <p>Return:     A score from the RubricItems and feedback related to the rubric and criteria.</p> Source code in <code>src/any_guardrail/guardrails/flowjudge.py</code> <pre><code>def validate(self, inputs: List[Dict[str, str]], output: Dict[str, str]) -&gt; GuardrailOutput:\n    \"\"\"\n    Classifies the desired input and output according to the associated metric provided to the judge.\n\n    Args:\n        inputs: A dictionary mapping the required input names to the inputs.\n        output: A dictionary mapping the required output name to the output.\n    Return:\n        A score from the RubricItems and feedback related to the rubric and criteria.\n    \"\"\"\n    eval_input = self._pre_processing(inputs, output)\n    result = self._inference(eval_input)\n    return GuardrailOutput(explanation=result.feedback, score=result.score)\n</code></pre>"},{"location":"api/guardrails/#any_guardrail.guardrails.glider","title":"<code>any_guardrail.guardrails.glider</code>","text":""},{"location":"api/guardrails/#any_guardrail.guardrails.glider.GLIDER","title":"<code>GLIDER</code>","text":"<p>               Bases: <code>Guardrail</code></p> <p>A prompt based guardrail from Patronus AI that utilizes pass criteria and a rubric to judge text. It outputs its reasoning, highlights for what determined the score, and an integer score. For more information, see the model card: GLIDER</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>HuggingFace path to model.</p> required <code>pass_criteria</code> <code>str</code> <p>A question or description of what you are validating.</p> required <code>rubric</code> <code>str</code> <p>A scoring rubric, describing to the model how to score the provided data.</p> required Raise <p>ValueError: Can only use model path to GLIDER from HuggingFace.</p> Source code in <code>src/any_guardrail/guardrails/glider.py</code> <pre><code>class GLIDER(Guardrail):\n    \"\"\"\n    A prompt based guardrail from Patronus AI that utilizes pass criteria and a rubric to judge text. It outputs its reasoning,\n    highlights for what determined the score, and an integer score. For more information, see the model card:\n    [GLIDER](https://huggingface.co/PatronusAI/glider)\n\n    Args:\n        model_id: HuggingFace path to model.\n        pass_criteria: A question or description of what you are validating.\n        rubric: A scoring rubric, describing to the model how to score the provided data.\n\n    Raise:\n        ValueError: Can only use model path to GLIDER from HuggingFace.\n    \"\"\"\n\n    SUPPORTED_MODELS = [\"PatronusAI/glider\"]\n\n    def __init__(self, model_id: str, pass_criteria: str, rubric: str) -&gt; None:\n        super().__init__(model_id)\n        self.pass_criteria = pass_criteria\n        self.rubric = rubric\n        self.system_prompt = SYSTEM_PROMPT_GLIDER\n\n    def validate(self, input_text: str, output_text: str) -&gt; GuardrailOutput:\n        \"\"\"\n        Uses the provided pass criteria and rubric to just the input and output text provided.\n\n        Args:\n            input_text: the initial text\n            output_text: the subsequent text\n        Returns:\n            An explanation in the format provided by the system prompt\n        \"\"\"\n        message = self._pre_processing(input_text, output_text)\n        result = self._inference(message)\n        return GuardrailOutput(explanation=result)\n\n    def _load_model(self) -&gt; None:\n        pipe = pipeline(\"text-classification\", self.model_id)\n        self.model = pipe\n\n    def _pre_processing(self, input_text: str, output_text: str) -&gt; List[Dict[str, str]]:\n        data = DEFAULT_DATA_FORMAT.format(input_text=input_text, output_text=output_text)\n        prompt = self.system_prompt.format(data=data, pass_criteria=self.pass_criteria, rubric=self.rubric)\n        message = [{\"role\": \"user\", \"content\": prompt}]\n        return message\n\n    def _inference(self, message: List[Dict[str, str]]) -&gt; str:\n        return self.model(message)[0][\"generated_text\"]\n</code></pre>"},{"location":"api/guardrails/#any_guardrail.guardrails.glider.GLIDER.validate","title":"<code>validate(input_text, output_text)</code>","text":"<p>Uses the provided pass criteria and rubric to just the input and output text provided.</p> <p>Parameters:</p> Name Type Description Default <code>input_text</code> <code>str</code> <p>the initial text</p> required <code>output_text</code> <code>str</code> <p>the subsequent text</p> required <p>Returns:     An explanation in the format provided by the system prompt</p> Source code in <code>src/any_guardrail/guardrails/glider.py</code> <pre><code>def validate(self, input_text: str, output_text: str) -&gt; GuardrailOutput:\n    \"\"\"\n    Uses the provided pass criteria and rubric to just the input and output text provided.\n\n    Args:\n        input_text: the initial text\n        output_text: the subsequent text\n    Returns:\n        An explanation in the format provided by the system prompt\n    \"\"\"\n    message = self._pre_processing(input_text, output_text)\n    result = self._inference(message)\n    return GuardrailOutput(explanation=result)\n</code></pre>"},{"location":"api/guardrails/#any_guardrail.guardrails.harmguard","title":"<code>any_guardrail.guardrails.harmguard</code>","text":""},{"location":"api/guardrails/#any_guardrail.guardrails.harmguard.HarmGuard","title":"<code>HarmGuard</code>","text":"<p>               Bases: <code>Guardrail</code></p> <p>Prompt injection detection encoder based model. For more information, please see the model card: HarmGuard</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>HuggingFace path to model.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>Can only use model path for HarmGuard from HuggingFace</p> Source code in <code>src/any_guardrail/guardrails/harmguard.py</code> <pre><code>class HarmGuard(Guardrail):\n    \"\"\"\n    Prompt injection detection encoder based model. For more information, please see the model card:\n    [HarmGuard](https://huggingface.co/hbseong/HarmAug-Guard)\n\n    Args:\n        model_id: HuggingFace path to model.\n\n    Raises:\n        ValueError: Can only use model path for HarmGuard from HuggingFace\n    \"\"\"\n\n    SUPPORTED_MODELS = [\"hbseong/HarmAug-Guard\"]\n\n    def __init__(self, model_id: str, threshold: float = HARMGUARD_DEFAULT_THRESHOLD) -&gt; None:\n        super().__init__(model_id)\n        self.threshold = threshold\n\n    def validate(self, input_text: str, output_text: str = \"\") -&gt; GuardrailOutput:\n        \"\"\"\n        Classifies input text and, optionally, output text for prompt injection detection.\n\n        Args:\n            input_text: the initial text that you want to validate\n            output_text: the subsequent text that you want to validate\n\n        Returns:\n            True if it is a prompt injection attack, False otherwise, and the associated final score.\n        \"\"\"\n        inputs = self._pre_processing(input_text, output_text)\n        outputs = self._inference(inputs)\n        final_unsafe, final_score = self._post_processing(outputs)\n\n        return GuardrailOutput(unsafe=final_unsafe, score=final_score)\n\n    def _load_model(self) -&gt; None:\n        tokenizer = AutoTokenizer.from_pretrained(self.model_id)  # type: ignore[no-untyped-call]\n        model = AutoModelForSequenceClassification.from_pretrained(self.model_id)\n        model.eval()\n        self.model = model\n        self.tokenizer = tokenizer\n\n    def _pre_processing(self, input_text: str, output_text: str) -&gt; torch.Tensor:\n        if output_text:\n            return self.tokenizer(input_text, return_tensors=\"pt\")\n        else:\n            return self.tokenizer(input_text, output_text, return_tensors=\"pt\")\n\n    def _inference(self, inputs: torch.Tensor) -&gt; torch.Tensor:\n        with torch.no_grad():\n            outputs = self.model(**inputs)\n        return outputs\n\n    def _post_processing(self, outputs: torch.Tensor) -&gt; Tuple[bool, float]:\n        unsafe_prob = F.softmax(outputs.logits, dim=-1)[:, 1]\n        final_score = unsafe_prob.item()\n        return final_score &gt; self.threshold, final_score\n</code></pre>"},{"location":"api/guardrails/#any_guardrail.guardrails.harmguard.HarmGuard.validate","title":"<code>validate(input_text, output_text='')</code>","text":"<p>Classifies input text and, optionally, output text for prompt injection detection.</p> <p>Parameters:</p> Name Type Description Default <code>input_text</code> <code>str</code> <p>the initial text that you want to validate</p> required <code>output_text</code> <code>str</code> <p>the subsequent text that you want to validate</p> <code>''</code> <p>Returns:</p> Type Description <code>GuardrailOutput</code> <p>True if it is a prompt injection attack, False otherwise, and the associated final score.</p> Source code in <code>src/any_guardrail/guardrails/harmguard.py</code> <pre><code>def validate(self, input_text: str, output_text: str = \"\") -&gt; GuardrailOutput:\n    \"\"\"\n    Classifies input text and, optionally, output text for prompt injection detection.\n\n    Args:\n        input_text: the initial text that you want to validate\n        output_text: the subsequent text that you want to validate\n\n    Returns:\n        True if it is a prompt injection attack, False otherwise, and the associated final score.\n    \"\"\"\n    inputs = self._pre_processing(input_text, output_text)\n    outputs = self._inference(inputs)\n    final_unsafe, final_score = self._post_processing(outputs)\n\n    return GuardrailOutput(unsafe=final_unsafe, score=final_score)\n</code></pre>"},{"location":"api/guardrails/#any_guardrail.guardrails.injecguard","title":"<code>any_guardrail.guardrails.injecguard</code>","text":""},{"location":"api/guardrails/#any_guardrail.guardrails.injecguard.InjecGuard","title":"<code>InjecGuard</code>","text":"<p>               Bases: <code>Guardrail</code></p> <p>Prompt injection detection encoder based model. For more information, please see the model card: InjecGuard</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>HuggingFace path to model.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>Can only use the model path for InjecGuard from HuggingFace</p> Source code in <code>src/any_guardrail/guardrails/injecguard.py</code> <pre><code>class InjecGuard(Guardrail):\n    \"\"\"\n    Prompt injection detection encoder based model. For more information, please see the model card:\n    [InjecGuard](https://huggingface.co/leolee99/InjecGuard)\n\n    Args:\n        model_id: HuggingFace path to model.\n\n    Raises:\n        ValueError: Can only use the model path for InjecGuard from HuggingFace\n    \"\"\"\n\n    SUPPORTED_MODELS = [\"leolee99/InjecGuard\"]\n\n    def __init__(self, model_id: str) -&gt; None:\n        super().__init__(model_id)\n\n    def validate(self, input_text: str) -&gt; GuardrailOutput:\n        \"\"\"\n        Classify some text to see if it contains a prompt injection attack.\n\n        Args:\n            input_text: the text to validate for prompt injection attacks\n        Returns:\n            True if there is a prompt injection attack, False otherwise\n        \"\"\"\n        classification = self._inference(input_text)\n        return GuardrailOutput(unsafe=self._post_processing(classification))\n\n    def _load_model(self) -&gt; None:\n        tokenizer = AutoTokenizer.from_pretrained(self.model_id)  # type: ignore[no-untyped-call]\n        model = AutoModelForSequenceClassification.from_pretrained(self.model_id)\n        pipe = pipeline(\"text-classification\", model=model, tokenizer=tokenizer)\n        self.model = pipe\n        self.tokenizer = tokenizer\n\n    def _inference(self, input_text: str) -&gt; List[Dict[str, str | float]]:\n        return self.model(input_text)\n\n    def _post_processing(self, classification: List[Dict[str, str | float]]) -&gt; bool:\n        return classification[0][\"label\"] == INJECGUARD_LABEL\n</code></pre>"},{"location":"api/guardrails/#any_guardrail.guardrails.injecguard.InjecGuard.validate","title":"<code>validate(input_text)</code>","text":"<p>Classify some text to see if it contains a prompt injection attack.</p> <p>Parameters:</p> Name Type Description Default <code>input_text</code> <code>str</code> <p>the text to validate for prompt injection attacks</p> required <p>Returns:     True if there is a prompt injection attack, False otherwise</p> Source code in <code>src/any_guardrail/guardrails/injecguard.py</code> <pre><code>def validate(self, input_text: str) -&gt; GuardrailOutput:\n    \"\"\"\n    Classify some text to see if it contains a prompt injection attack.\n\n    Args:\n        input_text: the text to validate for prompt injection attacks\n    Returns:\n        True if there is a prompt injection attack, False otherwise\n    \"\"\"\n    classification = self._inference(input_text)\n    return GuardrailOutput(unsafe=self._post_processing(classification))\n</code></pre>"},{"location":"api/guardrails/#any_guardrail.guardrails.jasper","title":"<code>any_guardrail.guardrails.jasper</code>","text":""},{"location":"api/guardrails/#any_guardrail.guardrails.jasper.Jasper","title":"<code>Jasper</code>","text":"<p>               Bases: <code>Guardrail</code></p> <p>Prompt injection detection encoder based models. For more information, please see the model card: Jasper Deberta Jasper Gelectra</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>HuggingFace path to model.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>Can only use model paths for Jasper models from HuggingFace.</p> Source code in <code>src/any_guardrail/guardrails/jasper.py</code> <pre><code>class Jasper(Guardrail):\n    \"\"\"\n    Prompt injection detection encoder based models. For more information, please see the model card:\n    [Jasper Deberta](https://huggingface.co/JasperLS/deberta-v3-base-injection)\n    [Jasper Gelectra](https://huggingface.co/JasperLS/gelectra-base-injection)\n\n    Args:\n        model_id: HuggingFace path to model.\n\n    Raises:\n        ValueError: Can only use model paths for Jasper models from HuggingFace.\n    \"\"\"\n\n    SUPPORTED_MODELS = [\"JasperLS/deberta-v3-base-injection\", \"JasperLS/gelectra-base-injection\"]\n\n    def __init__(self, model_id: str) -&gt; None:\n        super().__init__(model_id)\n\n    def validate(self, input_text: str) -&gt; GuardrailOutput:\n        \"\"\"\n        Classify some text to see if it contains a prompt injection attack.\n\n        Args:\n            input_text: the text to validate for prompt injection attacks\n        Returns:\n            True if there is a prompt injection attack, False otherwise\n        \"\"\"\n        classification = self._inference(input_text)\n        return GuardrailOutput(unsafe=self._post_processing(classification))\n\n    def _load_model(self) -&gt; None:\n        tokenizer = AutoTokenizer.from_pretrained(self.model_id)  # type: ignore[no-untyped-call]\n        model = AutoModelForSequenceClassification.from_pretrained(self.model_id)\n        pipe = pipeline(\"text-classification\", model=model, tokenizer=tokenizer)\n        self.model = pipe\n        self.tokenizer = tokenizer\n\n    def _inference(self, input_text: str) -&gt; List[Dict[str, str | float]]:\n        return self.model(input_text)\n\n    def _post_processing(self, classification: List[Dict[str, str | float]]) -&gt; bool:\n        return classification[0][\"label\"] == JASPER_INJECTION_LABEL\n</code></pre>"},{"location":"api/guardrails/#any_guardrail.guardrails.jasper.Jasper.validate","title":"<code>validate(input_text)</code>","text":"<p>Classify some text to see if it contains a prompt injection attack.</p> <p>Parameters:</p> Name Type Description Default <code>input_text</code> <code>str</code> <p>the text to validate for prompt injection attacks</p> required <p>Returns:     True if there is a prompt injection attack, False otherwise</p> Source code in <code>src/any_guardrail/guardrails/jasper.py</code> <pre><code>def validate(self, input_text: str) -&gt; GuardrailOutput:\n    \"\"\"\n    Classify some text to see if it contains a prompt injection attack.\n\n    Args:\n        input_text: the text to validate for prompt injection attacks\n    Returns:\n        True if there is a prompt injection attack, False otherwise\n    \"\"\"\n    classification = self._inference(input_text)\n    return GuardrailOutput(unsafe=self._post_processing(classification))\n</code></pre>"},{"location":"api/guardrails/#any_guardrail.guardrails.pangolin","title":"<code>any_guardrail.guardrails.pangolin</code>","text":""},{"location":"api/guardrails/#any_guardrail.guardrails.pangolin.Pangolin","title":"<code>Pangolin</code>","text":"<p>               Bases: <code>Guardrail</code></p> <p>Prompt injection detection encoder based models. For more information, please see the model card: Pangolin Base Pangolin Large</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>HuggingFace path to model.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>Can only use model paths for Pangolin from HuggingFace</p> Source code in <code>src/any_guardrail/guardrails/pangolin.py</code> <pre><code>class Pangolin(Guardrail):\n    \"\"\"\n    Prompt injection detection encoder based models. For more information, please see the model card:\n    [Pangolin Base](https://huggingface.co/dcarpintero/pangolin-guard-base)\n    [Pangolin Large](https://huggingface.co/dcarpintero/pangolin-guard-large)\n\n    Args:\n        model_id: HuggingFace path to model.\n\n    Raises:\n        ValueError: Can only use model paths for Pangolin from HuggingFace\n    \"\"\"\n\n    SUPPORTED_MODELS = [\"dcarpintero/pangolin-guard-large\", \"dcarpintero/pangolin-guard-base\"]\n\n    def __init__(self, model_id: str) -&gt; None:\n        super().__init__(model_id)\n\n    def validate(self, input_text: str) -&gt; GuardrailOutput:\n        \"\"\"\n        Classify some text to see if it contains a prompt injection attack.\n\n        Args:\n            input_text: the text to validate for prompt injection attacks\n        Returns:\n            True if there is a prompt injection attack, False otherwise\n        \"\"\"\n        classification = self._inference(input_text)\n        return GuardrailOutput(unsafe=self._post_processing(classification))\n\n    def _load_model(self) -&gt; None:\n        pipe = pipeline(\"text-classification\", self.model_id)\n        self.model = pipe\n\n    def _inference(self, input_text: str) -&gt; List[Dict[str, str | float]]:\n        return self.model(input_text)\n\n    def _post_processing(self, classification: List[Dict[str, str | float]]) -&gt; bool:\n        return classification[0][\"label\"] == PANGOLIN_INJECTION_LABEL\n</code></pre>"},{"location":"api/guardrails/#any_guardrail.guardrails.pangolin.Pangolin.validate","title":"<code>validate(input_text)</code>","text":"<p>Classify some text to see if it contains a prompt injection attack.</p> <p>Parameters:</p> Name Type Description Default <code>input_text</code> <code>str</code> <p>the text to validate for prompt injection attacks</p> required <p>Returns:     True if there is a prompt injection attack, False otherwise</p> Source code in <code>src/any_guardrail/guardrails/pangolin.py</code> <pre><code>def validate(self, input_text: str) -&gt; GuardrailOutput:\n    \"\"\"\n    Classify some text to see if it contains a prompt injection attack.\n\n    Args:\n        input_text: the text to validate for prompt injection attacks\n    Returns:\n        True if there is a prompt injection attack, False otherwise\n    \"\"\"\n    classification = self._inference(input_text)\n    return GuardrailOutput(unsafe=self._post_processing(classification))\n</code></pre>"},{"location":"api/guardrails/#any_guardrail.guardrails.protectai","title":"<code>any_guardrail.guardrails.protectai</code>","text":""},{"location":"api/guardrails/#any_guardrail.guardrails.protectai.ProtectAI","title":"<code>ProtectAI</code>","text":"<p>               Bases: <code>Guardrail</code></p> <p>Prompt injection detection encoder based models. For more information, please see the model cards: ProtectA</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>HuggingFace path to model.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>Can only use model paths for ProtectAI from HuggingFace.</p> Source code in <code>src/any_guardrail/guardrails/protectai.py</code> <pre><code>class ProtectAI(Guardrail):\n    \"\"\"\n    Prompt injection detection encoder based models. For more information, please see the model cards:\n    [ProtectA](https://huggingface.co/collections/protectai/llm-security-65c1f17a11c4251eeab53f40)\n\n    Args:\n        model_id: HuggingFace path to model.\n\n    Raises:\n        ValueError: Can only use model paths for ProtectAI from HuggingFace.\n    \"\"\"\n\n    SUPPORTED_MODELS = [\n        \"protectai/deberta-v3-base-prompt-injection\",\n        \"protectai/deberta-v3-small-prompt-injection-v2\",\n        \"protectai/deberta-v3-base-prompt-injection-v2\",\n    ]\n\n    def __init__(self, model_id: str) -&gt; None:\n        super().__init__(model_id)\n\n    def validate(self, input_text: str) -&gt; GuardrailOutput:\n        \"\"\"\n        Classify some text to see if it contains a prompt injection attack.\n\n        Args:\n            input_text: the text to validate for prompt injection attacks\n        Returns:\n            True if there is a prompt injection attack, False otherwise\n        \"\"\"\n        classification = self._inference(input_text)\n        return GuardrailOutput(unsafe=self._post_processing(classification))\n\n    def _load_model(self) -&gt; None:\n        tokenizer = AutoTokenizer.from_pretrained(self.model_id)  # type: ignore[no-untyped-call]\n        model = AutoModelForSequenceClassification.from_pretrained(self.model_id)\n        pipe = pipeline(\"text-classification\", model=model, tokenizer=tokenizer)\n        self.model = pipe\n        self.tokenizer = tokenizer\n\n    def _inference(self, input_text: str) -&gt; List[Dict[str, str | float]]:\n        return self.model(input_text)\n\n    def _post_processing(self, classification: List[Dict[str, str | float]]) -&gt; bool:\n        return classification[0][\"label\"] == PROTECTAI_INJECTION_LABEL\n</code></pre>"},{"location":"api/guardrails/#any_guardrail.guardrails.protectai.ProtectAI.validate","title":"<code>validate(input_text)</code>","text":"<p>Classify some text to see if it contains a prompt injection attack.</p> <p>Parameters:</p> Name Type Description Default <code>input_text</code> <code>str</code> <p>the text to validate for prompt injection attacks</p> required <p>Returns:     True if there is a prompt injection attack, False otherwise</p> Source code in <code>src/any_guardrail/guardrails/protectai.py</code> <pre><code>def validate(self, input_text: str) -&gt; GuardrailOutput:\n    \"\"\"\n    Classify some text to see if it contains a prompt injection attack.\n\n    Args:\n        input_text: the text to validate for prompt injection attacks\n    Returns:\n        True if there is a prompt injection attack, False otherwise\n    \"\"\"\n    classification = self._inference(input_text)\n    return GuardrailOutput(unsafe=self._post_processing(classification))\n</code></pre>"},{"location":"api/guardrails/#any_guardrail.guardrails.sentinel","title":"<code>any_guardrail.guardrails.sentinel</code>","text":""},{"location":"api/guardrails/#any_guardrail.guardrails.sentinel.Sentinel","title":"<code>Sentinel</code>","text":"<p>               Bases: <code>Guardrail</code></p> <p>Prompt injection detection encoder based model. For more information, please see the model card: Sentinel</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>HuggingFace path to model.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>Can only use model path for Sentinel from HuggingFace.</p> Source code in <code>src/any_guardrail/guardrails/sentinel.py</code> <pre><code>class Sentinel(Guardrail):\n    \"\"\"\n    Prompt injection detection encoder based model. For more information, please see the model card:\n    [Sentinel](https://huggingface.co/qualifire/prompt-injection-sentinel)\n\n    Args:\n        model_id: HuggingFace path to model.\n\n    Raises:\n        ValueError: Can only use model path for Sentinel from HuggingFace.\n    \"\"\"\n\n    SUPPORTED_MODELS = [\"qualifire/prompt-injection-sentinel\"]\n\n    def __init__(self, model_id: str) -&gt; None:\n        super().__init__(model_id)\n\n    def validate(self, input_text: str) -&gt; GuardrailOutput:\n        \"\"\"\n        Classify some text to see if it contains a prompt injection attack.\n\n        Args:\n            input_text: the text to validate for prompt injection attacks\n        Returns:\n            True if there is a prompt injection attack, False otherwise\n        \"\"\"\n        classification = self._inference(input_text)\n        return GuardrailOutput(unsafe=self._post_processing(classification))\n\n    def _load_model(self) -&gt; None:\n        tokenizer = AutoTokenizer.from_pretrained(self.model_id)  # type: ignore[no-untyped-call]\n        model = AutoModelForSequenceClassification.from_pretrained(self.model_id)\n        pipe = pipeline(\"text-classification\", model=model, tokenizer=tokenizer)\n        self.model = pipe\n        self.tokenizer = tokenizer\n\n    def _inference(self, input_text: str) -&gt; List[Dict[str, str | float]]:\n        return self.model(input_text)\n\n    def _post_processing(self, classification: List[Dict[str, str | float]]) -&gt; bool:\n        return classification[0][\"label\"] == SENTINEL_INJECTION_LABEL\n</code></pre>"},{"location":"api/guardrails/#any_guardrail.guardrails.sentinel.Sentinel.validate","title":"<code>validate(input_text)</code>","text":"<p>Classify some text to see if it contains a prompt injection attack.</p> <p>Parameters:</p> Name Type Description Default <code>input_text</code> <code>str</code> <p>the text to validate for prompt injection attacks</p> required <p>Returns:     True if there is a prompt injection attack, False otherwise</p> Source code in <code>src/any_guardrail/guardrails/sentinel.py</code> <pre><code>def validate(self, input_text: str) -&gt; GuardrailOutput:\n    \"\"\"\n    Classify some text to see if it contains a prompt injection attack.\n\n    Args:\n        input_text: the text to validate for prompt injection attacks\n    Returns:\n        True if there is a prompt injection attack, False otherwise\n    \"\"\"\n    classification = self._inference(input_text)\n    return GuardrailOutput(unsafe=self._post_processing(classification))\n</code></pre>"},{"location":"api/guardrails/#any_guardrail.guardrails.shield_gemma","title":"<code>any_guardrail.guardrails.shield_gemma</code>","text":""},{"location":"api/guardrails/#any_guardrail.guardrails.shield_gemma.ShieldGemma","title":"<code>ShieldGemma</code>","text":"<p>               Bases: <code>Guardrail</code></p> <p>Wrapper class for Google ShieldGemma models. For more information, please visit the model cards: Shield Gemma</p> <p>Note we do not support the image classifier.</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>HuggingFace path to model.</p> required <code>policy</code> <code>str</code> <p>The safety policy to enforce.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>Can only use model_ids to ShieldGemma from HuggingFace.</p> Source code in <code>src/any_guardrail/guardrails/shield_gemma.py</code> <pre><code>class ShieldGemma(Guardrail):\n    \"\"\"\n    Wrapper class for Google ShieldGemma models. For more information, please visit the model cards:\n    [Shield Gemma](https://huggingface.co/collections/google/shieldgemma-67d130ef8da6af884072a789)\n\n    Note we do not support the image classifier.\n\n    Args:\n        model_id: HuggingFace path to model.\n        policy: The safety policy to enforce.\n\n    Raises:\n        ValueError: Can only use model_ids to ShieldGemma from HuggingFace.\n    \"\"\"\n\n    SUPPORTED_MODELS = [\n        \"google/shieldgemma-2b\",\n        \"google/shieldgemma-9b\",\n        \"google/shieldgemma-27b\",\n        \"hf-internal-testing/tiny-random-Gemma3ForCausalLM\",\n    ]\n\n    def __init__(self, model_id: str, policy: str, threshold: float = DEFAULT_THRESHOLD) -&gt; None:\n        super().__init__(model_id)\n        self.policy = policy\n        self.system_prompt = SYSTEM_PROMPT_SHIELD_GEMMA\n        self.threshold = threshold\n\n    def validate(self, input_text: str) -&gt; GuardrailOutput:\n        \"\"\"\n        Classify input_text according to the safety policy.\n\n        Args:\n            input_text: the text you want to validate based on the policy\n        Returns:\n            True if the text violates the policy, False otherwise\n        \"\"\"\n        preprocessed_input = self._pre_processing(input_text)\n        logits = self._inference(preprocessed_input)\n        unsafe = self._post_processing(logits)\n        return GuardrailOutput(unsafe=unsafe)\n\n    def _load_model(self) -&gt; None:\n        tokenizer = AutoTokenizer.from_pretrained(self.model_id)  # type: ignore[no-untyped-call]\n        model = AutoModelForCausalLM.from_pretrained(self.model_id, device_map=\"auto\", torch_dtype=torch.bfloat16)\n        self.model = model\n        self.tokenizer = tokenizer\n\n    def _pre_processing(self, input_text: str) -&gt; torch.Tensor:\n        formatted_prompt = self.system_prompt.format(user_prompt=input_text, safety_policy=self.policy)\n        inputs = self.tokenizer(formatted_prompt, return_tensors=\"pt\").to(self.model.device)\n        return inputs\n\n    def _inference(self, inputs: torch.Tensor) -&gt; torch.FloatTensor:\n        with torch.no_grad():\n            logits = self.model(**inputs).logits\n        return logits\n\n    def _post_processing(self, logits: torch.FloatTensor) -&gt; bool:\n        vocab = self.tokenizer.get_vocab()\n        selected_logits = logits[0, -1, [vocab[\"Yes\"], vocab[\"No\"]]]\n        probabilities = softmax(selected_logits, dim=0)\n        score = probabilities[0].item()\n        return score &gt; self.threshold\n</code></pre>"},{"location":"api/guardrails/#any_guardrail.guardrails.shield_gemma.ShieldGemma.validate","title":"<code>validate(input_text)</code>","text":"<p>Classify input_text according to the safety policy.</p> <p>Parameters:</p> Name Type Description Default <code>input_text</code> <code>str</code> <p>the text you want to validate based on the policy</p> required <p>Returns:     True if the text violates the policy, False otherwise</p> Source code in <code>src/any_guardrail/guardrails/shield_gemma.py</code> <pre><code>def validate(self, input_text: str) -&gt; GuardrailOutput:\n    \"\"\"\n    Classify input_text according to the safety policy.\n\n    Args:\n        input_text: the text you want to validate based on the policy\n    Returns:\n        True if the text violates the policy, False otherwise\n    \"\"\"\n    preprocessed_input = self._pre_processing(input_text)\n    logits = self._inference(preprocessed_input)\n    unsafe = self._post_processing(logits)\n    return GuardrailOutput(unsafe=unsafe)\n</code></pre>"}]}